[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Introduction\nIn this exercise, I am working on TidyTuesday dataset (April 8th 2025). The dataset is about “Timely and Effective Care” in the United States from Centers for Medicare & Medicaid Services. The data was curated by Jon Harmon from Data Science Learning Community. The dataset contains several variables including state, condition, ID, name, score, footnote, and date of admission.\n\nstate (character): The two-letter code for the state (or territory, etc) where the hospital is located.\ncondition (character): The condition for which the patient was admitted. Six categories of conditions are included in the data.\nmeasure_id (character): The ID of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\nmeasure_name (character): The name of the thing being measured. Note that there are 22 unique IDs but only 21 unique names.\nscore (character): The score of the measure.\nfootnote (character): Footnotes that apply to this measure: 5 = “Results are not available for this reporting period.”, 25 = “State and national averages include Veterans Health Administration (VHA) hospital data.”, 26 = “State and national averages include Department of Defense (DoD) hospital data.”.\nstart_date (date): The date on which measurement began for this measure.\nend_date (date): The date on which measurement ended for this measure\n\n\n\nLoading Packages\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nWarning: package 'tibble' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.3\n\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\nWarning: package 'forcats' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at C:/Users/mn27712/MADA_NEW/muhammadnasir-mada2025-portofolio\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(lubridate)\n#install.packages(\"gt\")\nlibrary(gt)\n#install.packages(\"gtsummary\")\nlibrary(gtsummary)   \n\nWarning: package 'gtsummary' was built under R version 4.4.3\n\n#install.packages(\"cli\")\nlibrary(cli)\n\nWarning: package 'cli' was built under R version 4.4.3\n\n#install.packages(\"tidymodels\")\nlibrary(tidymodels)  \n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'infer' was built under R version 4.4.3\n\n\nWarning: package 'modeldata' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\n#install.packages(\"broom.mixed\")\n#install.packages(\"purrr\")\nlibrary(skimr)\n#install.packages(\"naniar\")\nlibrary(naniar)\n\n\nAttaching package: 'naniar'\n\nThe following object is masked from 'package:skimr':\n\n    n_complete\n\n#install.packages(\"treemapify\")\nlibrary(treemapify)\n#install.packages(\"usmap\")\nlibrary(usmap)\n#install.packages(\"car\")\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.4.3\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\n#Data sources\nDataset is downloaded from (https://data.cms.gov/provider-data/dataset/apyc-v239). Information regarding the dataset is obtained from tidytuesday github for the weekly data (April 8th 2025)\n\n#Importing the downloaded CSV\ncare &lt;- read_csv(here(\"tidytuesday-exercise\", \"data\", \"care_state.csv\")) \n\nRows: 1232 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): state, condition, measure_id, measure_name, footnote\ndbl  (1): score\ndate (2): start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n#check the structure of the dataset\nstr(care)\n\nspc_tbl_ [1,232 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ state       : chr [1:1232] \"AK\" \"AK\" \"AK\" \"AK\" ...\n $ condition   : chr [1:1232] \"Healthcare Personnel Vaccination\" \"Healthcare Personnel Vaccination\" \"Emergency Department\" \"Emergency Department\" ...\n $ measure_id  : chr [1:1232] \"HCP_COVID_19\" \"IMM_3\" \"OP_18b\" \"OP_18b_HIGH_MIN\" ...\n $ measure_name: chr [1:1232] \"Percentage of healthcare personnel who are up to date with COVID-19 vaccinations\" \"Healthcare workers given influenza vaccination Higher percentages are better\" \"Average (median) time patients spent in the emergency department before leaving from the visit A lower number o\"| __truncated__ \"Average time patients spent in the emergency department before being sent home A lower number of minutes is better (high)\" ...\n $ score       : num [1:1232] 7.3 80 140 157 136 136 NA 196 230 182 ...\n $ footnote    : chr [1:1232] NA NA \"25, 26\" \"25, 26\" ...\n $ start_date  : Date[1:1232], format: \"2024-01-01\" \"2023-10-01\" ...\n $ end_date    : Date[1:1232], format: \"2024-03-31\" \"2024-03-31\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   state = col_character(),\n  ..   condition = col_character(),\n  ..   measure_id = col_character(),\n  ..   measure_name = col_character(),\n  ..   score = col_double(),\n  ..   footnote = col_character(),\n  ..   start_date = col_date(format = \"\"),\n  ..   end_date = col_date(format = \"\")\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nLet’s see the data summry using skim()\n\nskim(care) # this is the smart version of summary() \n\n\nData summary\n\n\nName\ncare\n\n\nNumber of rows\n1232\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstate\n0\n1.00\n2\n2\n0\n56\n0\n\n\ncondition\n0\n1.00\n11\n35\n0\n6\n0\n\n\nmeasure_id\n0\n1.00\n5\n20\n0\n22\n0\n\n\nmeasure_name\n0\n1.00\n26\n172\n0\n21\n0\n\n\nfootnote\n168\n0.86\n1\n6\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nstart_date\n0\n1\n2023-01-01\n2024-01-01\n2023-04-01\n4\n\n\nend_date\n0\n1\n2023-12-31\n2024-03-31\n2024-03-31\n2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nscore\n155\n0.87\n134.04\n102.02\n1\n70\n93\n193\n730\n▇▃▁▁▁\n\n\n\n\n\n#Data Exploration\n\nExplore each variable\nIn this step, I would like to explore each variables to understand more the data distribution and pattern\nstate\n\ntable(care$state)\n\n\nAK AL AR AS AZ CA CO CT DC DE FL GA GU HI IA ID IL IN KS KY LA MA MD ME MI MN \n22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 \nMO MP MS MT NC ND NE NH NJ NM NV NY OH OK OR PA PR RI SC SD TN TX UT VA VI VT \n22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 22 \nWA WI WV WY \n22 22 22 22 \n\n\nAll state has same number of entries (22 entries)\ncondition\n\n# 1. Summarize the condition counts\ncondition_counts &lt;- care %&gt;%\n  count(condition, sort = TRUE)\n\n# 2. Create a nice table\ncondition_counts %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Condition Frequencies\",\n    subtitle = \"Timely and Effective Care Data\"\n  ) %&gt;%\n  cols_label(\n    condition = \"Condition\",\n    n = \"Count\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(n),\n    decimals = 0\n  ) %&gt;%\n  opt_table_font(\n    font = list(\n      google_font(\"Roboto\"), \n      default_fonts()\n    )\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(everything())\n  )\n\n\n\n\n\n\n\nCondition Frequencies\n\n\nTimely and Effective Care Data\n\n\nCondition\nCount\n\n\n\n\nEmergency Department\n672\n\n\nSepsis Care\n280\n\n\nHealthcare Personnel Vaccination\n112\n\n\nCataract surgery outcome\n56\n\n\nColonoscopy care\n56\n\n\nElectronic Clinical Quality Measure\n56\n\n\n\n\n\n\n\nScore of the measure\nDistribution of care score\n\nggplot(care, aes(x = score)) +\n  geom_density(fill = \"lightgreen\", color = \"black\") +\n  labs(\n    title = \"Distribution of Care Scores\",\n    x = \"Score\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 155 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nI want to explore the score of each measure\n\nggplot(care, aes(x = score, fill = measure_id)) +\n  geom_histogram(color = \"white\", bins = 30) +  # white borders between bars\n  facet_wrap(vars(measure_id), scales = \"free\") +  # free scales if measures vary a lot\n  labs(title = \"Distribution of Score by Measure\",\n       x = \"Score\",\n       y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nWarning: Removed 155 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNow I want to explore the average score of each condition.\n\n# 1. Summarize the average score by condition\navg_score_condition &lt;- care %&gt;%\n  group_by(condition) %&gt;%\n  summarise(avg_score = mean(score, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_score))\n\n# 2. Plot\nggplot(avg_score_condition, aes(x = reorder(condition, avg_score), y = avg_score, fill = condition)) +\n  geom_col() +  # bar plot\n  coord_flip() +\n  labs(title = \"Average Score by Condition\",\n       x = \"Condition\",\n       y = \"Average Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nstate_score_avg &lt;- care%&gt;%\n  group_by(state) %&gt;%\n  summarise(avg_score = mean(score, na.rm = TRUE))\n\n# Plot the map\nplot_usmap(data = state_score_avg, values = \"avg_score\", color = \"white\") +\n  scale_fill_continuous(\n    low = \"lightgreen\", high = \"darkgreen\", name = \"Avg Score\", label = scales::comma\n  ) +\n  labs(\n    title = \"Average Care Score by State\"\n  ) +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nNow I want to see the average waiting time in emergency room by state.\n\n# 1. Filter Emergency Department measures\ncare_emergency &lt;- care %&gt;%\n  filter(\n    condition == \"Emergency Department\",\n    grepl(\"Average time patients spent\", measure_name, ignore.case = TRUE)\n  )\n\n# 2. Calculate average score (visit time) for each state\nstate_avg_emergency &lt;- care_emergency %&gt;%\n  group_by(state) %&gt;%\n  summarise(avg_score = mean(score, na.rm = TRUE))\n\n# 3. Get the map points\nstate_map_points &lt;- usmap::us_map(regions = \"states\")\n\n# 4. Convert to sf manually\nstate_map_sf &lt;- sf::st_as_sf(state_map_points, wkt = \"geom\", crs = 4326)\n\n# 5. Extract centroid coordinates\nstate_centroids &lt;- state_map_sf %&gt;%\n  sf::st_centroid() %&gt;%\n  sf::st_coordinates() %&gt;%\n  as.data.frame()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n# 6. Build center dataset\nstate_centers &lt;- tibble(\n  state = state_map_sf$abbr,\n  x = state_centroids$X,\n  y = state_centroids$Y\n)\n\n# 7. Merge centers with average scores\nstate_avg_emergency_labels &lt;- left_join(state_avg_emergency, state_centers, by = \"state\")\n\n# 8. onvert score into hours\nstate_avg_emergency_labels &lt;- state_avg_emergency_labels %&gt;%\n  mutate(\n    avg_hours = round(avg_score / 60, 1)  # divide by 60 and round to 1 decimal\n  )\n\n# 9. Plot the map (now label with hours)\nplot_usmap(data = state_avg_emergency, values = \"avg_score\", color = \"white\") +\n  geom_text(\n    data = state_avg_emergency_labels, \n    aes(x = x, y = y, label = avg_hours),  # &lt;-- use avg_hours instead of avg_score\n    inherit.aes = FALSE, \n    size = 2.8, \n    color = \"black\"\n  ) +\n  scale_fill_gradient(\n    low = \"mistyrose\", \n    high = \"darkred\", \n    name = \"Avg ER Visit Time (minutes)\", \n    label = scales::comma\n  ) +\n  labs(\n    title = \"Average Emergency Room waiting Time by State (in Hours)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n###Explore missing values\nBefore doing further analysis, I want to explore the missing value\n\ngg_miss_var(care)+ labs(title = \"Number missing values for each variable\")  \n\n\n\n\n\n\n\n\nNow I want to explore missing value for each variable by measurement.\n\ngg_miss_var(care, facet = measure_id) + \n  labs(title = \"Number missing values for each variable by each measure_id\")\n\n\n\n\n\n\n\n\n\n\nData Wrangling\nIn this step, I will create a new dataset, which only contain variables needed to answer may research question. In the new dataset, I will convert dataset from long format into wide format. It will make me easy to perform data analysis. The columns are variables, and rows are states of USA. I will drop some variables and delete missing values.\n\n# Load required packages\nlibrary(tidyverse)\n\n# 1. Filter the dataset to only include the variables you want\ncare_selected &lt;- care %&gt;%\n  filter(measure_id %in% c(\n    \"HCP_COVID_19\",   # COVID-19 vaccination\n    \"IMM_3\",          # Influenza vaccination\n    \"OP_18b\",         # ED waiting time\n    \"OP_23\",          # Stroke ED brain scan time\n    \"SAFE_USE_OF_OPIOIDS\",  # Safe opioid prescribing\n    \"SEP_1\"           # Sepsis care quality\n  )) %&gt;%\n  select(state, measure_id, score)\n\n# 2. Pivot to wide format\ncare_final &lt;- care_selected %&gt;%\n  pivot_wider(\n    names_from = measure_id,\n    values_from = score\n  )\n\n# 3. Check the resulting dataset\n  glimpse(care_final)\n\nRows: 56\nColumns: 7\n$ state               &lt;chr&gt; \"AK\", \"AL\", \"AR\", \"AS\", \"AZ\", \"CA\", \"CO\", \"CT\", \"D…\n$ HCP_COVID_19        &lt;dbl&gt; 7.3, 5.9, 2.7, NA, 17.4, 22.4, 11.8, 13.2, 6.6, 38…\n$ IMM_3               &lt;dbl&gt; 80, 76, 81, NA, 85, 73, 92, 85, 94, 84, 60, 76, NA…\n$ OP_18b              &lt;dbl&gt; 140, 145, 133, NA, 168, 184, 133, 193, 310, 217, 1…\n$ OP_23               &lt;dbl&gt; 60, 67, 72, NA, 68, 73, 70, 72, 26, 73, 69, 69, NA…\n$ SAFE_USE_OF_OPIOIDS &lt;dbl&gt; 16, 14, 16, NA, 11, 14, 15, 18, 11, 15, 16, 15, NA…\n$ SEP_1               &lt;dbl&gt; 58, 61, 64, NA, 55, 67, 73, 57, 49, 48, 68, 57, NA…\n\n# 4. (Optional) View it nicely\nhead(care_final)\n\n# A tibble: 6 × 7\n  state HCP_COVID_19 IMM_3 OP_18b OP_23 SAFE_USE_OF_OPIOIDS SEP_1\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;               &lt;dbl&gt; &lt;dbl&gt;\n1 AK             7.3    80    140    60                  16    58\n2 AL             5.9    76    145    67                  14    61\n3 AR             2.7    81    133    72                  16    64\n4 AS            NA      NA     NA    NA                  NA    NA\n5 AZ            17.4    85    168    68                  11    55\n6 CA            22.4    73    184    73                  14    67\n\ndim(care_final)\n\n[1] 56  7\n\n\nI want to drop missing value\n\ncare_final &lt;- care_final %&gt;%\n  drop_na()\n\ndim(care_final)\n\n[1] 52  7\n\n\nFor the modeling purpose, the data is slitted into training data (75%) and test data (25%)\n\nseed = 4321\n\nset.seed(seed)  # setting seed\nsplit &lt;- initial_split(care_final, prop = 0.75)\n\ncare_train &lt;- training(split)\ncare_test &lt;- testing(split)\n\nmore exploration for the primary outcome\nBefore going further, I want to explore the score distribution of my primary outcome.\n\ncare_final %&gt;%\n  ggplot(aes(x = OP_18b)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"white\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", size = 1) +\n  labs(\n    title = \"Distribution of Emergency Department Waiting Time (OP_18b)\",\n    x = \"Median ED Waiting Time (Minutes)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nNote: The score show moderate right-skeewed. It needs to be transformed in the model for better result\n\n\n\nResearch Questions\nTimely access to emergency department (ED) care is essential for reducing patient morbidity and mortality. However, many hospitals across the United States continue to face challenges related to ED overcrowding and extended waiting times. Maintaining a healthy healthcare workforce is critical to ensuring efficient emergency care delivery. Vaccination coverage among healthcare personnel—specifically for COVID-19 and influenza—has been prioritized as a public health measure to reduce workforce illness and absenteeism. Higher rates of staff vaccination may improve hospital operations by preserving adequate staffing levels and maintaining patient flow, particularly in high-pressure environments such as emergency departments. Despite its importance, the relationship between healthcare personnel vaccination rates and operational outcomes like ED waiting times has not been extensively studied at a population level. Understanding whether improved vaccine coverage translates into more efficient emergency care could inform future workforce and infection control policies.\nThis exercise examines whether higher COVID-19 and influenza vaccination rates among healthcare workers are associated with shorter emergency department waiting times across U.S. states. Using standardized national measures, including healthcare worker vaccination rates and median ED visit times. This research evaluates potential associations between workforce health protection and patient access to timely care. In addition, hospital-level quality indicators such as safe opioid prescribing practices and sepsis care bundle compliance are included to account for broader institutional quality, ensuring a more accurate interpretation of the findings. By focusing on these key factors, this study seeks to provide evidence on whether strengthening healthcare personnel vaccination efforts could yield operational benefits beyond infection prevention, ultimately improving patient experience and emergency care system performance.\nRQ: Is higher COVID-19 and influenza vaccination score among healthcare personnel associated with shorter emergency department (ED) waiting times across U.S. states?\nHypotheses: Higher healthcare personnel vaccination score (COVID-19 and influenza) is associated with shorter emergency department waiting times across U.S. states.\n\n\nModeling\n##Initial Analysis In this initial analysis, I will perform linear model to see if the primary outome needs to be transformed, and to test multicolinearity.\n\nmodel1 &lt;- lm(OP_18b ~ HCP_COVID_19 + IMM_3 + OP_23 + SAFE_USE_OF_OPIOIDS + SEP_1, data = care_final)\n\nmulticolinearity test\n\nvif(model1)\n\n       HCP_COVID_19               IMM_3               OP_23 SAFE_USE_OF_OPIOIDS \n           1.557707            1.239288            1.077637            1.754512 \n              SEP_1 \n           2.022136 \n\n\nThe is no multicolinearity. Thi is good to go to the next step.\nNow I want to see the residual to see if the original data can be performedr transfomation is needed.\n\npar(mfrow = c(2, 2))  # Arrange 4 plots in 1 window\nplot(model1)\n\n\n\n\n\n\n\n\n\nThe residuals look mostly scattered randomly, but there’s a small curve (slight nonlinearity).\nNormal Q-Q plot shows Most points lie close to the diagonal line.\nDiagnostic plots showed that model residuals were approximately normally distributed with no strong evidence of nonlinearity, heteroscedasticity, or influential outliers, supporting the appropriateness of linear regression modeling for this dataset.\n\nI want to try log transformation.\n\nmodel1_log &lt;- lm(log(OP_18b) ~ HCP_COVID_19 + IMM_3 + OP_23 + SAFE_USE_OF_OPIOIDS + SEP_1, data = care_final)\n\n\npar(mfrow = c(2, 2))  # Arrange 4 plots in 1 window\nplot(model1_log)\n\n\n\n\n\n\n\n\nThere is no much change after using log transformation. Therefore, I will use without log transformation in my model.\n##Futher Analysis\nIn this exercise, I will use three different model, Multiple Linear Regression, LASSO Regression, and Random Forest. The outcome variable is median of waiting time in emergency department (OP_18b). The predictors include Percentage of healthcare personnel who are up to date with COVID-19 vaccinations (HCP_COVID_19), Healthcare workers given influenza vaccination Higher percentages are better (IMM_3), Safe Use of Opioids - Concurrent Prescribing (SAFE_USE_OF_OPIOIDS), Percentage of patients who came to the emergency department with stroke symptoms who received brain scan results within 45 minutes of arrival (OP_23), and Percentage of patients who received appropriate care for severe sepsis and septic shock (SEP_1).\n\nLinear Regression Model\nFirst of all, multiple linear regression is fitted.\n\nset.seed(seed)\n# Define a linear model with all predictors\ncare_lm &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# work flow \ncare_lm_wf &lt;- workflow() %&gt;% \n  add_model(care_lm) %&gt;% \n  add_formula(OP_18b ~ HCP_COVID_19 + IMM_3 + SAFE_USE_OF_OPIOIDS + OP_23 + SEP_1)\n\n# Set up cross-validation (15-fold CV)\ncare_lm_cv_results &lt;- fit_resamples(\n  care_lm_wf,\n  resamples = vfold_cv(care_train, v = 15),\n  metrics = metric_set(rmse, rsq),\n  control = control_resamples(save_pred = TRUE)\n)\n\n#Print result \ncollect_metrics(care_lm_cv_results)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   24.8      15   3.54  Preprocessor1_Model1\n2 rsq     standard    0.582    15   0.104 Preprocessor1_Model1\n\n\nNow I want to fit into all training data\n\n# Fit model on the training data\ncare_lm_train &lt;- care_lm_wf %&gt;% fit(care_train)\n\n# Compute predictions on training data\ncare_lm_preds &lt;- predict(care_lm_train, care_train) %&gt;% bind_cols(care_train)\n\n\n# Plot observed vs. predicted\nggplot(care_lm_preds, aes(x = OP_18b, y = .pred)) +\n  geom_point(color = \"purple\", alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"Observed vs. Predicted Values - Linear Regression\",\n       x = \"Observed\",\n       y = \"Predicted\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe multiple linear regression model reasonably captures the general trend between predictors and ED waiting time across states; however, there is evidence of underestimation, particularly at higher observed values, suggesting some model misspecification or nonlinearity not captured.\n\nlm_coefs &lt;- tidy(care_lm_train)\nprint(lm_coefs)\n\n# A tibble: 6 × 5\n  term                estimate std.error statistic p.value\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          129.       88.5       1.46   0.154 \n2 HCP_COVID_19           0.973     0.493     1.97   0.0570\n3 IMM_3                  0.376     0.511     0.736  0.467 \n4 SAFE_USE_OF_OPIOIDS    1.30      2.42      0.536  0.595 \n5 OP_23                  0.256     0.702     0.365  0.717 \n6 SEP_1                 -0.946     0.845    -1.12   0.271 \n\n\n###Random Forest\nThe second model is Random Forest.\n\nset.seed(seed)\n\n# Set up the tuning grid\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 10)),  # mtry between 1 and 10\n  min_n(range = c(1, 16)), # min_n between 1 and 16\n  levels = 4              # 4 levels for each parameter\n)\n\n# Define the model specification using the ranger engine, with fixed trees at 300\nrf_cv &lt;- rand_forest(\n  mode = \"regression\",\n  mtry = tune(),   \n  min_n = tune(),  \n  trees = 300      \n) %&gt;%\n  set_engine(\"ranger\", seed = seed, importance = \"impurity\") \n\n# Create a workflow\nrf_wf_cv &lt;- workflow() %&gt;%\n  add_model(rf_cv) %&gt;%\n  add_formula(OP_18b ~ HCP_COVID_19 + IMM_3 + SAFE_USE_OF_OPIOIDS + OP_23 + SEP_1)\n\nset.seed(seed)\n# Perform tuning with tune_grid()\nrf_tune_results_cv &lt;- tune_grid(\n  rf_wf_cv,\n  resamples = vfold_cv(care_train, v = 15, repeats = 15),\n  grid = rf_grid,\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE) \n)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\n\n→ A | warning: ! 7 columns were requested but there were 5 predictors in the data.\n               ℹ 5 predictors will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: ! 10 columns were requested but there were 5 predictors in the data.\n               ℹ 5 predictors will be used.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x2   B: x1\nThere were issues with some computations   A: x9   B: x8\nThere were issues with some computations   A: x16   B: x16\nThere were issues with some computations   A: x27   B: x27\nThere were issues with some computations   A: x39   B: x38\nThere were issues with some computations   A: x50   B: x50\nThere were issues with some computations   A: x62   B: x61\nThere were issues with some computations   A: x73   B: x73\nThere were issues with some computations   A: x85   B: x84\nThere were issues with some computations   A: x96   B: x95\nThere were issues with some computations   A: x107   B: x107\nThere were issues with some computations   A: x119   B: x118\nThere were issues with some computations   A: x130   B: x130\nThere were issues with some computations   A: x142   B: x141\nThere were issues with some computations   A: x153   B: x152\nThere were issues with some computations   A: x164   B: x164\nThere were issues with some computations   A: x176   B: x175\nThere were issues with some computations   A: x187   B: x186\nThere were issues with some computations   A: x198   B: x198\nThere were issues with some computations   A: x210   B: x209\nThere were issues with some computations   A: x220   B: x219\nThere were issues with some computations   A: x231   B: x230\nThere were issues with some computations   A: x242   B: x241\nThere were issues with some computations   A: x253   B: x253\nThere were issues with some computations   A: x265   B: x264\nThere were issues with some computations   A: x276   B: x276\nThere were issues with some computations   A: x287   B: x287\nThere were issues with some computations   A: x299   B: x298\nThere were issues with some computations   A: x310   B: x309\nThere were issues with some computations   A: x321   B: x321\nThere were issues with some computations   A: x333   B: x332\nThere were issues with some computations   A: x344   B: x343\nThere were issues with some computations   A: x355   B: x355\nThere were issues with some computations   A: x367   B: x366\nThere were issues with some computations   A: x378   B: x377\nThere were issues with some computations   A: x389   B: x389\nThere were issues with some computations   A: x401   B: x400\nThere were issues with some computations   A: x412   B: x411\nThere were issues with some computations   A: x423   B: x423\nThere were issues with some computations   A: x434   B: x433\nThere were issues with some computations   A: x444   B: x443\nThere were issues with some computations   A: x455   B: x454\nThere were issues with some computations   A: x466   B: x466\nThere were issues with some computations   A: x478   B: x477\nThere were issues with some computations   A: x489   B: x489\nThere were issues with some computations   A: x501   B: x500\nThere were issues with some computations   A: x512   B: x511\nThere were issues with some computations   A: x523   B: x523\nThere were issues with some computations   A: x535   B: x534\nThere were issues with some computations   A: x546   B: x545\nThere were issues with some computations   A: x557   B: x557\nThere were issues with some computations   A: x569   B: x568\nThere were issues with some computations   A: x580   B: x579\nThere were issues with some computations   A: x591   B: x591\nThere were issues with some computations   A: x603   B: x602\nThere were issues with some computations   A: x614   B: x613\nThere were issues with some computations   A: x625   B: x625\nThere were issues with some computations   A: x637   B: x636\nThere were issues with some computations   A: x647   B: x647\nThere were issues with some computations   A: x659   B: x658\nThere were issues with some computations   A: x669   B: x668\nThere were issues with some computations   A: x680   B: x679\nThere were issues with some computations   A: x691   B: x691\nThere were issues with some computations   A: x703   B: x702\nThere were issues with some computations   A: x714   B: x713\nThere were issues with some computations   A: x725   B: x725\nThere were issues with some computations   A: x737   B: x736\nThere were issues with some computations   A: x748   B: x747\nThere were issues with some computations   A: x759   B: x759\nThere were issues with some computations   A: x771   B: x770\nThere were issues with some computations   A: x782   B: x781\nThere were issues with some computations   A: x793   B: x792\nThere were issues with some computations   A: x804   B: x804\nThere were issues with some computations   A: x811   B: x810\nThere were issues with some computations   A: x817   B: x816\nThere were issues with some computations   A: x823   B: x822\nThere were issues with some computations   A: x830   B: x829\nThere were issues with some computations   A: x837   B: x836\nThere were issues with some computations   A: x843   B: x842\nThere were issues with some computations   A: x847   B: x847\nThere were issues with some computations   A: x851   B: x850\nThere were issues with some computations   A: x855   B: x854\nThere were issues with some computations   A: x858   B: x857\nThere were issues with some computations   A: x862   B: x862\nThere were issues with some computations   A: x867   B: x866\nThere were issues with some computations   A: x872   B: x872\nThere were issues with some computations   A: x877   B: x876\nThere were issues with some computations   A: x881   B: x880\nThere were issues with some computations   A: x885   B: x884\nThere were issues with some computations   A: x888   B: x887\nThere were issues with some computations   A: x892   B: x891\nThere were issues with some computations   A: x900   B: x899\nThere were issues with some computations   A: x900   B: x900\n\n#show_notes(rf_tune_results_cv_tt)\nautoplot(rf_tune_results_cv)\n\n\n\n\n\n\n\n\n\nbest_rf_params &lt;- rf_tune_results_cv %&gt;% select_best(metric = \"rmse\")\n\nrf_wf_final &lt;- finalize_workflow(rf_wf_cv, best_rf_params)\n\n\n# Fit the final random forest model on the training dataset\nrf_model_final &lt;- fit(rf_wf_final, data = care_train)\n\n# Generate predictions on the training set and combine them with the actual outcomes\nrf_preds &lt;- predict(rf_model_final, care_train) %&gt;%\n  bind_cols(care_train)\n\n# Create the observed vs predicted plot for the random forest model\nggplot(rf_preds, aes(x = OP_18b, y = .pred)) +\n  geom_point(color = \"red\", alpha = 0.7) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"Observed vs Predicted: Random Forest Model\",\n       x = \"Observed Quality Score\",\n       y = \"Predicted Quality Score\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Summarize the tuning results by RMSE\nrf_tune_results_cv %&gt;% \n  collect_metrics() %&gt;% \n  filter(.metric == \"rmse\") %&gt;% \n  arrange(mean) %&gt;% \n  print(n = Inf)\n\n# A tibble: 16 × 8\n    mtry min_n .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1    16 rmse    standard    25.5   225   0.811 Preprocessor1_Model13\n 2     1    11 rmse    standard    25.7   225   0.809 Preprocessor1_Model09\n 3     4    16 rmse    standard    25.8   225   0.822 Preprocessor1_Model14\n 4     1     6 rmse    standard    25.9   225   0.807 Preprocessor1_Model05\n 5     7    16 rmse    standard    26.0   225   0.823 Preprocessor1_Model15\n 6    10    16 rmse    standard    26.0   225   0.823 Preprocessor1_Model16\n 7     4    11 rmse    standard    26.0   225   0.815 Preprocessor1_Model10\n 8     1     1 rmse    standard    26.2   225   0.802 Preprocessor1_Model01\n 9     7    11 rmse    standard    26.2   225   0.814 Preprocessor1_Model11\n10    10    11 rmse    standard    26.2   225   0.814 Preprocessor1_Model12\n11     4     6 rmse    standard    26.3   225   0.803 Preprocessor1_Model06\n12     7     6 rmse    standard    26.5   225   0.799 Preprocessor1_Model07\n13    10     6 rmse    standard    26.5   225   0.799 Preprocessor1_Model08\n14     4     1 rmse    standard    26.6   225   0.798 Preprocessor1_Model02\n15     7     1 rmse    standard    26.8   225   0.796 Preprocessor1_Model03\n16    10     1 rmse    standard    26.8   225   0.796 Preprocessor1_Model04\n\n\n\nrf_best_result &lt;- rf_tune_results_cv %&gt;% select_best(metric = \"rmse\")\nrf_best_result\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     1    16 Preprocessor1_Model13\n\nrf_tune_results_cv %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean) %&gt;%\n  slice(1)\n\n# A tibble: 1 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     1    16 rmse    standard    25.5   225   0.811 Preprocessor1_Model13\n\n\n\n\nLASSO Regression\n\nset.seed(seed)\nlasso_grid &lt;- grid_regular(\n  penalty(range = c(0.0001, 1)),  ## from 0.0001 to 1 (no log)\n  levels = 30                # 30 grid points (fine search)\n)\n#setting up Lasso model specification\nlasso_spec &lt;- linear_reg(\n  penalty = 0.1,  #Regularization strength \n  mixture = 1     # 1 = Lasso\n) %&gt;% \n  set_engine(\"glmnet\")\n\n\n#fitting Lasso model\nlasso_fit &lt;- lasso_spec %&gt;% \n  fit(OP_18b ~ HCP_COVID_19 + IMM_3 + SAFE_USE_OF_OPIOIDS + OP_23 + SEP_1, \n      data = care_train)\n\n# Example lasso workflow\nlasso_wf &lt;- workflow() %&gt;%\n  add_model(lasso_spec) %&gt;%   \n  add_formula(OP_18b ~ HCP_COVID_19 + IMM_3 + SAFE_USE_OF_OPIOIDS + OP_23 + SEP_1 )   \n\n\n# Cross-validation\nlasso_cv_results &lt;- tune_grid(\n  lasso_wf,\n  resamples = vfold_cv(care_train, v = 15),\n  grid = lasso_grid,                 # if you tuned lambda\n  metrics = metric_set(rmse),\n  control = control_grid(save_pred = TRUE)\n)\n\nWarning: No tuning parameters have been detected, performance will be evaluated\nusing the resamples with no tuning. Did you want to [tune()] parameters?\n\n\nWarning: package 'glmnet' was built under R version 4.4.3\n\n# Get the best RMSE\nlasso_cv_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean) %&gt;%\n  slice(1)\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    24.8    15    3.51 Preprocessor1_Model1\n\n# coefficients (non-zero ones only)\ntidy(lasso_fit) %&gt;% \n  filter(estimate != 0) %&gt;% \n  arrange(desc(abs(estimate)))\n\n# A tibble: 6 × 3\n  term                estimate penalty\n  &lt;chr&gt;                  &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          131.        0.1\n2 SAFE_USE_OF_OPIOIDS    1.25      0.1\n3 HCP_COVID_19           0.964     0.1\n4 SEP_1                 -0.930     0.1\n5 IMM_3                  0.369     0.1\n6 OP_23                  0.238     0.1\n\n\n\n#making predictions\ntrain_results &lt;- care_train%&gt;% \n  bind_cols(predict(lasso_fit, new_data = care_train))\n\n#evaluating performance\nlasso_metrics &lt;- train_results %&gt;% \n  metrics(truth = OP_18b , estimate = .pred)\n#Print \nprint(lasso_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      23.8  \n2 rsq     standard       0.190\n3 mae     standard      18.4  \n\n\n\n#Observed vs Predicted plot\nggplot(train_results, aes(x = OP_18b, y = .pred)) +\n  geom_point(alpha = 0.5, color = \"pink\") +\n  geom_abline(slope = 1, linetype = \"dashed\") +\n  labs(title = \"Lasso Regression Performance\",\n       subtitle = paste(\"Penalty =\", lasso_fit$spec$args$penalty),\n       x = \"Observed Scores\", \n       y = \"Predicted Scores\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Linear Regression RMSE\nlm_rmse &lt;- care_lm_cv_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  summarize(mean_rmse = mean(mean)) %&gt;%\n  pull(mean_rmse)\n\n# Lasso RMSE\nlasso_rmse &lt;- lasso_cv_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  summarize(mean_rmse = mean(mean)) %&gt;%\n  pull(mean_rmse)\n\n# Random Forest RMSE\nrf_rmse &lt;- rf_tune_results_cv %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean) %&gt;%\n  slice(1) %&gt;%\n  pull(mean)\n\n\n# Create a comparison table\nrmse_results &lt;- tibble(\n  Model = c(\"Linear Regression\", \"Lasso\", \"Random Forest\"),\n  RMSE = c(lm_rmse, lasso_rmse, rf_rmse)\n)\n\n# Print the table\nprint(rmse_results)\n\n# A tibble: 3 × 2\n  Model              RMSE\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Linear Regression  24.8\n2 Lasso              24.8\n3 Random Forest      25.5\n\n\n\nggplot(rmse_results, aes(x = reorder(Model, RMSE), y = RMSE)) +\n  geom_segment(aes(xend = Model, y = 0, yend = RMSE), color = \"grey\") +\n  geom_point(size = 4, color = \"blue\") +\n  geom_text(aes(label = round(RMSE, 2)), vjust = -1, size = 3.5) +\n  labs(\n    title = \" RMSE by Model\",\n    x = \"Model\",\n    y = \"RMSE\"\n  ) +\n  theme_minimal()+\n  coord_flip()\n\n\n\n\n\n\n\n\nFrom the RMSE, Linear regression perform best model fit.\n\n\nFinal Model\nBased on the RMSE, Linear Regression outformed the other models. The model can be used to looking at the effect of vaciination on waiting time in the emergency department cross the United States.\nAs a final step, I will fit the model into test data using linear model.\n\n# Make predictions on test data\nlm_test_preds &lt;- predict(care_lm_train , new_data = care_test) %&gt;%\n  bind_cols(care_test)\n\n# Calculate RMSE\nlm_test_rmse &lt;- rmse(lm_test_preds, truth = OP_18b, estimate = .pred)\n\n# Print RMSE\ncat(\"Test RMSE:\", lm_test_rmse$.estimate, \"\\n\")\n\nTest RMSE: 62.10533 \n\n\nI wnat to show the predicted vs observed value both train and test data\n\ncare_lm_preds &lt;- care_lm_preds %&gt;%\n  mutate(dataset = \"Train\")\n\nlm_test_preds &lt;- lm_test_preds %&gt;%\n  mutate(dataset = \"Test\")\n# combined predicted values \n\ncombined_preds &lt;- bind_rows(care_lm_preds, lm_test_preds)\n\n #4. Now plot\nggplot(combined_preds, aes(x = OP_18b, y = .pred, color = dataset)) +\n  geom_point(size = 2) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"black\") +\n  labs(\n    title = \"Observed vs Predicted Waiting Time (Train and Test)\",\n    x = \"Observed Waiting Time (OP_18b)\",\n    y = \"Predicted Waiting Time\",\n    color = \"Dataset\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n###Discussion\nThree models were performed in this project: Linear Regression, Lasso, and Random Forest. Among the models, Linear Regression achieved the lowest RMSE (24.77), closely followed by Lasso (24.79), while Random Forest had a higher RMSE (25.46). This indicates that Linear Regression performed the best, yielding the most accurate predictions on the dataset compared to Lasso and Random Forest, although the difference between Linear Regression and Lasso was very small.\nThe scatterplot comparing observed and predicted emergency department (ED) waiting times of the final model (linear model) illustrates the performance of the linear regression model on both training and testing datasets. In general, the predicted values follow the trend of the observed values, although considerable variability remains. Most predictions for mid-range observed waiting times (approximately 120 to 180 minutes) are relatively close to the ideal 45-degree line, indicating reasonable model calibration in this range. However, for states with higher observed waiting times (above 200 minutes), the model consistently underestimates the true waiting times, suggesting limited ability to capture extreme values. Notably, the pattern of errors between the training and testing sets is similar, indicating that the model is not overfitting the training data. Nonetheless, the dispersion around the ideal line and underprediction at high observed values point to opportunities for model improvement, such as using more flexible modeling approaches like Random Forest or incorporating additional predictors to better account for variation in ED waiting times."
  }
]