[
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "The data is obtained from The University of Michigan Health and Retirement Study (HRS). HRS is a longitudinal panel study that surveys a representative sample of approximately 20,000 people in America, supported by the National Institute on Aging (NIA U01AG009740) and the Social Security Administration.\nThrough its unique and in-depth interviews, the HRS provides an invaluable and growing body of multidisciplinary data that researchers can use to address important questions about the challenges and opportunities of aging.\nPlease visit: https://hrsdata.isr.umich.edu/data-products/public-survey-data?_gl=18ofhfg_gaMTc5MDM4ODE3NS4xNzM4MzU5MjU5_ga_FF28MW3MW2*MTczODM1OTI1OS4xLjEuMTczODM1OTI3NC4wLjAuMA..\nThe data is in STATA format.\nI plan to conduct survival analysis for retired people who had high Cholesterol in 2014 until they Heart Attack."
  },
  {
    "objectID": "data-exercise/data-exercise.html#select-variables-needed-for-each-year",
    "href": "data-exercise/data-exercise.html#select-variables-needed-for-each-year",
    "title": "Data Exercise",
    "section": "Select variables needed for each year",
    "text": "Select variables needed for each year\nVariables that needed in this study include Household Identification number, Respondent Person Identification number, High Cholesterol Status, Years first had heart attack, Month first had heart attack, Ever had heart attack.\n\n#data2014\nda2014 &lt;- df2014 %&gt;% \n  select(\"HHID\", \"PN\", \"OC283\", \"OC040\", \"OC258\", \"OC259\") %&gt;%\n  rename(hhid = HHID, pn = PN, chol= OC283, ha = OC040, year = OC258, month= OC259)\n\n\nda2014 &lt;- lapply(da2014, function(x) { attributes(x) &lt;- NULL; x }) #delete the column lables from dataset, because their position is incorrect after deleting some variables. \nda2014 &lt;- as.data.frame(da2014)  # Convert back to a dataframe\n\n#data2016\nda2016 &lt;- df2016 %&gt;% \n  select(\"HHID\", \"PN\", \"PC283\", \"PC040\", \"PC258\", \"PC259\") %&gt;%\n  rename(hhid = HHID, pn = PN, chol= PC283, ha = PC040, year = PC258, month= PC259)\n\n\nda2016 &lt;- lapply(da2016, function(x) { attributes(x) &lt;- NULL; x }) #delete the column lables from dataset, because their position is incorrect after deleting some variables. \nda2016 &lt;- as.data.frame(da2016)  # Convert back to a dataframe\n\n#data2018 \nda2018 &lt;- df2018 %&gt;%\n  select(\"hhid\", \"pn\", \"QC283\", \"QC040\", \"QC258\", \"QC259\") %&gt;%\n  rename( chol = QC283, ha= QC040, year = QC258, month = QC259) # I rename the column names because in this year, they use different name. \n\nda2018 &lt;- lapply(da2018, function(x) { attributes(x) &lt;- NULL; x }) #delete the column lables from dataset, because their position is incorrect after deleting some variables. \nda2018 &lt;- as.data.frame(da2018)  # Convert back to a dataframe\n\nda2020 &lt;- df2020 %&gt;%\n  select(\"HHID\", \"PN\", \"RC283\", \"RC040\", \"RC258\", \"RC259\") %&gt;%\n  rename( hhid = HHID, pn = PN, chol = RC283, ha= RC040, year = RC258, month = RC259) # I rename the column names because in this year, they use different name. \n\nda2022 &lt;- df2022 %&gt;%\n  select(\"HHID\", \"PN\", \"SC283\", \"SC040\", \"SC258\", \"SC259\") %&gt;%\n  rename( hhid = HHID, pn = PN, chol = SC283, ha= SC040, year = SC258, month = SC259) # I rename the column names because in this year, they use different name."
  },
  {
    "objectID": "data-exercise/data-exercise.html#combine-dataset",
    "href": "data-exercise/data-exercise.html#combine-dataset",
    "title": "Data Exercise",
    "section": "Combine dataset",
    "text": "Combine dataset\n\n# Perform a full join step-by-step\nmerged_data &lt;- da2014 %&gt;%\n  full_join(da2016, by = c(\"hhid\", \"pn\"), suffix = c(\"_2014\", \"_2016\")) %&gt;%\n  full_join(da2018, by = c(\"hhid\", \"pn\"), suffix = c(\"\", \"_2018\")) %&gt;%\n  full_join(da2020, by = c(\"hhid\", \"pn\"), suffix = c(\"\", \"_2020\")) %&gt;%\n  full_join(da2022, by = c(\"hhid\", \"pn\"), suffix = c(\"\", \"_2022\")) \n\n# View result\nhead(merged_data)\n\n    hhid  pn chol_2014 ha_2014 year_2014 month_2014 chol_2016 ha_2016 year_2016\n1 000003 020         5      NA        NA         NA        NA      NA        NA\n2 010001 010         1      NA        NA         NA        NA      NA        NA\n3 010003 030         1      NA        NA         NA        NA      NA        NA\n4 010004 040         5      NA        NA         NA        NA      NA        NA\n5 010013 040         1      NA        NA         NA        NA      NA        NA\n6 010013 010         1       5      1995         NA        NA      NA       995\n  month_2016 chol ha year month chol_2020 ha_2020 year_2020 month_2020\n1         NA   NA NA   NA    NA        NA      NA        NA         NA\n2         NA   NA NA   NA    NA        NA      NA        NA         NA\n3         NA    1 NA   NA    NA         1      NA        NA         NA\n4         NA    5 NA   NA    NA         1      NA        NA         NA\n5         NA    1 NA   NA    NA         1      NA        NA         NA\n6         NA   NA NA   NA    NA        NA      NA        NA         NA\n  chol_2022 ha_2022 year_2022 month_2022\n1        NA      NA        NA         NA\n2         5      NA        NA         NA\n3         5       5        NA         NA\n4         1      NA        NA         NA\n5         5      NA        NA         NA\n6        NA      NA        NA         NA\n\n\nI realize that we only need cholesterol status in 2014, therefore, I want to delete cholesterol in 2016 and 2018\n\ndf &lt;- merged_data[, !(colnames(merged_data) %in% c(\"chol_2016\", \"chol\", \"chol_2020\", \"chol_2022\"))] %&gt;% # deleting cholesterol from 2016 and 2018 \n  rename(ha_2018 = ha, year_2018 =year, month_2018 = month) # I rename variables from 2018"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "About the dataset\nThe Botswana Combination Prevention Project (BCPP) was a collaborative research effort led by the Botswana Ministry of Health (MOH), the Harvard School of Public Health/Botswana Harvard AIDS Institute Partnership (BHP), and the U.S. Centers for Disease Control and Prevention (CDC). This community-based randomized trial aimed to assess the impact of various HIV prevention strategies on reducing HIV incidence across 15 intervention and 15 control communities. The intervention communities received comprehensive HIV testing, linkage to care, and universal treatment services, guided by the UNAIDS 90-90-90 targets: ensuring 90% of individuals with HIV are aware of their status, 90% of those diagnosed are on antiretroviral therapy (ART), and 90% of those on ART achieve viral suppression.\nThe BCPP study was structured around two interrelated protocols: the Evaluation Protocol and the Intervention Protocol. The Evaluation Protocol assessed the primary outcome—HIV incidence—along with key secondary outcomes, focusing on data collected from the Baseline Household Survey, the HIV Incidence Cohort, and the End of Study Survey. Meanwhile, the Intervention Protocol involved the implementation of a combination prevention (CP) package in combination prevention communities (CPCs), monitoring the uptake of interventions such as expanded HIV testing and counseling, enhanced male circumcision services, and improved access to HIV care and treatment.\nThe dataset is available and free to download at CDC Website:https://data.cdc.gov/Global-Health/Botswana-Combination-Prevention-Project-BCPP-Publi/qcw5-4m9q/about_data.\nThe study is a cohort study with 3 phases. In this exercise, I only work on year 3 dataset. There are many information covered in this dataset, including demographic information of the respondent, soccioeconomic factors, HIV exposure, HIV status and conditions.\n\n\nInstall all packages needed\nInstall and library all packages needed in this section.\n\ninstall.packages(\"tidyverse\")\n\nThe following package(s) will be installed:\n- tidyverse [2.0.0]\nThese packages will be installed into \"~/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio/renv/library/macos/R-4.4/x86_64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidyverse ...                      OK [linked from cache]\nSuccessfully installed 1 package in 10 milliseconds.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ninstall.packages(\"ggplot2\")\n\nThe following package(s) will be installed:\n- ggplot2 [3.5.1]\nThese packages will be installed into \"~/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio/renv/library/macos/R-4.4/x86_64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggplot2 ...                        OK [linked from cache]\nSuccessfully installed 1 package in 8.4 milliseconds.\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/rayleenlewis/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio\n\ninstall.packages(\"patchwork\")  # This package is to redefine \"/\" operator for plot arrangement\n\nThe following package(s) will be installed:\n- patchwork [1.3.0]\nThese packages will be installed into \"~/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio/renv/library/macos/R-4.4/x86_64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing patchwork ...                      OK [linked from cache]\nSuccessfully installed 1 package in 8.5 milliseconds.\n\nlibrary(patchwork)\ninstall.packages(\"writexl\")\n\nThe following package(s) will be installed:\n- writexl [1.5.1]\nThese packages will be installed into \"~/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio/renv/library/macos/R-4.4/x86_64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing writexl ...                        OK [linked from cache]\nSuccessfully installed 1 package in 8.2 milliseconds.\n\nlibrary(writexl)\nlibrary(haven)\nlibrary(dplyr)\ninstall.packages(\"janitor\")\n\nThe following package(s) will be installed:\n- janitor [2.2.1]\nThese packages will be installed into \"~/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio/renv/library/macos/R-4.4/x86_64-apple-darwin20\".\n\n# Installing packages --------------------------------------------------------\n- Installing janitor ...                        OK [linked from cache]\nSuccessfully installed 1 package in 9.4 milliseconds.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(procs)\n\nLoading required package: common\n\n\n\n\nLoading the dataset\n\n## set path  dictionary \nbostwana &lt;- here::here(\"cdcdata-exercise\",\"data\", \"cdcbostwana.csv\") # set the pathway to create relative path \n\nbostwana &lt;- read_csv(bostwana) # read the dataset stored in specified path\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 11369 Columns: 322\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (217): de_subj_idC, de_hh_idC, de_plot_idC, region, random_arm, survey, ...\ndbl  (64): community_rndmN, pair_rndmN, interview_days, yeardone, age_at_int...\nlgl  (41): religion_name, religious_affil, relig_theogrp, ethnicity, prev_hi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(bostwana)\n\n# A tibble: 6 × 322\n  de_subj_idC de_hh_idC de_plot_idC region community_rndmN pair_rndmN random_arm\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     \n1 10011A      16863A    14700A      North…              25         13 Intervent…\n2 1001A       8552A     17400A      South…               4          1 Intervent…\n3 10021A      13235A    4578A       Centr…               5          8 Standard …\n4 10023A      7028A     7820A       Centr…               9          4 Standard …\n5 10026A      5500A     4704A       South…              19          3 Intervent…\n6 10032A      2667A     8896A       South…              14          2 Standard …\n# ℹ 315 more variables: interview_days &lt;dbl&gt;, yeardone &lt;dbl&gt;, survey &lt;chr&gt;,\n#   gender &lt;chr&gt;, age_at_interview &lt;dbl&gt;, age5cat &lt;chr&gt;,\n#   length_residence &lt;chr&gt;, permanent_resident &lt;chr&gt;, intend_residency &lt;chr&gt;,\n#   nights_away &lt;chr&gt;, cattle_postlands &lt;chr&gt;, religion_name &lt;lgl&gt;,\n#   religious_affil &lt;lgl&gt;, relig_theogrp &lt;lgl&gt;, ethnicity &lt;lgl&gt;,\n#   marital_status &lt;chr&gt;, num_wives &lt;dbl&gt;, husband_wives &lt;dbl&gt;,\n#   live_alone &lt;chr&gt;, livewith_family &lt;chr&gt;, livewith_partner &lt;chr&gt;, …\n\ndim(bostwana)\n\n[1] 11369   322\n\n\nThe dataset contains 11369 observations and 322 columns.\n\n\nData Cleaning\nI want to select interesting variables for further analysis and clean the data as needed including dropping/ lablig missing values.\n\nbost_df &lt;- bostwana %&gt;% \n  select(\"region\", \"random_arm\", \"gender\", \"age_at_interview\", \"age5cat\", \"employment_status\", \"circumcised\", \"hiv_status_current\", \"hiv_status_time\", \"viral_load_yr3\",  \"cd4_survey_yr3\", \"cd4_surveydays_yr3\", \"arv_duration_days\")\n\nhead(bost_df)\n\n# A tibble: 6 × 13\n  region   random_arm       gender age_at_interview age5cat    employment_status\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            \n1 Northern Intervention     M                  45.7 45-54 yea… Unemployed not l…\n2 Southern Intervention     F                  51.8 45-54 yea… Unemployed looki…\n3 Central  Standard of Care F                  40.9 35-44 yea… Unemployed looki…\n4 Central  Standard of Care F                  18.7 16-24 yea… Unemployed not l…\n5 Southern Intervention     F                  35.8 35-44 yea… Employed         \n6 Southern Standard of Care F                  56.9 55-64 yea… Unemployed looki…\n# ℹ 7 more variables: circumcised &lt;chr&gt;, hiv_status_current &lt;chr&gt;,\n#   hiv_status_time &lt;chr&gt;, viral_load_yr3 &lt;dbl&gt;, cd4_survey_yr3 &lt;dbl&gt;,\n#   cd4_surveydays_yr3 &lt;dbl&gt;, arv_duration_days &lt;dbl&gt;\n\nsummary(bost_df)\n\n    region           random_arm           gender          age_at_interview\n Length:11369       Length:11369       Length:11369       Min.   :17.20   \n Class :character   Class :character   Class :character   1st Qu.:26.70   \n Mode  :character   Mode  :character   Mode  :character   Median :35.70   \n                                                          Mean   :38.03   \n                                                          3rd Qu.:48.20   \n                                                          Max.   :67.90   \n                                                                          \n   age5cat          employment_status  circumcised        hiv_status_current\n Length:11369       Length:11369       Length:11369       Length:11369      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n hiv_status_time    viral_load_yr3    cd4_survey_yr3   cd4_surveydays_yr3\n Length:11369       Min.   :     40   Min.   :  20.0   Min.   : 358      \n Class :character   1st Qu.:     40   1st Qu.: 401.0   1st Qu.: 875      \n Mode  :character   Median :     40   Median : 573.0   Median : 895      \n                    Mean   :   4305   Mean   : 602.1   Mean   : 881      \n                    3rd Qu.:     40   3rd Qu.: 757.0   3rd Qu.: 911      \n                    Max.   :2243242   Max.   :1567.0   Max.   :1184      \n                    NA's   :8130      NA's   :10594    NA's   :8032      \n arv_duration_days\n Min.   :   0     \n 1st Qu.: 908     \n Median :2393     \n Mean   :2432     \n 3rd Qu.:3764     \n Max.   :5879     \n NA's   :8222     \n\n\nNote: To make easy for further analysis, I selected several variables:\n\nregion = region of sbject\nrandom_arm = randomized arm\nage= age\nage5cat = age category\nemployment_status = employment status\nhiv_status_current = Current HIV status\nviral_load_yr3 = viral load in year 3 of the study\ncd4_survey_yr3 = CD4 count in year 3\ncd4_surveydays_yr3 = numbers of days enrolled to survey year 3.\narv_duration_days = number of day (duration) taking ARV\n\nIn this excercise, I would like to see the relationship between viral load, lenght of enrollment, ARV duration, and CD4 count. Therefore, I will drop all missing data in those variables.\n\ndata &lt;- bost_df %&gt;% \n  drop_na(viral_load_yr3, cd4_survey_yr3, cd4_surveydays_yr3, arv_duration_days) %&gt;% # I drop all observation wit N/A data\n  filter(viral_load_yr3 != 0, \n         cd4_survey_yr3 != 0, \n         cd4_surveydays_yr3 != 0, \n         arv_duration_days !=0) # I found some observation contain 0 I drop those observation\n\n\nsapply(data, class) # I want to check class of all variables \n\n            region         random_arm             gender   age_at_interview \n       \"character\"        \"character\"        \"character\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n       \"character\"        \"character\"        \"character\"        \"character\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n       \"character\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nFor N/A information in categorical data, I lable those missing value wih 999. However, the values are in characters. I want to change the values into factors to make us easy in analysis.\n\ndata &lt;- data %&gt;% \n  mutate(region = recode(region, \n                         \"Northern\" = 0,\n                         \"Southern\" = 1, \n                         \"Central\" = 2), \n         random_arm = recode(random_arm, \n                             \"Standard of Care\" = 0,\n                             \"Intervention\" = 1), \n         gender = recode(gender,\n                         \"M\" = 0,\n                         \"F\" =1), \n         age5cat = recode (age5cat, \n                           \"16-24 years\" = 0, \n                           \"25-34 years\" = 1, \n                           \"35-44 years\" = 2, \n                           \"45-54 years\" = 3, \n                           \"55-64 years\" = 4), \n         employment_status = recode(employment_status , \n                                    \"Employed\" = 0, \n                                    \"Unemployed looking for work\" = 1, \n                                    \"Unemployed not looking for work\" = 2), \n         circumcised= recode(circumcised , \n                             \"No\" = 0, \n                             \"Yes\" = 1), \n         hiv_status_current = recode(hiv_status_current, \n                                     \"HIV-uninfected\" = 0, \n                                     \"HIV-infected\" = 1, \n                                     \"Refused HIV testing\" = 2), \n         hiv_status_time = recode(hiv_status_time, \n                                  \"HIV-negative\" = 0, \n                                  \"HIV-positive: previously diagnosed\" = 1, \n                                  \"Refused HIV testing\" = 2, \n                                  \"HIV-positive: newly discovered\" = 3)) # this part is to recode from character to numeric to allow us convert ito factors. \n\nsapply(data, class) # check character of the variables \n\n            region         random_arm             gender   age_at_interview \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nNow, I want to convert categorical variables into factors\n\ndata &lt;- data %&gt;%\n  mutate(across (c(\"region\", \"random_arm\", \"gender\", \"age5cat\", \"employment_status\", \"circumcised\", \"hiv_status_current\",  \"hiv_status_time\"), as.factor)) # this is to convert multiple variables into factors. as.factor() only can work in single variable. \n\nsapply(data, class) # check character of the variables \n\n            region         random_arm             gender   age_at_interview \n          \"factor\"           \"factor\"           \"factor\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n          \"factor\"           \"factor\"           \"factor\"           \"factor\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n          \"factor\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nI want to replace all N/A with 999 in categorical variables.\n\ndata &lt;- data %&gt;%\n  mutate(across(where(is.factor), ~ replace_na(as.character(.), \"999\"))) %&gt;% # replace the values\n  mutate(across(where(is.character), as.factor)) # convert the variable back to factor. \n\ndim(data)\n\n[1] 539  13\n\n\nThe data is clean now and ready for data exploration and analysis. There are 539 obserations and 13 variables in the final data.\n\nsummary_stats &lt;- data %&gt;%\n  summarise(\n    mean_viral_load = mean(viral_load_yr3, na.rm = TRUE),\n    sd_viral_load = sd(viral_load_yr3, na.rm = TRUE),\n    mean_cd4 = mean(cd4_survey_yr3, na.rm = TRUE),\n    sd_cd4 = sd(cd4_survey_yr3, na.rm = TRUE)\n  )\nprint(summary_stats)\n\n# A tibble: 1 × 4\n  mean_viral_load sd_viral_load mean_cd4 sd_cd4\n            &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1           1784.        13182.     620.   279.\n\n\nNow I want to make summary statistics for Viral load and visualize it ina plot.\n\nsummary_by_arm &lt;- data %&gt;%\n  group_by(random_arm) %&gt;%\n  summarise(\n    mean_cd4 = mean(cd4_survey_yr3, na.rm = TRUE),\n    sd_cd4 = sd(cd4_survey_yr3, na.rm = TRUE)\n  ) # create by randomise_arm \n\nplot1 &lt;- ggplot(summary_by_arm, aes(x = random_arm, y = mean_cd4, fill = random_arm)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  labs(title = \"Mean CD4 Count by randomise group with Standard Deviation\",\n       x = \"Groups\", y = \"Mean CD4 Count\", \n       caption = \"Red (0)= Standard of Care, Green (1)= Intervention \") +\n  theme_minimal() +\n  theme(plot.background = element_rect(color = \"black\", size = 1)) \n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nprint(plot1)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"CD4 Mean and SD .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot1) # save the pictures created \n\nSaving 7 x 5 in image\n\n\nNow I want to make summary statistics for viral load and visualize it in a plot.\n\nsummary_by_arm_vl &lt;- data %&gt;%\n  group_by(random_arm) %&gt;%\n  summarise(\n    mean_vl = mean(viral_load_yr3, na.rm = TRUE),\n    sd_vl = sd(viral_load_yr3, na.rm = TRUE)\n  ) # create by randomise_arm \n\nplot2 &lt;- ggplot(summary_by_arm_vl, aes(x = random_arm, y = mean_vl, fill = random_arm)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_vl - sd_vl, ymax = mean_vl + sd_vl), width = 0.2) +\n  labs(title = \"Mean Viral Load Count by randomise group with Standard Deviation\",\n       x = \"Groups\", y = \"Mean Viral Load\", \n       caption = \"Red (0)= Standard of Care, Green (1)= Intervention \") +\n  theme_minimal() +\n  theme(plot.background = element_rect(color = \"black\", size = 1)) \n\nprint(plot2)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"Viral Load Mean and SD .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot2) # save the pictures created\n\nSaving 7 x 5 in image\n\n\n\n\nData Exploration\nIn this part, I want to explore the data and vizualise it before data analysis.\n\nplot3 &lt;- ggplot(data, aes(x = viral_load_yr3, y = cd4_survey_yr3, color = region)) + \n  geom_point(size = 3) +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"Viral Load VS CD4 count based on the region\", \n       x = \"Viral load\", \n       y = \"CD4\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) + # Increase size and boldness of title and axes\n  scale_color_manual(values = c(\"0\" = \"#53d127\", \"1\" = \"#2ce1b0\", \"2\" = \"#ff5733\"))+ # crete color manually \n  theme(plot.background = element_rect(color = \"black\", size = 1))\n\nprint(plot3)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"Viral Load VS CD4 based on region .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot3) # save the pictures created\n\nSaving 7 x 5 in image\n\n\n\nplot4 &lt;- ggplot(data, aes(x = arv_duration_days , y = cd4_survey_yr3, color = region)) + \n  geom_boxplot() +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"ARV Duration VS CD4 count based on the region\", \n       x = \"ARV Duration (days)\", \n       y = \"CD4 count\",\n       caption = \"Green = Nothern, Blue= Northern, and red= Central\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) + # Increase size and boldness of title and axes\n  theme(plot.background = element_rect(color = \"black\", size = 1)) +\n  scale_x_continuous(limits = c(0, 1000)) + # Set x-axis maximum limit to 1000\n  scale_color_manual(values = c(\"0\" = \"#53d127\", \"1\" = \"#2ce1b0\", \"2\" = \"#ff5733\")) # crete color manually \n\nprint(plot4)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"ARV Duration VS CD4 count based on the region .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot4) # save the pictures created\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`stat_boxplot()`).\n\n\n\n\nThis section contributed by Rayleen Lewis.\n\nPerforming a few more checks\nPrior to creating the synthetic dataset, I wanted to check on a few more things with the data, like number of persons in each region and arm of the study. It also looks like most people are virally suppressed, so I wanted to look at the viral load among people who had values &gt;200. I also want to get the median and IQR of CD4 count by region.\n\n#Number of persons in each region\ntable(data$region)\n\n\n  0   1   2 \n160 192 187 \n\n#Number of persons in each arm of the study\ntable(data$random_arm)\n\n\n  0   1 \n239 300 \n\n#Confirming that basically everyone is virally suppressed\ndata_supp &lt;- data %&gt;%\n  filter(viral_load_yr3&gt;200)\nggplot(data_supp, aes(x = viral_load_yr3)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ndata_rl &lt;- data %&gt;%\n  mutate(viral_2 = if_else(viral_load_yr3 &lt; 200, 0, 1))\ntable(data_rl$viral_2)\n\n\n  0   1 \n510  29 \n\n#95% are virally suppressed (viral load &lt;200)\n\n#Mean (SD) of CD4 count by arm\nproc_means(data, var = cd4_survey_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN   STD   MIN   MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 cd4_survey_yr3   539  620.  279.    70  1567\n2 0         1   239 cd4_survey_yr3   239  604.  265.    70  1540\n3 1         1   300 cd4_survey_yr3   300  632.  290.    78  1567\n\nproc_means(data, var = viral_load_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN    STD   MIN    MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 viral_load_yr3   539 1784. 13182.    40 177075\n2 0         1   239 viral_load_yr3   239 3256. 19241.    40 177075\n3 1         1   300 viral_load_yr3   300  610.  3854.    40  37415\n\n\n\n\nGenerating synthetic data\nI aimed to reproduce a dataset similar to the dataset above. The following prompt was used in ChatGPT to generate the code in the next code chunk: using R, create a dataset with 539 observations and variables called region with 3 levels (values 0, 1, 2), arm with 2 levels (0, 1), cd4 (continuous), viral_load (continuous). 55% of observations should have an arm value of 1. 95% of the values for viral_load should be &lt;200. Among observations with arm = 0, viral_load should have a mean of 3256 and standard deviation of 19241 and cd4 should have a mean of 604 and standard deviation of 265. Among observations with arm = 1, viral_load should have a mean of 610 and standard deviation of 3854 and cd4 should have a mean of 632 and standard deviation of 290. 160 observations should have a region value of 0, 192 should have a region value equal to 1 and the rest should have a region value equal to 2.\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- ifelse(arm == 0,\n                     rnorm(sum(arm == 0), mean = 3256, sd = 19241),\n                     rnorm(sum(arm == 1), mean = 610, sd = 3854))\n\n# Ensure 95% of viral_load values are &lt;200\nthreshold &lt;- quantile(viral_load, 0.95)\nviral_load &lt;- ifelse(viral_load &gt; 200, runif(length(viral_load), min = 0, max = 200), viral_load)\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load       \n 0:160   0:228   Min.   :-251.6   Min.   :-48569.28  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.: -3267.70  \n 2:187           Median : 637.1   Median :    24.23  \n                 Mean   : 625.9   Mean   : -3507.56  \n                 3rd Qu.: 800.0   3rd Qu.:   125.58  \n                 Max.   :1518.6   Max.   :   198.68  \n\n\nI noticed that &gt;25% of the viral load values were negative. I needed to update the code to set the minimum value to 40 (based on the original dataset). To update the code, I used the following follow-up prompt: can you update the code so that the min value for viral_load is 40?\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- ifelse(arm == 0,\n                     rnorm(sum(arm == 0), mean = 3256, sd = 19241),\n                     rnorm(sum(arm == 1), mean = 610, sd = 3854))\n\n# Ensure 95% of viral_load values are &lt;200 while maintaining a minimum of 40\nthreshold &lt;- quantile(viral_load, 0.95)\nviral_load &lt;- ifelse(viral_load &gt; 200, runif(length(viral_load), min = 40, max = 200), viral_load)\nviral_load &lt;- pmax(viral_load, 40)  # Ensure minimum value is 40\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load    \n 0:160   0:228   Min.   :-251.6   Min.   : 40.00  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.: 40.00  \n 2:187           Median : 637.1   Median : 59.39  \n                 Mean   : 625.9   Mean   : 87.73  \n                 3rd Qu.: 800.0   3rd Qu.:140.22  \n                 Max.   :1518.6   Max.   :198.95  \n\n\n\n\nRecreating tables and plots from above\nTo check the synthetic dataset, I will recreate some of the results from above.\n\n# Mean and SD of viral load and CD4 count by arm\n#Synthetic dataset\nproc_means(dataset, var = cd4, class = arm)\n\n  CLASS TYPE FREQ VAR   N     MEAN      STD       MIN      MAX\n1  &lt;NA&gt;    0  539 cd4 539 625.8906 261.6480 -251.6203 1518.566\n2     0    1  228 cd4 228 599.0607 250.5687 -201.0123 1255.786\n3     1    1  311 cd4 311 645.5601 268.1762 -251.6203 1518.566\n\n#Original dataset\nproc_means(data, var = cd4_survey_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN   STD   MIN   MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 cd4_survey_yr3   539  620.  279.    70  1567\n2 0         1   239 cd4_survey_yr3   239  604.  265.    70  1540\n3 1         1   300 cd4_survey_yr3   300  632.  290.    78  1567\n\n#Synthetic dataset\nproc_means(dataset, var = viral_load, class = arm)\n\n  CLASS TYPE FREQ        VAR   N     MEAN      STD MIN      MAX\n1  &lt;NA&gt;    0  539 viral_load 539 87.72695 54.60308  40 198.9452\n2     0    1  228 viral_load 228 90.65165 55.76379  40 198.9452\n3     1    1  311 viral_load 311 85.58280 53.72527  40 198.2108\n\n#Original dataset\nproc_means(data, var = viral_load_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN    STD   MIN    MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 viral_load_yr3   539 1784. 13182.    40 177075\n2 0         1   239 viral_load_yr3   239 3256. 19241.    40 177075\n3 1         1   300 viral_load_yr3   300  610.  3854.    40  37415\n\n\nThe means and standard deviations for each arm are very comparable between the original and synthetic datasets. The means are within 15 of each other. The number in each arm is also similar.\nI also used ChatGPT to create plots similar to the plots above to see how well ChatGPT could continue to use the dataset. I supplied this prompt to create the code for the plots: write code to create three plots. first, plot the mean cd4 by arm with error bars showing 1 standard deviation, color arm = 0 as red and arm = 1 as teal. For the second plot, perform the same as the first plot using viral_load instead of cd4. For the third plot, create a scatterplot with cd4 as the y and viral_load as the x, coloring the dots by region.\nAs a note, based on my prompt, R assigned one of the colors in Plots 1 and 2 as teal, which isn’t a color R recognized. I changed teal to blue in the code below.\n\n# Compute summary statistics for CD4 by arm\ncd4_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_cd4 = mean(cd4),\n            sd_cd4 = sd(cd4)) \n\n# First plot: Mean CD4 by arm with error bars\np1 &lt;- ggplot(cd4_summary, aes(x = arm, y = mean_cd4, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean CD4 by Arm\", x = \"Arm\", y = \"Mean CD4 Count\") +\n  theme_minimal()\n\n# Compute summary statistics for Viral Load by arm\nviral_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_viral = mean(viral_load),\n            sd_viral = sd(viral_load)) \n\n# Second plot: Mean Viral Load by arm with error bars\np2 &lt;- ggplot(viral_summary, aes(x = arm, y = mean_viral, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_viral - sd_viral, ymax = mean_viral + sd_viral), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean Viral Load by Arm\", x = \"Arm\", y = \"Mean Viral Load\") +\n  theme_minimal()\n\n# Third plot: Scatterplot of CD4 vs. Viral Load, colored by Region\np3 &lt;- ggplot(dataset, aes(x = viral_load, y = cd4, color = region)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"blue\", \"1\" = \"green\", \"2\" = \"purple\")) +\n  labs(title = \"Scatterplot of CD4 vs Viral Load\", x = \"Viral Load\", y = \"CD4 Count\") +\n  theme_minimal()\n\n# Display plots\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\nThe synthetic viral load variable has a max of 200, even though 5% of values were supposed to be above 200 leading to a mean value of 3300 for arm = 0 and 610 for arm = 1. To get around this, I told ChatGPT what the problem was and suggested a solution: For the dataset creation, the mean viral_load is too low. Can the order of the steps for confirming 95% of value are below 200 and sampling from the distribution be switched?\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- numeric(n)  # Initialize vector\nviral_load[arm == 0] &lt;- rnorm(sum(arm == 0), mean = 3256, sd = 19241)\nviral_load[arm == 1] &lt;- rnorm(sum(arm == 1), mean = 610, sd = 3854)\n\n# Ensure 95% of viral_load values are &lt;200 while keeping original distribution\nhigh_values &lt;- which(viral_load &gt;= quantile(viral_load, 0.95))\nviral_load[high_values] &lt;- runif(length(high_values), min = 40, max = 200)  # Keep min at 40\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load      \n 0:160   0:228   Min.   :-251.6   Min.   :-48569.3  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.: -3309.8  \n 2:187           Median : 637.1   Median :   268.9  \n                 Mean   : 625.9   Mean   :   101.8  \n                 3rd Qu.: 800.0   3rd Qu.:  4466.5  \n                 Max.   :1518.6   Max.   : 25375.5  \n\n\n\n# Compute summary statistics for CD4 by arm\ncd4_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_cd4 = mean(cd4),\n            sd_cd4 = sd(cd4)) \n\n# First plot: Mean CD4 by arm with error bars\np1 &lt;- ggplot(cd4_summary, aes(x = arm, y = mean_cd4, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean CD4 by Arm\", x = \"Arm\", y = \"Mean CD4 Count\") +\n  theme_minimal()\n\n# Compute summary statistics for Viral Load by arm\nviral_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_viral = mean(viral_load),\n            sd_viral = sd(viral_load)) \n\n# Second plot: Mean Viral Load by arm with error bars\np2 &lt;- ggplot(viral_summary, aes(x = arm, y = mean_viral, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_viral - sd_viral, ymax = mean_viral + sd_viral), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean Viral Load by Arm\", x = \"Arm\", y = \"Mean Viral Load\") +\n  theme_minimal()\n\n# Third plot: Scatterplot of CD4 vs. Viral Load, colored by Region\np3 &lt;- ggplot(dataset, aes(x = viral_load, y = cd4, color = region)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"blue\", \"1\" = \"green\", \"2\" = \"purple\")) +\n  labs(title = \"Scatterplot of CD4 vs Viral Load\", x = \"Viral Load\", y = \"CD4 Count\") +\n  theme_minimal()\n\n# Display plots\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\nThis resulted in viral_load values that were negative again, so I added a final prompt: Can you update so the lowest value for viral_load is 40?\nThe following code was created:\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- numeric(n)  # Initialize vector\nviral_load[arm == 0] &lt;- rnorm(sum(arm == 0), mean = 3256, sd = 19241)\nviral_load[arm == 1] &lt;- rnorm(sum(arm == 1), mean = 610, sd = 3854)\n\n# Ensure 95% of viral_load values are &lt;200 while keeping original distribution\nhigh_values &lt;- which(viral_load &gt;= quantile(viral_load, 0.95))\nviral_load[high_values] &lt;- runif(length(high_values), min = 40, max = 200)  # Keep min at 40\n\n# Ensure minimum viral_load is 40\nviral_load &lt;- pmax(viral_load, 40)\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load     \n 0:160   0:228   Min.   :-251.6   Min.   :   40.0  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.:   40.0  \n 2:187           Median : 637.1   Median :  268.9  \n                 Mean   : 625.9   Mean   : 3525.9  \n                 3rd Qu.: 800.0   3rd Qu.: 4466.5  \n                 Max.   :1518.6   Max.   :25375.5  \n\n\n\n# Compute summary statistics for CD4 by arm\ncd4_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_cd4 = mean(cd4),\n            sd_cd4 = sd(cd4)) \n\n# First plot: Mean CD4 by arm with error bars\np1 &lt;- ggplot(cd4_summary, aes(x = arm, y = mean_cd4, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean CD4 by Arm\", x = \"Arm\", y = \"Mean CD4 Count\") +\n  theme_minimal()\n\n# Compute summary statistics for Viral Load by arm\nviral_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_viral = mean(viral_load),\n            sd_viral = sd(viral_load)) \n\n# Second plot: Mean Viral Load by arm with error bars\np2 &lt;- ggplot(viral_summary, aes(x = arm, y = mean_viral, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_viral - sd_viral, ymax = mean_viral + sd_viral), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean Viral Load by Arm\", x = \"Arm\", y = \"Mean Viral Load\") +\n  theme_minimal()\n\n# Third plot: Scatterplot of CD4 vs. Viral Load, colored by Region\np3 &lt;- ggplot(dataset, aes(x = viral_load, y = cd4, color = region)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"blue\", \"1\" = \"green\", \"2\" = \"purple\")) +\n  labs(title = \"Scatterplot of CD4 vs Viral Load\", x = \"Viral Load\", y = \"CD4 Count\") +\n  theme_minimal()\n\n# Display plots\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\nThe bar chart for CD4 by arm is very similar to the original data. Based on the bar chart for viral load, the means appear simialr but the standard errors are reduced compared to the original data. The final plot, the scatterplot of CD4 count by viral load, showed that many more observations had higher viral loads and many more had viral loads above 200. It seems like ChatGPT isn’t handling the prompts for this variable well. Overall, the synthetic data look good but not perfect."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My Name is Muhammad Nasir, Call me “NASIR”, it’s originally from Arabic Language “Naseer” meaning “The Helper”. I love that name. However, many people hard to pronounce it, so many people here (or when I was in the UK) call me Muhammad, which I is fine for me. I am from Indonesia. I am a little boy from small village Called “Kindang”, a remote area in the foot of Bawakaraeng Mountain in South Sulawesi Indonesia. However, I left the village since I was 12 YO, moving to a city where I did my middle school, and keep moving from one city to other cities for each level of my education. I am the luckiest guy in the village (or in the world) because I can skip the chain of poverty and the darkness of lack of education for myself (and my family now), I am the first generation going to school in entire my big Family. So I am so PROUD to be here and lived in several different countries (Education is the gate of the world, and education changes life).\nI love “NATURE”, specially hidden gem. and I am lucky to be born and grown up in Indonesia, where billions of hidden gem that we can visit. One of the hidden “HEAVEN” that I can show you in the picture below, called ” PAISU POK LAKE” in Central Sulawesi, it takes eight hours by boat to enjoy this beautiful view. You can see “ancient trees buried in the lake”.\n\n\nPictures: PAISU POK Lake in Banggai Kepulauan Island in Indonesia.\nFun facts: After I got 12 YO, I never live in a city more than 4 years.\nHobby: Playing badminton, Hiking and Hide myself and sleep in hidden gems for days.\n\nEducational Background\nI obtained my Bachelor of Public health with Environmental Health Concentration from Universitas Muslim Indonesia in Makassar. I did my Master degree at The University of Birmingham, UK on Public and Environmental Science. This course was prepared for being a practitioner in environmental health. I did pre doctoral program at the University of Illinois at Urbana-Champaign before starting my PhD at the University of Georgia.\n\n\nWorking Experience\nI worked in several different topics/ fields. Before I started my master, I worked as an intern at Ministry of Environment Republic Indonesia on a project called ” Culture and biodiversity in Raja Ampat ” (I highly recommend you to GOOGLE “RAJA AMPAT INDONESIA”, you would see a heaven that you must put in your travel lists). After my Master, I worked on a Project “WASH Post Disaster in Central Sulawesi” funded by WHO. Then in 2019, I worked with Universitas Indonesia and UNICEF on Adolescence Mental Health Post Disaster in Central Sulawesi. In 2020, I worked at Kerti Praja Foundation on a project called ” Implementation of Integrated Gender-Based Violence (GBV) and Sexual Reproductive Health (SRH) Service in Primary Care in four different provinces in Indonesia, funded by UNFPA Indonesia and Government of Japan. Right before I came to the USA, I work at WHO Indonesia as a national consultant for Malaria Elimination in Central Sulawesi (only three months and have to resign to pursue higher degree). Although I worked in many different topics, the nature of the jobs that I did was similar, most of the things that I did was about community empowerment, program development, and policy advocacy to local governments. None of my previous work related to Epidemiology or Biostatistics, but I am so lucky to be in this department at the moment.\n\nPicture: Delivering a training for community leader about GBV survivor outreach in community level (my little guard, SEAN, who always accompanies me at work. Luckily, most work places in my place are child friendly).\n\n\nData analysis experience\nHonestly, I am very new in quantitative data analysis, let say I started it at UGA. Most of my work deal with qualitative data or for quantitative data, they are prepared by data analysts. Therefore, my data analysis experience only relate to class assignment or projects. I started Using R last year, so please help me grow and I am happy if people can involve me in their project for data analysis (for Free :) ), I just need to learn and get more experience.\n\n\nResearch Interest\nI am currently interested in Climate Change and its implication to public health. I am also interested in communicable diseases such as Tuberculosis, Soil-transmitted Helminths, water born diseases, and vector born diseases."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "Install all packages needed",
    "section": "",
    "text": "y— title: “R Coding Exercise”\nPlaceholder file for the future R coding exercise."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#natalies-addition-to-muhammads-exercise-3",
    "href": "coding-exercise/coding-exercise.html#natalies-addition-to-muhammads-exercise-3",
    "title": "Install all packages needed",
    "section": "Natalie’s Addition to Muhammad’s Exercise 3",
    "text": "Natalie’s Addition to Muhammad’s Exercise 3\nThis section is contributed to by Natalie Cann.\nI will first load packages needed for this exercise.\n\nlibrary(dslabs)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\n\nI will use the “help()” function on the “us_contagious_diseases” data from dslabs.\n\nhelp(us_contagious_diseases)\n\nThe help file told me that this data frame contains yearly counts for Hepatitis A, Measles, Mumps, Pertussis, Polio, Rubella, and Smallpox in the United States.\nNow, I will use str() on the “us_contagious_diseases” data frame.\n\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n\nThis informs me that the data set contains 6 variables (disease, state, year, weeks_reporting, count, and population) and 16,065 observations. Disease and state are factors, while the other four variables are numeric.\nNext, I will use the summary() on the “us_contagious_diseases” data frame.\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n\nThe summary above shows the diseases and states. It also shows the minimum, 1st quartile, median, mean, 3rd quartile and maximum values for the numeric variables (year, weeks_reporting, count, and population).\nFirst, I will create a data frame containing only data for the state of Georgia.\n\nGA_contagious_diseases &lt;- filter(us_contagious_diseases, state == \"Georgia\") # I am using filter to obtain only data from GA\nView(GA_contagious_diseases) # I am viewing the data frame to ensure it only contains data from GA\n\nThis worked, as only GA data is shown in the new data frame.\nNow, I will run str() and summary() on the GA_contagious_diseases data frame.\n\nstr(GA_contagious_diseases)\n\n'data.frame':   315 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 11 11 11 11 11 11 11 11 11 11 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  47 51 50 48 51 50 47 50 48 48 ...\n $ count          : num  509 922 990 750 763 ...\n $ population     : num  4306523 4373252 4442463 4514462 4589575 ...\n\nsummary(GA_contagious_diseases)\n\n        disease          state          year      weeks_reporting\n Hepatitis A:46   Georgia   :315   Min.   :1928   Min.   : 0.00  \n Measles    :75   Alabama   :  0   1st Qu.:1950   1st Qu.:33.00  \n Mumps      :35   Alaska    :  0   Median :1975   Median :45.00  \n Pertussis  :56   Arizona   :  0   Mean   :1971   Mean   :37.66  \n Polio      :41   Arkansas  :  0   3rd Qu.:1990   3rd Qu.:49.00  \n Rubella    :37   California:  0   Max.   :2011   Max.   :52.00  \n Smallpox   :25   (Other)   :  0                                 \n     count           population     \n Min.   :    0.0   Min.   :2901933  \n 1st Qu.:    8.5   1st Qu.:3444578  \n Median :   42.0   Median :5009127  \n Mean   :  643.0   Mean   :5235135  \n 3rd Qu.:  352.0   3rd Qu.:6478216  \n Max.   :22965.0   Max.   :9830160  \n                                    \n\n\nThis shows me that all the variables are the same as before, but they only contain data from GA.\nNow, I will create a data frame that contains only data from GA in 1950. I will use this data frame to create a plot of the number of cases of each disease in GA in 1950.\n\nGA_contagious_diseases_1950 &lt;- filter(GA_contagious_diseases, year == 1950) # I am using filter to obtain only data from GA in 1950\n\n# I will run str() and summar() on the GA_contagious_diseases_1950 data frame to get a better look at the data\nstr(GA_contagious_diseases_1950) \n\n'data.frame':   4 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 2 4 5 7\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 11 11 11 11\n $ year           : num  1950 1950 1950 1950\n $ weeks_reporting: num  48 50 38 0\n $ count          : num  2159 1041 492 0\n $ population     : num  3444578 3444578 3444578 3444578\n\nsummary(GA_contagious_diseases_1950)\n\n        disease         state        year      weeks_reporting     count       \n Hepatitis A:0   Georgia   :4   Min.   :1950   Min.   : 0.0    Min.   :   0.0  \n Measles    :1   Alabama   :0   1st Qu.:1950   1st Qu.:28.5    1st Qu.: 369.0  \n Mumps      :0   Alaska    :0   Median :1950   Median :43.0    Median : 766.5  \n Pertussis  :1   Arizona   :0   Mean   :1950   Mean   :34.0    Mean   : 923.0  \n Polio      :1   Arkansas  :0   3rd Qu.:1950   3rd Qu.:48.5    3rd Qu.:1320.5  \n Rubella    :0   California:0   Max.   :1950   Max.   :50.0    Max.   :2159.0  \n Smallpox   :1   (Other)   :0                                                  \n   population     \n Min.   :3444578  \n 1st Qu.:3444578  \n Median :3444578  \n Mean   :3444578  \n 3rd Qu.:3444578  \n Max.   :3444578  \n                  \n\n\nYou can see that now, the only year appearing in the data set is 1950. Therefore, all the variables now reflect only data from 1950 in GA.\nNext, I will rename the “count” variable to “number_of_cases”.\n\nGA_contagious_diseases_1950 &lt;- rename(GA_contagious_diseases_1950, number_of_cases = count) # I am renaming the \"count\" variable to \"number_of_cases\" via the rename() function (and then assigning it back to GA_contagious_diseases_1950)\n\ncolnames(GA_contagious_diseases_1950) # I am checking to see if the previous step was done properly\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"number_of_cases\" \"population\"     \n\n\nI see this was done correctly, as the variable that used to be called “count” is now called “number_of_cases”.\nNow, I will create a bar graph to display the number of cases of each contagious disease reported in GA in 1950.\n\ncustom_colors_1950 &lt;- c(\"Measles\" = \"#6fe51e\", \"Pertussis\" = \"#2ce1b0\", \"Polio\" = \"#2cb2e1\", \"Smallpox\" = \"#3a83e6\") # I am creating a vector of colors that I will use to fill each disease's bar on the graph below\n\nggplot(GA_contagious_diseases_1950, aes (x = disease, y = number_of_cases, fill = disease)) + # Using ggplot on the GA_contagious_diseases_1950 data frame and setting x and y equal to diseases and number of cases (respectively) and setting fill equal to disease\n  geom_bar(stat = \"identity\") + # Specifying the geom as geom_bar() to create a bar graph \n  labs(title = \"Reported Number of Cases of Contagious Diseases \\n in State of Georgia in 1950\", x = \"Disease\", y = \"Number of Cases\") + # Renaming title and axes\n  scale_fill_manual(values = custom_colors_1950) + # Setting the fill colors of the bars to the custom colors I created above \n  geom_text(aes(label = number_of_cases), vjust = -0.5) + # Adding text labels to the top of each bar to show the number of cases\n  theme(legend.position = \"bottom\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) + # Putting legend at the bottom; increasing size, boldness, and center of title/axes\n  scale_y_continuous(limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) # Setting the y-axis limits and breaks to better see the text of number of cases at the top\n\n\n\n\n\n\n\n\nYou can see from the bar graph above that the number of Measles cases in GA in 1950 was the highest out of all the diseases shown here (with the number of cases being 2159).\nNow, I will create a data frame that contains Measles data from all states in 1950.\n\nMeasles_1950 &lt;- filter(us_contagious_diseases, disease == \"Measles\", year == 1950) # I am using filter to obtain only Measles data from all states in 1950\n\n# I will run str() and summary() on the Measles_1950 data frame to get a better look at the data\nstr(Measles_1950) \n\n'data.frame':   51 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year           : num  1950 1950 1950 1950 1950 1950 1950 1950 1950 1950 ...\n $ weeks_reporting: num  47 0 49 50 50 50 51 44 43 49 ...\n $ count          : num  1556 0 2389 1739 14728 ...\n $ population     : num  3061743 NA 749587 1909511 10586224 ...\n\nsummary(Measles_1950)\n\n        disease          state         year      weeks_reporting\n Hepatitis A: 0   Alabama   : 1   Min.   :1950   Min.   : 0.0   \n Measles    :51   Alaska    : 1   1st Qu.:1950   1st Qu.:46.0   \n Mumps      : 0   Arizona   : 1   Median :1950   Median :49.0   \n Pertussis  : 0   Arkansas  : 1   Mean   :1950   Mean   :45.9   \n Polio      : 0   California: 1   3rd Qu.:1950   3rd Qu.:50.0   \n Rubella    : 0   Colorado  : 1   Max.   :1950   Max.   :51.0   \n Smallpox   : 0   (Other)   :45                                 \n     count           population      \n Min.   :    0.0   Min.   :  160083  \n 1st Qu.:  853.5   1st Qu.:  791896  \n Median : 2301.0   Median : 2233351  \n Mean   : 5909.4   Mean   : 3075456  \n 3rd Qu.: 6000.0   3rd Qu.: 3444578  \n Max.   :36859.0   Max.   :14830192  \n                   NA's   :2         \n\n\nThis shows data only from the Measles disease; it includes data from every state that reported measles data in 1950.\nOnce again, I will rename the “count” variable to “number_of_cases”.\n\nMeasles_1950 &lt;- rename(Measles_1950, number_of_cases = count) # I am renaming the \"count\" variable to \"number_of_cases\" via the rename() function \n\ncolnames(Measles_1950) # Making sure this was done correctly\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"number_of_cases\" \"population\"     \n\n\nAs seen by the output, I can see that this variable is now called “number_of_cases”.\nNow, I will create a bar graph to display the number of Measles cases reported in each state in 1950.\n\ncustom_colors_Measles_1950 &lt;- c(\"#53d127\", \"#71f45d\", \"#2ce1b0\", \"#40dcd9\", \"#2cb2e1\", \"#3a83e6\")\n\nggplot(Measles_1950, aes(x = state, y = number_of_cases, fill = number_of_cases)) + \n  geom_bar(stat = \"identity\") +  # Correct stat for using y data directly\n  labs(title = \"Reported Number of Measles Cases \\n in the United States in 1950\", \n       x = \"State\", \n       y = \"Number of Cases\") + # Rename title and axes\n  geom_text(aes(label = number_of_cases), vjust = -0.5, size = 1.5) +  # Display labels above bars\n  theme(legend.position = \"none\",  # Remove legend \n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\"), # Increase size and boldness of title and axes\n        axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate state names\n  scale_fill_gradientn(colors = custom_colors_Measles_1950) # Set custom color scheme made\n\n\n\n\n\n\n\n\nThis bar graph shows the number of Measles cases reported in each state in 1950. As you can see, Michigan had the highest number of reported cases of Measles; on the other hand, Alaska had the lowest number of reported cases of Measles.\nNow, I will create a new variable within the Measles_1950 data frame that assess number of cases in each region of the country.\n\nMeasles_1950$region &lt;- ifelse(Measles_1950$state %in% c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\", \"New York\", \"New Jersey\", \"Pennsylvania\", \"Delaware\", \"Maryland\", \"District Of Columbia\"), \"Northeast\", # contains states from the Northeast\n                        ifelse(Measles_1950$state %in% c(\"Ohio\", \"Michigan\", \"Indiana\", \"Illinois\", \"Wisconsin\", \"Minnesota\", \"Iowa\", \"Missouri\", \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\"),  \"Midwest\", # contains states from the Midwest\n                        ifelse(Measles_1950$state %in% c(\"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\", \"Kentucky\", \"Tennessee\", \"Alabama\", \"Mississippi\", \"Arkansas\", \"Louisiana\"), \"South\", # contains states from the South\n                        ifelse(Measles_1950$state %in% c(\"Texas\", \"Oklahoma\", \"New Mexico\", \"Arizona\"), \"Southwest\", # contains states from the Southwest\n                        ifelse(Measles_1950$state %in% c(\"Alaska\", \"Hawaii\"), \"Non-Contiguous\", # contains states not connected to the main US\n                        \"West\"))))) # any other state will be categorized as the West\n\nhead(Measles_1950) # I am checking to ensure this was done properly\n\n  disease      state year weeks_reporting number_of_cases population\n1 Measles    Alabama 1950              47            1556    3061743\n2 Measles     Alaska 1950               0               0         NA\n3 Measles    Arizona 1950              49            2389     749587\n4 Measles   Arkansas 1950              50            1739    1909511\n5 Measles California 1950              50           14728   10586224\n6 Measles   Colorado 1950              50            5239    1325089\n          region\n1          South\n2 Non-Contiguous\n3      Southwest\n4          South\n5           West\n6           West\n\n\nI will create a version of this data frame that contains the number of cases of Measles in each region of the US.\n\nregion_Measles_cases_1950 &lt;- Measles_1950 %&gt;% # Creating new data frame that groups data by region and then includes the total number of cases in each region\n  group_by(region) %&gt;%\n  summarize(total_cases = sum(number_of_cases))\n\nNow, we can create a bar graph to display the number of Measles cases reported in each region in 1950.\n\nggplot(region_Measles_cases_1950, aes(x = region, y = total_cases, fill = total_cases)) +  # Using ggplot on the region_Measles_cases_1950 data frame and setting x and y equal to region and total_cases (respectively) and setting fill equal to region\n  geom_bar(stat = \"identity\") +  # Use 'identity' since we are providing y values directly\n  labs(title = \"Reported Measles Cases by Region in 1950\", \n       x = \"Region\", \n       y = \"Number of Cases\") + # Rename title and axes\n  geom_text(aes(label = total_cases), vjust = -0.5, size = 3) +  # Add labels above bars\n  theme(legend.position = \"right\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\"), # Increase size and boldness of title and axes\n        axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate state names\n  scale_y_continuous(limits = c(0, 130000), breaks = seq(0, 130000, by = 20000)) + # Setting the y-axis limits and breaks to better see the text of number of cases at the top\n  scale_fill_gradientn(colors = custom_colors_Measles_1950) # Set custom color scheme made\n\n\n\n\n\n\n\n\nAs you can see, the Midwest had the highest number of reported Measles cases in 1950 (115836). The Non-Contiguous region had the lowest number of reported Measles cases in 1950 (0) - it is important to note that Alaska and Hawaii are the only states in this region and that Alaska did not report any cases.\nI will create a Scatterplot of the number of cases of Measles in each state in 1950.\n\nMeasles_1950$state &lt;- factor(Measles_1950$state, \n                             levels = unique(Measles_1950$state[order(Measles_1950$region)]))\n# I am reordering the states based on the region they are in (instead of alphabetical order) via levels(), unique(), and order()\n\nggplot(Measles_1950, aes(x = state, y = number_of_cases, color = region)) + \n  geom_point(size = 3) +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"Reported Measles Cases in Each State in 1950\", \n       x = \"State\", \n       y = \"Number of Cases\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) +  # Increase size and boldness of title and axes\n  scale_color_manual(values = c(\"Northeast\" = \"#53d127\", \"Midwest\" = \"#71f45d\", \"South\" = \"#2ce1b0\", \n                               \"Southwest\" = \"#47e2e7\", \"Non-Contiguous\" = \"#2cb2e1\", \"West\" = \"#3a83e6\"))  # Assign specific colors to each region\n\n\n\n\n\n\n\n\nAs you can see from the scatterplot above, the Midwest and the Northeast have the greatest amount of variation in the number of reported Measles cases in 1950.\nI will now use the lm() function to fit a linear model with number_of_cases as the outcome and region of the United States as the predictor. Then, I will apply the summary() function to view the results.\n\nfit_1950 &lt;- lm(number_of_cases ~ region, data = Measles_1950) # Running a linear fit via lm()\nsummary(fit_1950) # Viewing results of linear model via summary()\n\n\nCall:\nlm(formula = number_of_cases ~ region, data = Measles_1950)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9259  -5078  -1130   1937  27206 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              9653       2353   4.103 0.000169 ***\nregionNon-Contiguous    -9620       6224  -1.546 0.129227    \nregionNortheast         -1420       3327  -0.427 0.671600    \nregionSouth             -6784       3327  -2.039 0.047339 *  \nregionSouthwest         -5254       4705  -1.117 0.270105    \nregionWest              -5802       3594  -1.615 0.113405    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8150 on 45 degrees of freedom\nMultiple R-squared:  0.1334,    Adjusted R-squared:  0.03716 \nF-statistic: 1.386 on 5 and 45 DF,  p-value: 0.2476\n\n\nThe P-value of this model is greater than 0.05, which means we must fail to reject the null hypothesis (that there is no relationship between the number of Measles cases and the region of the US). This means that the region of the US does not have a significant impact on the number of Measles cases reported in 1950.\nI will run an ANOVA (Analysis of Variance) test to asses if there are significant differences in the means of the number of Measles cases reported in each region in the US in 1950.\n\nanova_result &lt;- aov(number_of_cases ~ region, data = Measles_1950) # Running ANOVA test with aov() function\nsummary(anova_result) # Viewing a summary of results\n\n            Df    Sum Sq  Mean Sq F value Pr(&gt;F)\nregion       5 4.602e+08 92049896   1.386  0.248\nResiduals   45 2.989e+09 66418472               \n\n\nThe F-value of the ANOVA table is 1.386, and the P-value is 0.248. Since 0.248 &gt; 0.05, we must fail to reject the null hypothesis, which is stated previously. Therefore, this result is consistent with the linear fit model results above; region of the US does not appear to have a significant impact on the number of Measles cases in 1950.\nI will now put the summary into a table.\n\nanova_df &lt;- as.data.frame(summary(anova_result)[[1]]) # Putting the summary of the ANOVA test into a data frame \nprint(anova_df) # To Printing the  data frame\n\n            Df     Sum Sq  Mean Sq  F value    Pr(&gt;F)\nregion       5  460249478 92049896 1.385908 0.2476114\nResiduals   45 2988831236 66418472       NA        NA\n\n\nHere is the printed ANOVA results."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "HELLO\n\nYOU ARE IN NASIR’S PERSONAL WEBSITE\nWelcome to my website and data analysis portfolio.\nPlease use the Menu Bar above to explore about me and my portofolio in research, work, and academic life."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/rayleenlewis/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Chol                  0             1 201.  63.2  120 160 176 243  310 ▅▇▁▇▂\n4 Agecat                0             1   2    1.22   0   2   2   3    3 ▃▁▁▆▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/rayleenlewis/Desktop/MADA-Mac-local/muhammadnasir-mada2025-portofolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n$ Chol   &lt;dbl&gt; 160, 140, 163, 243, 300, 259, 120, 310, 270, 123, 145, 176, 167…\n$ Agecat &lt;dbl&gt; 0, 0, 1, 3, 3, 2, 3, 2, 3, 0, 0, 3, 2, 3\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Chol      \n Length:14          Min.   :  45.0   Length:14          Min.   :120.0  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:148.8  \n Mode  :character   Median :  70.0   Mode  :character   Median :171.5  \n                    Mean   : 602.7                      Mean   :200.8  \n                    3rd Qu.:  90.0                      3rd Qu.:255.0  \n                    Max.   :7000.0                      Max.   :310.0  \n                    NA's   :1                                          \n     Agecat     \n Min.   :0.000  \n 1st Qu.:0.250  \n Median :2.000  \n Mean   :1.786  \n 3rd Qu.:3.000  \n Max.   :3.000  \n                \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender  Chol Agecat\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 180        80 M        160      0\n2 175        70 O        140      0\n3 sixty      60 F        163      1\n4 178        76 F        243      3\n5 192        90 NA       300      3\n6 6          55 F        259      2\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n200.79\n66.29\n120\n148.75\n171.5\n255\n310\n▇▇▁▇▃\n\n\nAgecat\n0\n1.00\n1.79\n1.31\n0\n0.25\n2.0\n3\n3\n▅▁▁▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n203.69\n68.07\n120\n145.00\n176\n259\n310\n▇▆▁▇▃\n\n\nAgecat\n0\n1.00\n1.85\n1.34\n0\n0.00\n2\n3\n3\n▅▁▁▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n203.69\n68.07\n120\n145.00\n176\n259\n310\n▇▆▁▇▃\n\n\nAgecat\n0\n1.00\n1.85\n1.34\n0\n0.00\n2\n3\n3\n▅▁▁▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nChol\n0\n1\n216.36\n66.24\n120\n163.5\n235\n264.5\n310\n▃▆▁▇▃\n\n\nAgecat\n0\n1\n2.18\n1.17\n0\n2.0\n3\n3.0\n3\n▂▁▁▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nChol\n0\n1\n216.36\n66.24\n120\n163.5\n235\n264.5\n310\n▃▆▁▇▃\n\n\nAgecat\n0\n1\n2.18\n1.17\n0\n2.0\n3\n3.0\n3\n▂▁▁▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nChol\n0\n1\n201.11\n63.16\n120\n160\n176\n243\n310\n▅▇▁▇▂\n\n\nAgecat\n0\n1\n2.00\n1.22\n0\n2\n2\n3\n3\n▃▁▁▆▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. I added two variables in the dataset; Total Cholesterol and Age group/ age category.For Please check the Code book to see the dictionary or code of added variables."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.976545\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.245261\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nChol\n0\n1\nNA\nNA\nNA\n201.11111\n63.155452\n120\n160\n176\n243\n310\n▅▇▁▇▂\n\n\nnumeric\nAgecat\n0\n1\nNA\nNA\nNA\n2.00000\n1.224745\n0\n2\n2\n3\n3\n▃▁▁▆▇"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  }
]