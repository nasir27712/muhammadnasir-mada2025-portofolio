[
  {
    "objectID": "test/test.html",
    "href": "test/test.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "About the dataset\nThe Botswana Combination Prevention Project (BCPP) was a collaborative research effort led by the Botswana Ministry of Health (MOH), the Harvard School of Public Health/Botswana Harvard AIDS Institute Partnership (BHP), and the U.S. Centers for Disease Control and Prevention (CDC). This community-based randomized trial aimed to assess the impact of various HIV prevention strategies on reducing HIV incidence across 15 intervention and 15 control communities. The intervention communities received comprehensive HIV testing, linkage to care, and universal treatment services, guided by the UNAIDS 90-90-90 targets: ensuring 90% of individuals with HIV are aware of their status, 90% of those diagnosed are on antiretroviral therapy (ART), and 90% of those on ART achieve viral suppression.\nThe BCPP study was structured around two interrelated protocols: the Evaluation Protocol and the Intervention Protocol. The Evaluation Protocol assessed the primary outcome—HIV incidence—along with key secondary outcomes, focusing on data collected from the Baseline Household Survey, the HIV Incidence Cohort, and the End of Study Survey. Meanwhile, the Intervention Protocol involved the implementation of a combination prevention (CP) package in combination prevention communities (CPCs), monitoring the uptake of interventions such as expanded HIV testing and counseling, enhanced male circumcision services, and improved access to HIV care and treatment.\nThe dataset is available and free to download at CDC Website:https://data.cdc.gov/Global-Health/Botswana-Combination-Prevention-Project-BCPP-Publi/qcw5-4m9q/about_data.\nThe study is a cohort study with 3 phases. In this exercise, I only work on year 3 dataset. There are many information covered in this dataset, including demographic information of the respondent, soccioeconomic factors, HIV exposure, HIV status and conditions.\n\n\nInstall all packages needed\nInstall and library all packages needed in this section.\n\ninstall.packages(\"tidyverse\")\n\nThe following package(s) will be installed:\n- tidyverse [2.0.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidyverse ...                      OK [linked from cache]\nSuccessfully installed 1 package in 36 milliseconds.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ninstall.packages(\"ggplot2\")\n\nThe following package(s) will be installed:\n- ggplot2 [3.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggplot2 ...                        OK [linked from cache]\nSuccessfully installed 1 package in 37 milliseconds.\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\ninstall.packages(\"patchwork\")  # This package is to redefine \"/\" operator for plot arrangement\n\nThe following package(s) will be installed:\n- patchwork [1.3.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing patchwork ...                      OK [linked from cache]\nSuccessfully installed 1 package in 36 milliseconds.\n\nlibrary(patchwork)\ninstall.packages(\"writexl\")\n\nThe following package(s) will be installed:\n- writexl [1.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing writexl ...                        OK [linked from cache]\nSuccessfully installed 1 package in 36 milliseconds.\n\nlibrary(writexl)\nlibrary(haven)\nlibrary(dplyr)\ninstall.packages(\"janitor\")\n\nThe following package(s) will be installed:\n- janitor [2.2.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing janitor ...                        OK [linked from cache]\nSuccessfully installed 1 package in 36 milliseconds.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\n\nLoading the dataset\n\n## set path  dictionary \nbostwana &lt;- here::here(\"test\",\"data\", \"cdcbostwana.csv\") # set the pathway to create relative path \n\nbostwana &lt;- read_csv(bostwana) # read the dataset stored in specified path\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 11369 Columns: 322\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (217): de_subj_idC, de_hh_idC, de_plot_idC, region, random_arm, survey, ...\ndbl  (64): community_rndmN, pair_rndmN, interview_days, yeardone, age_at_int...\nlgl  (41): religion_name, religious_affil, relig_theogrp, ethnicity, prev_hi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(bostwana)\n\n# A tibble: 6 × 322\n  de_subj_idC de_hh_idC de_plot_idC region community_rndmN pair_rndmN random_arm\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     \n1 10011A      16863A    14700A      North…              25         13 Intervent…\n2 1001A       8552A     17400A      South…               4          1 Intervent…\n3 10021A      13235A    4578A       Centr…               5          8 Standard …\n4 10023A      7028A     7820A       Centr…               9          4 Standard …\n5 10026A      5500A     4704A       South…              19          3 Intervent…\n6 10032A      2667A     8896A       South…              14          2 Standard …\n# ℹ 315 more variables: interview_days &lt;dbl&gt;, yeardone &lt;dbl&gt;, survey &lt;chr&gt;,\n#   gender &lt;chr&gt;, age_at_interview &lt;dbl&gt;, age5cat &lt;chr&gt;,\n#   length_residence &lt;chr&gt;, permanent_resident &lt;chr&gt;, intend_residency &lt;chr&gt;,\n#   nights_away &lt;chr&gt;, cattle_postlands &lt;chr&gt;, religion_name &lt;lgl&gt;,\n#   religious_affil &lt;lgl&gt;, relig_theogrp &lt;lgl&gt;, ethnicity &lt;lgl&gt;,\n#   marital_status &lt;chr&gt;, num_wives &lt;dbl&gt;, husband_wives &lt;dbl&gt;,\n#   live_alone &lt;chr&gt;, livewith_family &lt;chr&gt;, livewith_partner &lt;chr&gt;, …\n\ndim(bostwana)\n\n[1] 11369   322\n\n\nThe dataset contains 11369 observations and 322 columns.\n\n\nData Cleaning\nI want to select interesting variables for further analysis and clean the data as needed including dropping/ lablig missing values.\n\nbost_df &lt;- bostwana %&gt;% \n  select(\"region\", \"random_arm\", \"gender\", \"age_at_interview\", \"age5cat\", \"employment_status\", \"circumcised\", \"hiv_status_current\", \"hiv_status_time\", \"viral_load_yr3\",  \"cd4_survey_yr3\", \"cd4_surveydays_yr3\", \"arv_duration_days\")\n\nhead(bost_df)\n\n# A tibble: 6 × 13\n  region   random_arm       gender age_at_interview age5cat    employment_status\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            \n1 Northern Intervention     M                  45.7 45-54 yea… Unemployed not l…\n2 Southern Intervention     F                  51.8 45-54 yea… Unemployed looki…\n3 Central  Standard of Care F                  40.9 35-44 yea… Unemployed looki…\n4 Central  Standard of Care F                  18.7 16-24 yea… Unemployed not l…\n5 Southern Intervention     F                  35.8 35-44 yea… Employed         \n6 Southern Standard of Care F                  56.9 55-64 yea… Unemployed looki…\n# ℹ 7 more variables: circumcised &lt;chr&gt;, hiv_status_current &lt;chr&gt;,\n#   hiv_status_time &lt;chr&gt;, viral_load_yr3 &lt;dbl&gt;, cd4_survey_yr3 &lt;dbl&gt;,\n#   cd4_surveydays_yr3 &lt;dbl&gt;, arv_duration_days &lt;dbl&gt;\n\nsummary(bost_df)\n\n    region           random_arm           gender          age_at_interview\n Length:11369       Length:11369       Length:11369       Min.   :17.20   \n Class :character   Class :character   Class :character   1st Qu.:26.70   \n Mode  :character   Mode  :character   Mode  :character   Median :35.70   \n                                                          Mean   :38.03   \n                                                          3rd Qu.:48.20   \n                                                          Max.   :67.90   \n                                                                          \n   age5cat          employment_status  circumcised        hiv_status_current\n Length:11369       Length:11369       Length:11369       Length:11369      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n hiv_status_time    viral_load_yr3    cd4_survey_yr3   cd4_surveydays_yr3\n Length:11369       Min.   :     40   Min.   :  20.0   Min.   : 358      \n Class :character   1st Qu.:     40   1st Qu.: 401.0   1st Qu.: 875      \n Mode  :character   Median :     40   Median : 573.0   Median : 895      \n                    Mean   :   4305   Mean   : 602.1   Mean   : 881      \n                    3rd Qu.:     40   3rd Qu.: 757.0   3rd Qu.: 911      \n                    Max.   :2243242   Max.   :1567.0   Max.   :1184      \n                    NA's   :8130      NA's   :10594    NA's   :8032      \n arv_duration_days\n Min.   :   0     \n 1st Qu.: 908     \n Median :2393     \n Mean   :2432     \n 3rd Qu.:3764     \n Max.   :5879     \n NA's   :8222     \n\n\nNote: To make easy for further analysis, I selected several variables:\n\nregion = region of sbject\nrandom_arm = randomized arm\nage= age\nage5cat = age category\nemployment_status = employment status\nhiv_status_current = Current HIV status\nviral_load_yr3 = viral load in year 3 of the study\ncd4_survey_yr3 = CD4 count in year 3\ncd4_surveydays_yr3 = numbers of days enrolled to survey year 3.\narv_duration_days = number of day (duration) taking ARV\n\nIn this excercise, I would like to see the relationship between viral load, lenght of enrollment, ARV duration, and CD4 count. Therefore, I will drop all missing data in those variables.\n\ndata &lt;- bost_df %&gt;% \n  drop_na(viral_load_yr3, cd4_survey_yr3, cd4_surveydays_yr3, arv_duration_days) %&gt;% # I drop all observation wit N/A data\n  filter(viral_load_yr3 != 0, \n         cd4_survey_yr3 != 0, \n         cd4_surveydays_yr3 != 0, \n         arv_duration_days !=0) # I found some observation contain 0 I drop those observation\n\n\nsapply(data, class) # I want to check class of all variables \n\n            region         random_arm             gender   age_at_interview \n       \"character\"        \"character\"        \"character\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n       \"character\"        \"character\"        \"character\"        \"character\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n       \"character\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nFor N/A information in categorical data, I lable those missing value wih 999. However, the values are in characters. I want to change the values into factors to make us easy in analysis.\n\ndata &lt;- data %&gt;% \n  mutate(region = recode(region, \n                         \"Northern\" = 0,\n                         \"Southern\" = 1, \n                         \"Central\" = 2), \n         random_arm = recode(random_arm, \n                             \"Standard of Care\" = 0,\n                             \"Intervention\" = 1), \n         gender = recode(gender,\n                         \"M\" = 0,\n                         \"F\" =1), \n         age5cat = recode (age5cat, \n                           \"16-24 years\" = 0, \n                           \"25-34 years\" = 1, \n                           \"35-44 years\" = 2, \n                           \"45-54 years\" = 3, \n                           \"55-64 years\" = 4), \n         employment_status = recode(employment_status , \n                                    \"Employed\" = 0, \n                                    \"Unemployed looking for work\" = 1, \n                                    \"Unemployed not looking for work\" = 2), \n         circumcised= recode(circumcised , \n                             \"No\" = 0, \n                             \"Yes\" = 1), \n         hiv_status_current = recode(hiv_status_current, \n                                     \"HIV-uninfected\" = 0, \n                                     \"HIV-infected\" = 1, \n                                     \"Refused HIV testing\" = 2), \n         hiv_status_time = recode(hiv_status_time, \n                                  \"HIV-negative\" = 0, \n                                  \"HIV-positive: previously diagnosed\" = 1, \n                                  \"Refused HIV testing\" = 2, \n                                  \"HIV-positive: newly discovered\" = 3)) # this part is to recode from character to numeric to allow us convert ito factors. \n\nsapply(data, class) # check character of the variables \n\n            region         random_arm             gender   age_at_interview \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nNow, I want to convert categorical variables into factors\n\ndata &lt;- data %&gt;%\n  mutate(across (c(\"region\", \"random_arm\", \"gender\", \"age5cat\", \"employment_status\", \"circumcised\", \"hiv_status_current\",  \"hiv_status_time\"), as.factor)) # this is to convert multiple variables into factors. as.factor() only can work in single variable. \n\nsapply(data, class) # check character of the variables \n\n            region         random_arm             gender   age_at_interview \n          \"factor\"           \"factor\"           \"factor\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n          \"factor\"           \"factor\"           \"factor\"           \"factor\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n          \"factor\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nI want to replace all N/A with 999 in categorical variables.\n\ndata &lt;- data %&gt;%\n  mutate(across(where(is.factor), ~ replace_na(as.character(.), \"999\"))) %&gt;% # replace the values\n  mutate(across(where(is.character), as.factor)) # convert the variable back to factor. \n\ndim(data)\n\n[1] 539  13\n\n\nThe data is clean now and ready for data exploration and analysis. There are 539 obserations and 13 variables in the final data.\n\nsummary_stats &lt;- data %&gt;%\n  summarise(\n    mean_viral_load = mean(viral_load_yr3, na.rm = TRUE),\n    sd_viral_load = sd(viral_load_yr3, na.rm = TRUE),\n    mean_cd4 = mean(cd4_survey_yr3, na.rm = TRUE),\n    sd_cd4 = sd(cd4_survey_yr3, na.rm = TRUE)\n  )\nprint(summary_stats)\n\n# A tibble: 1 × 4\n  mean_viral_load sd_viral_load mean_cd4 sd_cd4\n            &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1           1784.        13182.     620.   279.\n\n\nNow I want to make summary statistics for Viral load and visualize it ina plot.\n\nsummary_by_arm &lt;- data %&gt;%\n  group_by(random_arm) %&gt;%\n  summarise(\n    mean_cd4 = mean(cd4_survey_yr3, na.rm = TRUE),\n    sd_cd4 = sd(cd4_survey_yr3, na.rm = TRUE)\n  ) # create by randomise_arm \n\nplot1 &lt;- ggplot(summary_by_arm, aes(x = random_arm, y = mean_cd4, fill = random_arm)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  labs(title = \"Mean CD4 Count by randomise group with Standard Deviation\",\n       x = \"Groups\", y = \"Mean CD4 Count\", \n       caption = \"Red (0)= Standard of Care, Green (1)= Intervention \") +\n  theme_minimal() +\n  theme(plot.background = element_rect(color = \"black\", size = 1)) \n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nprint(plot1)\n\n\n\n\n\n\n\nfigure_file = here(\"test\", \"pictures\", \"CD4 Mean and SD .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot1) # save the pictures created \n\nSaving 7 x 5 in image\n\n\nNow I want to make summary statistics for viral load and visualize it in a plot.\n\nsummary_by_arm_vl &lt;- data %&gt;%\n  group_by(random_arm) %&gt;%\n  summarise(\n    mean_vl = mean(viral_load_yr3, na.rm = TRUE),\n    sd_vl = sd(viral_load_yr3, na.rm = TRUE)\n  ) # create by randomise_arm \n\nplot2 &lt;- ggplot(summary_by_arm_vl, aes(x = random_arm, y = mean_vl, fill = random_arm)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_vl - sd_vl, ymax = mean_vl + sd_vl), width = 0.2) +\n  labs(title = \"Mean Viral Load Count by randomise group with Standard Deviation\",\n       x = \"Groups\", y = \"Mean Viral Load\", \n       caption = \"Red (0)= Standard of Care, Green (1)= Intervention \") +\n  theme_minimal() +\n  theme(plot.background = element_rect(color = \"black\", size = 1)) \n\nprint(plot2)\n\n\n\n\n\n\n\nfigure_file = here(\"test\", \"pictures\", \"Viral Load Mean and SD .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot2) # save the pictures created\n\nSaving 7 x 5 in image\n\n\n\n\nData Exploration\nIn this part, I want to explore the data and vizualise it before data analysis.\n\nplot3 &lt;- ggplot(data, aes(x = viral_load_yr3, y = cd4_survey_yr3, color = region)) + \n  geom_point(size = 3) +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"Viral Load VS CD4 count based on the region\", \n       x = \"Viral load\", \n       y = \"CD4\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) + # Increase size and boldness of title and axes\n  scale_color_manual(values = c(\"0\" = \"#53d127\", \"1\" = \"#2ce1b0\", \"2\" = \"#ff5733\"))+ # crete color manually \n  theme(plot.background = element_rect(color = \"black\", size = 1))\n\nprint(plot3)\n\n\n\n\n\n\n\nfigure_file = here(\"test\", \"pictures\", \"Viral Load VS CD4 based on region .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot3) # save the pictures created\n\nSaving 7 x 5 in image\n\n\n\nplot4 &lt;- ggplot(data, aes(x = arv_duration_days , y = cd4_survey_yr3, color = region)) + \n  geom_boxplot() +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"ARV Duration VS CD4 count based on the region\", \n       x = \"ARV Duration (days)\", \n       y = \"CD4 count\",\n       caption = \"Green = Nothern, Blue= Northern, and red= Central\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) + # Increase size and boldness of title and axes\n  theme(plot.background = element_rect(color = \"black\", size = 1)) +\n  scale_x_continuous(limits = c(0, 1000)) + # Set x-axis maximum limit to 1000\n  scale_color_manual(values = c(\"0\" = \"#53d127\", \"1\" = \"#2ce1b0\", \"2\" = \"#ff5733\")) # crete color manually \n\nprint(plot4)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nfigure_file = here(\"test\", \"pictures\", \"ARV Duration VS CD4 count based on the region .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot4) # save the pictures created\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`stat_boxplot()`)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. I added two variables in the dataset; Total Cholesterol and Age group/ age category.For Please check the Code book to see the dictionary or code of added variables."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nAgecat\n0\n1\nFALSE\n3\n3: 4, 2: 3, 0: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nEmp_status\n0\n1\nFALSE\n3\n3: 4, 2: 3, 0: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nChol\n0\n1\nNA\nNA\nNA\n201.11111\n63.15545\n120\n160\n176\n243\n310\n▅▇▁▇▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`        \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                   \n1 Height          height in centimeters                 numeric value &gt;0 or NA  \n2 Weight          weight in kilograms                   numeric value &gt;0 or NA  \n3 Gender          identified gender (male/female/other) M/F/O/NA                \n4 Chol            Total Cholesterol                     &lt;NA&gt;                    \n5 Agecat          Age Chategory                         0 &lt;= 30, 1= 31-40, 2= 4…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n$ Chol   &lt;dbl&gt; 160, 140, 163, 243, 300, 259, 120, 310, 270, 123, 145, 176, 167…\n$ Agecat &lt;dbl&gt; 0, 0, 1, 3, 3, 2, 3, 2, 3, 0, 0, 3, 2, 3\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Chol      \n Length:14          Min.   :  45.0   Length:14          Min.   :120.0  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:148.8  \n Mode  :character   Median :  70.0   Mode  :character   Median :171.5  \n                    Mean   : 602.7                      Mean   :200.8  \n                    3rd Qu.:  90.0                      3rd Qu.:255.0  \n                    Max.   :7000.0                      Max.   :310.0  \n                    NA's   :1                                          \n     Agecat     \n Min.   :0.000  \n 1st Qu.:0.250  \n Median :2.000  \n Mean   :1.786  \n 3rd Qu.:3.000  \n Max.   :3.000  \n                \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender  Chol Agecat\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 180        80 M        160      0\n2 175        70 O        140      0\n3 sixty      60 F        163      1\n4 178        76 F        243      3\n5 192        90 NA       300      3\n6 6          55 F        259      2\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n200.79\n66.29\n120\n148.75\n171.5\n255\n310\n▇▇▁▇▃\n\n\nAgecat\n0\n1.00\n1.79\n1.31\n0\n0.25\n2.0\n3\n3\n▅▁▁▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height)) %&gt;%\n  mutate(Agecat = as.factor(Agecat))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nAgecat\n0\n1\nFALSE\n3\n3: 6, 0: 4, 2: 3\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n203.69\n68.07\n120\n145.00\n176\n259\n310\n▇▆▁▇▃\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nAgecat\n0\n1\nFALSE\n3\n3: 6, 0: 4, 2: 3\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n203.69\n68.07\n120\n145.00\n176\n259\n310\n▇▆▁▇▃\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nAgecat\n0\n1\nFALSE\n3\n3: 6, 2: 3, 0: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nChol\n0\n1\n216.36\n66.24\n120\n163.5\n235\n264.5\n310\n▃▆▁▇▃\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender) \nd3$Emp_status &lt;- as.factor(d3$Agecat)\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\nAgecat\n0\n1\nFALSE\n3\n3: 6, 2: 3, 0: 2\n\n\nEmp_status\n0\n1\nFALSE\n3\n3: 6, 2: 3, 0: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nChol\n0\n1\n216.36\n66.24\n120\n163.5\n235\n264.5\n310\n▃▆▁▇▃\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nAgecat\n0\n1\nFALSE\n3\n3: 4, 2: 3, 0: 2\n\n\nEmp_status\n0\n1\nFALSE\n3\n3: 4, 2: 3, 0: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nChol\n0\n1\n201.11\n63.16\n120\n160\n176\n243\n310\n▅▇▁▇▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata2 &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata2, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Chol                  0             1 201.  63.2  120 160 176 243  310 ▅▇▁▇▂\n4 Agecat                0             1   2    1.22   0   2   2   3    3 ▃▁▁▆▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Data Presentation",
    "section": "",
    "text": "Install Packages\n\ninstall.packages(\"tidyverse\")\n\nThe following package(s) will be installed:\n- tidyverse [2.0.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidyverse ...                      OK [linked from cache]\nSuccessfully installed 1 package in 45 milliseconds.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ninstall.packages(\"ggplot2\")\n\nThe following package(s) will be installed:\n- ggplot2 [3.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggplot2 ...                        OK [linked from cache]\nSuccessfully installed 1 package in 40 milliseconds.\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\ninstall.packages(\"patchwork\")  # This package is to redefine \"/\" operator for plot arrangement\n\nThe following package(s) will be installed:\n- patchwork [1.3.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing patchwork ...                      OK [linked from cache]\nSuccessfully installed 1 package in 40 milliseconds.\n\nlibrary(patchwork)\ninstall.packages(\"writexl\")\n\nThe following package(s) will be installed:\n- writexl [1.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing writexl ...                        OK [linked from cache]\nSuccessfully installed 1 package in 47 milliseconds.\n\nlibrary(writexl)\nlibrary(haven)\ninstall.packages(\"ggforce\")\n\nThe following package(s) will be installed:\n- ggforce [0.4.2]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggforce ...                        OK [linked from cache]\nSuccessfully installed 1 package in 40 milliseconds.\n\nlibrary(ggforce)\ninstall.packages(\"dplyr\")\n\nThe following package(s) will be installed:\n- dplyr [1.1.4]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing dplyr ...                          OK [linked from cache]\nSuccessfully installed 1 package in 39 milliseconds.\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(lubridate)\ninstall.packages(\"ggridges\") \n\nThe following package(s) will be installed:\n- ggridges [0.5.6]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggridges ...                       OK [linked from cache]\nSuccessfully installed 1 package in 39 milliseconds.\n\nlibrary(ggridges)  #\nlibrary(forcats)\ninstall.packages(\"gt\")\n\nThe following package(s) will be installed:\n- gt [0.11.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing gt ...                             OK [linked from cache]\nSuccessfully installed 1 package in 32 milliseconds.\n\nlibrary(gt)\ninstall.packages(\"gtExtras\", dependencies = TRUE)\n\nThe following package(s) will be installed:\n- gtExtras [0.5.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing gtExtras ...                       OK [linked from cache]\nSuccessfully installed 1 package in 29 milliseconds.\n\nlibrary(gtExtras)\n\n\n\nAbout the dataset\nIn this exercise, I use US Weather History Dataset which contains weather summary of several cities. In this exercise, I only use Philadelphia as example, because the chart are same across the cities. I got the dataset and chart from fivethirtyeight. Link: https://fivethirtyeight.com/features/what-12-months-of-record-setting-temperatures-looks-like-across-the-u-s/\nDataset link: https://data.fivethirtyeight.com/\nFigure 1 is the original graph.\n\n#| label: fig-temperature\n#| fig-cap: \"Temperature Record in Philadelphia.\"\n#| echo: FALSE\nknitr::include_graphics(here(\"presentation-exercise\",\"results\",\"figures\",\"phila-ori.png\"))\n\n\n\n\n\n\n\n\n\n\nRead the dataset\n\n# Load the dataset\ndata_loc &lt;- here(\"presentation-exercise\", \"data\", \"KPHL.csv\") \ndata &lt;- read_csv(data_loc)\n\nRows: 365 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): date\ndbl (12): actual_mean_temp, actual_min_temp, actual_max_temp, average_min_te...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata$date &lt;- as.Date(data$date, format=\"%Y-%m-%d\") # convert variable into date format \nhead(data)\n\n# A tibble: 6 × 13\n  date       actual_mean_temp actual_min_temp actual_max_temp average_min_temp\n  &lt;date&gt;                &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 2014-07-01               83              72              93               68\n2 2014-07-02               86              75              96               68\n3 2014-07-03               83              74              92               68\n4 2014-07-04               73              68              78               68\n5 2014-07-05               74              64              83               69\n6 2014-07-06               75              64              86               69\n# ℹ 8 more variables: average_max_temp &lt;dbl&gt;, record_min_temp &lt;dbl&gt;,\n#   record_max_temp &lt;dbl&gt;, record_min_temp_year &lt;dbl&gt;,\n#   record_max_temp_year &lt;dbl&gt;, actual_precipitation &lt;dbl&gt;,\n#   average_precipitation &lt;dbl&gt;, record_precipitation &lt;dbl&gt;\n\n\n\nsummary(data)\n\n      date            actual_mean_temp actual_min_temp actual_max_temp\n Min.   :2014-07-01   Min.   :10.00    Min.   : 2.00   Min.   :17.00  \n 1st Qu.:2014-09-30   1st Qu.:39.00    1st Qu.:33.00   1st Qu.:46.00  \n Median :2014-12-30   Median :59.00    Median :50.00   Median :69.00  \n Mean   :2014-12-30   Mean   :55.88    Mean   :47.27   Mean   :63.98  \n 3rd Qu.:2015-03-31   3rd Qu.:73.00    3rd Qu.:64.00   3rd Qu.:82.00  \n Max.   :2015-06-30   Max.   :86.00    Max.   :77.00   Max.   :96.00  \n average_min_temp average_max_temp record_min_temp  record_max_temp \n Min.   :25.00    Min.   :40.00    Min.   :-11.00   Min.   : 61.00  \n 1st Qu.:32.00    1st Qu.:49.00    1st Qu.:  9.00   1st Qu.: 73.00  \n Median :46.00    Median :66.00    Median : 29.00   Median : 88.00  \n Mean   :47.22    Mean   :64.72    Mean   : 28.03   Mean   : 84.99  \n 3rd Qu.:63.00    3rd Qu.:81.00    3rd Qu.: 46.00   3rd Qu.: 97.00  \n Max.   :70.00    Max.   :87.00    Max.   : 59.00   Max.   :106.00  \n record_min_temp_year record_max_temp_year actual_precipitation\n Min.   :1872         Min.   :1874         Min.   :0.0000      \n 1st Qu.:1930         1st Qu.:1931         1st Qu.:0.0000      \n Median :1960         Median :1957         Median :0.0000      \n Mean   :1947         Mean   :1958         Mean   :0.1245      \n 3rd Qu.:1969         3rd Qu.:1990         3rd Qu.:0.0500      \n Max.   :2014         Max.   :2014         Max.   :2.0100      \n average_precipitation record_precipitation\n Min.   :0.0800        Min.   :0.850       \n 1st Qu.:0.1000        1st Qu.:1.550       \n Median :0.1100        Median :1.880       \n Mean   :0.1138        Mean   :2.135       \n 3rd Qu.:0.1200        3rd Qu.:2.490       \n Max.   :0.1500        Max.   :8.020       \n\n\n\n\nPrompt to AI\nMe: Hi Good morning, thanks for being my best friend for coding activities.\nChatGPT: Of course, I will be always be your friend, what can I help you?\nMe: Today, I want to regenerate a chart. Here I provide the picture of the chart, the original link of the chart, and the dataset. Please generate the code to create the exact same chart, including the color, display, and also legend. Please look at the chart very details to be able to create the same chart. Please generate the code in R.\n\n# Create the plot\nplot_1 &lt;- ggplot(data, aes(x = date)) +\n  # Shaded area for normal range\n  geom_ribbon(aes(ymin = average_min_temp, ymax = average_max_temp), fill = \"tan\", alpha = 0.5) +\n  # Shaded area for record range\n  geom_ribbon(aes(ymin = record_min_temp, ymax = record_max_temp), fill = \"grey\", alpha = 0.3) +\n  # Lines for actual min and max temps\n  geom_linerange(aes(ymin = actual_min_temp, ymax = actual_max_temp), color = \"black\") +\n  # Points for new record highs and lows\n  geom_point(data = filter(data, actual_max_temp &gt; record_max_temp), aes(y = actual_max_temp), color = \"red\", size = 2) +\n  geom_point(data = filter(data, actual_min_temp &lt; record_min_temp), aes(y = actual_min_temp), color = \"blue\", size = 2) +\n  # Labels and themes\n  labs(title = \"Philadelphia\",\n       x = NULL,\n       y = \"Temperature (°F)\",\n       caption = \"Data source: wunderground.com\\nAuthor: Randy Olson (randalolson.com / @randal_olson)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"))\n\nplot(plot_1)\n\n\n\n\n\n\n\n\nMe: Hi, thank you for generating the code. However, there are some detail different with the original chart.\nHere, I modify by myself with some help from ChatGPT. In this step, I try to adjust the line. I see from the original chart, it is look more bar than line.\n\n# Create the plot\nplot_2 &lt;- ggplot(data, aes(x = date)) +\n  # Shaded area for normal range\n  geom_ribbon(aes(ymin = average_min_temp, ymax = average_max_temp), fill = \"#9B9B9B\", alpha = 1) +\n  # Shaded area for record range\n  geom_ribbon(aes(ymin = record_min_temp, ymax = record_max_temp), fill = \"#696651\", alpha = 0.3) +\n  geom_linerange(aes(ymin = actual_min_temp, ymax = actual_max_temp), color = \"black\", size = 0.8) +\n  geom_linerange(aes(ymin = actual_min_temp, ymax = actual_max_temp), color = \"#4B3B47\", size = 2, alpha = 0.8)+\n  geom_point(data = filter(data, actual_max_temp &gt; record_max_temp), aes(y = actual_max_temp), color = \"red\", size = 10) +\n  geom_point(data = filter(data, actual_min_temp &lt; record_min_temp), aes(y = actual_min_temp), color = \"blue\", size = 2) +\n  # Labels and themes\n  labs(title = \"Philadelphia\",\n       x = NULL,\n       y = \"Temperature (°F)\",\n       caption = \"Data source: wunderground.com\\nAuthor: Randy Olson (randalolson.com / @randal_olson)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplot(plot_2)\n\n\n\n\n\n\n\n\nThis is the step where I change many things, including, put the date (months) in both sides (top and bottom, and put ’14 below Jul, and ’15 below Jan), I also put the temperature both side. In the previous chart, the grip also did not show every month. Therefore, I adjust the grip into each month and the temperature for every 10 Fahrenheit degree. I adjust the position of the caption (source and author).\n\nplot_3 &lt;- ggplot(data, aes(x = date)) +\n  # Shaded area for normal range\n  geom_ribbon(aes(ymin = average_min_temp, ymax = average_max_temp), fill = \"#9B9B9B\", alpha = 1) +\n  # Shaded area for record range\n  geom_ribbon(aes(ymin = record_min_temp, ymax = record_max_temp), fill = \"#696651\", alpha = 0.3) +\n  # Temperature ranges\n  geom_linerange(aes(ymin = actual_min_temp, ymax = actual_max_temp), color = \"black\", linewidth = 0.8) +\n  geom_linerange(aes(ymin = actual_min_temp, ymax = actual_max_temp), color = \"#4B3B47\", linewidth = 2, linetype = 1) +\n  # Record-breaking temperatures (Fixed)\n  geom_point(data = data %&gt;% filter(!is.na(actual_max_temp), actual_max_temp &gt; record_max_temp),\n             aes(x = date, y = actual_max_temp), color = \"red\", size = 3) +\n  geom_point(data = data %&gt;% filter(!is.na(actual_min_temp), actual_min_temp &lt; record_min_temp),\n             aes(x = date, y = actual_min_temp), color = \"blue\", size = 3) +\n  # Labels and themes\n  labs(title = \"Philadelphia\",\n       x = NULL,\n       y = \"Temperature (°F)\",\n       caption = \"Data source: wunderground.com\\nAuthor: Randy Olson (randalolson.com / @randal_olson)\") +\n  # Custom theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    panel.background = element_rect(fill = \"white\", color = NA),\n    panel.grid.major = element_line(color = \"grey50\", linetype = \"dashed\"),  # Remove empty linetype value\n    panel.grid.minor = element_blank(),\n    # Align caption to the left\n    plot.caption.position = \"plot\",\n    plot.caption = element_text(hjust = 0, face = \"italic\", size = 10)\n  ) +\n  # Define x-axis (date) for both top and bottom\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\",\n               sec.axis = dup_axis(name = NULL)) +  # Duplicate axis at top\n  # Define y-axis (temperature) for both left and right\n  scale_y_continuous(breaks = seq(-10, 110, 10),\n                     sec.axis = dup_axis(name = NULL),\n                     limits = c(min(data$record_min_temp) - 5, max(data$record_max_temp) + 5))  # Ensure all values fit\n\n# Plot the chart\nplot(plot_3)\n\n\n\n\n\n\n\n\nThe chart looks much better. However, the new record for high and low temperature did not show up in the chart. Therefore, I try to create a new set then insert it into the chart.\n\n# Identify new record highs and lows\nweather_data_subset &lt;- data\nnew_max_records &lt;- weather_data_subset %&gt;%\n  filter(record_max_temp &lt;= actual_max_temp)\n\nnew_min_records &lt;- weather_data_subset %&gt;%\n  filter(record_min_temp &gt;= actual_min_temp)\n\n# Insert the new record into the plot \nplot1 &lt;- ggplot(weather_data_subset, aes(x = date)) +\n  # Shaded area for normal range\n  geom_ribbon(aes(ymin = average_min_temp, ymax = average_max_temp), fill = \"#73736D\", alpha = 0.9) +\n  # Shaded area for record range\n  geom_ribbon(aes(ymin = record_min_temp, ymax = record_max_temp), fill =  \"#9a9a77\", alpha = 0.7) +\n  # Temperature ranges\n  geom_linerange(aes(ymin = actual_min_temp, ymax = actual_max_temp), color = \"black\", linewidth = 0.8) +\n  geom_linerange(aes(ymin = actual_min_temp, ymax = actual_max_temp), color = \"#4B3B47\", linewidth = 1, alpha = 0.8) +\n  # Record-breaking temperatures\n  geom_point(data = new_max_records, aes(x = date, y = actual_max_temp), color = \"red\", size = 2) +\n  geom_point(data = new_min_records, aes(x = date, y = actual_min_temp), color = \"lightblue\", size = 2) +\n  # create the legend \n  geom_rect(aes(xmin = as.Date(\"2014-08-10\") -4, xmax = as.Date(\"2014-08-11\") + 4, ymin = 0, ymax = 40), fill = \"#9a9a77\", alpha = 0.7) + # First bar\n  geom_rect(aes(xmin = as.Date(\"2014-08-10\") -4, xmax = as.Date(\"2014-08-11\") + 4, ymin = 10, ymax = 30), fill = \"#73736D\", alpha = 0.9) + # Second bar\n  geom_rect(aes(xmin = as.Date(\"2014-08-10\") -4, xmax = as.Date(\"2014-08-11\") + 4, ymin = 15, ymax = 25), fill = \"#4B3B47\", alpha = 0.8) +   # Third bar\n  ## Add two points inside the horizontal bar\n  geom_point(aes(x = as.Date(\"2014-08-10\"), y = 42), color = \"red\", size = 3) +  # Point 1\n  geom_point(aes(x = as.Date(\"2014-08-10\"), y = -2), color = \"lightblue\", size = 3) +  # Point 2\n  geom_text(aes(x = as.Date(\"2014-09-10\"), y = 25, label = \"New \\nrecord high\"), family= \"Serif\", color = \"#73736D\", vjust = -1, size = 3) + # adding text \n  geom_text(aes(x = as.Date(\"2014-09-10\"), y = -12, label = \"New \\nrecod low\"), family = \"Serif\",color = \"#73736D\", vjust = -1, size = 3) + # Adding text \n  geom_text(aes(x = as.Date(\"2014-07-10\"), y = 30, label = \"Record high\"), family =\"Serif\", color =\"#73736D\", vjust = -1, size = 3) + # adding text \n  geom_text(aes(x = as.Date(\"2014-07-10\"), y = 2, label = \"Recod low\"), family =\"Serif\", color = \"#73736D\", vjust = -1, size = 3) + # Adding text \n  geom_text(aes(x = as.Date(\"2014-09-10\"), y = 15, label = \"Normal range\"), family = \"Serif\", color =\"#73736D\", vjust = -1, size = 3) + # adding text \n  geom_text(aes(x = as.Date(\"2014-07-06\"), y = 14, label = \"Actual range\"), family = \"Serif\",color = \"#73736D\", vjust = -1, size = 3) + # Adding text \n  # Labels and themes\n  labs(title = \"Philadelphia\",\n       x = NULL,\n       y = \"Temperature (°F)\",\n       caption = \"Data source: wunderground.com\\nAuthor: Randy Olson (randalolson.com / @randal_olson)\") +\n  # Custom theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    panel.background = element_rect(fill = \"white\", color = NA),\n    panel.grid.major = element_line(color = \"grey50\", linetype = \"longdash\", linewidth= 0.2),\n    panel.grid.minor = element_blank(),\n    plot.caption.position = \"plot\",\n    plot.caption = element_text(hjust = 0, size = 10),\n    axis.text.x = element_text(size = 12)  # Ensure x-axis labels are readable\n  ) +\n  # Custom x-axis labels\n  scale_x_date(\n    breaks = seq(as.Date(\"2014-07-01\"), as.Date(\"2015-07-01\"), by = \"1 month\"),\n    labels = c(\"Jul\\n'14\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\", \"Jan\\n'15\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\"), # Jul\\'14 is to place '14 under Jul, and the same for jan 15\n    sec.axis = dup_axis(name = NULL)\n  ) +\n  # Define y-axis (temperature)\n  scale_y_continuous(\n    breaks = seq(-10, 110, 10),\n    sec.axis = dup_axis(name = NULL),\n    limits = c(min(weather_data_subset$record_min_temp) - 5, \n               max(weather_data_subset$record_max_temp) + 5),\n    labels = function(x) paste0(x, \"°F\"))  # Add degree symbol to y-axis labels\n  \n# save the chart\n\nchart_location &lt;- here( \"presentation-exercise\",\"results\", \"figures\", \"philadelphia.png\") # to set up location for the pictures created \nggsave(filename = chart_location, plot=plot1, width = 8, height = 6, units = \"in\", dpi = 300) # save the pictures created \n\nWarning in geom_point(aes(x = as.Date(\"2014-08-10\"), y = 42), color = \"red\", : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_point(aes(x = as.Date(\"2014-08-10\"), y = -2), color = \"lightblue\", : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = as.Date(\"2014-09-10\"), y = 25, label = \"New \\nrecord high\"), : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = as.Date(\"2014-09-10\"), y = -12, label = \"New \\nrecod low\"), : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = as.Date(\"2014-07-10\"), y = 30, label = \"Record high\"), : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = as.Date(\"2014-07-10\"), y = 2, label = \"Recod low\"), : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = as.Date(\"2014-09-10\"), y = 15, label = \"Normal range\"), : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = as.Date(\"2014-07-06\"), y = 14, label = \"Actual range\"), : All aesthetics have length 1, but the data has 365 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nThe most challenging part in this chart is the legend. I spent hours to work on it. Finally I made it, but not exactly same.\n\n\n\n\n\n\n\n\nFigure 1: Temperature Record in Philadelphia (imitation)\n\n\n\n\n\n\n\nPart 2: Creating a table\nI create a table containing weather summary of Philadelphia in each season. To create this table, I got help from ChatGPT, I added multiple prompt step by step. I went through multiple steps.\nBefore asking AI to generate the code to create a table, I create subsets for each seasons.\n\n#extract data\nweather_data &lt;- data\n\n\n# Extract month for filtering\nweather_data$Month &lt;- format(data$date, \"%m\")\n\n# Create subsets\nsummer &lt;- filter(weather_data, Month %in% c(\"06\", \"07\", \"08\"))  # June - August\nfall &lt;- filter(weather_data, Month %in% c(\"09\", \"10\", \"11\"))    # September - November\nwinter &lt;- filter(weather_data, Month %in% c(\"12\", \"01\", \"02\"))  # December - February\nspring &lt;- filter(weather_data, Month %in% c(\"03\", \"04\", \"05\"))  # March - May\n\nsummary(summer)\n\n      date            actual_mean_temp actual_min_temp actual_max_temp\n Min.   :2014-07-01   Min.   :56.00    Min.   :53.00   Min.   :58.00  \n 1st Qu.:2014-07-23   1st Qu.:73.00    1st Qu.:64.00   1st Qu.:81.00  \n Median :2014-08-15   Median :76.00    Median :68.00   Median :84.50  \n Mean   :2014-11-12   Mean   :76.14    Mean   :67.30   Mean   :84.35  \n 3rd Qu.:2015-06-07   3rd Qu.:80.00    3rd Qu.:70.25   3rd Qu.:89.00  \n Max.   :2015-06-30   Max.   :86.00    Max.   :77.00   Max.   :96.00  \n average_min_temp average_max_temp record_min_temp record_max_temp \n Min.   :59.00    Min.   :79.00    Min.   :44.00   Min.   : 93.00  \n 1st Qu.:66.00    1st Qu.:84.00    1st Qu.:48.75   1st Qu.: 98.00  \n Median :68.00    Median :86.00    Median :52.00   Median : 99.00  \n Mean   :67.09    Mean   :85.04    Mean   :51.68   Mean   : 99.16  \n 3rd Qu.:69.00    3rd Qu.:87.00    3rd Qu.:55.00   3rd Qu.:100.25  \n Max.   :70.00    Max.   :87.00    Max.   :59.00   Max.   :106.00  \n record_min_temp_year record_max_temp_year actual_precipitation\n Min.   :1884         Min.   :1888         Min.   :0.0000      \n 1st Qu.:1952         1st Qu.:1930         1st Qu.:0.0000      \n Median :1963         Median :1957         Median :0.0000      \n Mean   :1960         Mean   :1961         Mean   :0.1818      \n 3rd Qu.:1972         3rd Qu.:1994         3rd Qu.:0.1950      \n Max.   :1999         Max.   :2012         Max.   :1.9600      \n average_precipitation record_precipitation    Month          \n Min.   :0.0900        Min.   :1.040        Length:92         \n 1st Qu.:0.1100        1st Qu.:1.958        Class :character  \n Median :0.1200        Median :2.440        Mode  :character  \n Mean   :0.1226        Mean   :2.689                          \n 3rd Qu.:0.1400        3rd Qu.:3.050                          \n Max.   :0.1500        Max.   :8.020                          \n\nsummary(fall)\n\n      date            actual_mean_temp actual_min_temp actual_max_temp\n Min.   :2014-09-01   Min.   :28.00    Min.   :20.00   Min.   :36.00  \n 1st Qu.:2014-09-23   1st Qu.:50.50    1st Qu.:42.00   1st Qu.:57.00  \n Median :2014-10-16   Median :61.00    Median :52.00   Median :70.00  \n Mean   :2014-10-16   Mean   :58.79    Mean   :50.54   Mean   :66.55  \n 3rd Qu.:2014-11-07   3rd Qu.:67.00    3rd Qu.:59.00   3rd Qu.:75.00  \n Max.   :2014-11-30   Max.   :84.00    Max.   :74.00   Max.   :93.00  \n average_min_temp average_max_temp record_min_temp record_max_temp \n Min.   :35.00    Min.   :50.00    Min.   : 8.00   Min.   : 70.00  \n 1st Qu.:41.50    1st Qu.:59.00    1st Qu.:25.00   1st Qu.: 78.00  \n Median :48.00    Median :67.00    Median :31.00   Median : 87.00  \n Mean   :49.34    Mean   :66.95    Mean   :31.53   Mean   : 85.23  \n 3rd Qu.:57.50    3rd Qu.:75.00    3rd Qu.:39.00   3rd Qu.: 92.00  \n Max.   :65.00    Max.   :83.00    Max.   :53.00   Max.   :102.00  \n record_min_temp_year record_max_temp_year actual_precipitation\n Min.   :1875         Min.   :1879         Min.   :0.0000      \n 1st Qu.:1944         1st Qu.:1927         1st Qu.:0.0000      \n Median :1961         Median :1949         Median :0.0000      \n Mean   :1954         Mean   :1949         Mean   :0.0911      \n 3rd Qu.:1969         3rd Qu.:1975         3rd Qu.:0.0100      \n Max.   :2013         Max.   :2014         Max.   :1.0500      \n average_precipitation record_precipitation    Month          \n Min.   :0.0800        Min.   :0.900        Length:91         \n 1st Qu.:0.1000        1st Qu.:1.600        Class :character  \n Median :0.1100        Median :1.950        Mode  :character  \n Mean   :0.1093        Mean   :2.276                          \n 3rd Qu.:0.1200        3rd Qu.:2.725                          \n Max.   :0.1400        Max.   :6.630                          \n\nsummary(winter)\n\n      date            actual_mean_temp actual_min_temp actual_max_temp\n Min.   :2014-12-01   Min.   :10.00    Min.   : 2.00   Min.   :17.00  \n 1st Qu.:2014-12-23   1st Qu.:27.25    1st Qu.:20.00   1st Qu.:34.00  \n Median :2015-01-14   Median :34.00    Median :27.00   Median :41.00  \n Mean   :2015-01-14   Mean   :33.06    Mean   :25.74   Mean   :39.87  \n 3rd Qu.:2015-02-05   3rd Qu.:39.75    3rd Qu.:33.00   3rd Qu.:45.75  \n Max.   :2015-02-28   Max.   :56.00    Max.   :47.00   Max.   :65.00  \n average_min_temp average_max_temp record_min_temp   record_max_temp\n Min.   :25.00    Min.   :40.00    Min.   :-11.000   Min.   :61.00  \n 1st Qu.:26.00    1st Qu.:41.00    1st Qu.:  0.000   1st Qu.:65.00  \n Median :27.00    Median :42.00    Median :  3.000   Median :68.00  \n Mean   :27.84    Mean   :42.94    Mean   :  2.778   Mean   :68.03  \n 3rd Qu.:29.00    3rd Qu.:45.00    3rd Qu.:  6.750   3rd Qu.:70.00  \n Max.   :34.00    Max.   :50.00    Max.   : 15.000   Max.   :79.00  \n record_min_temp_year record_max_temp_year actual_precipitation\n Min.   :1875         Min.   :1874         Min.   :0.0000      \n 1st Qu.:1897         1st Qu.:1941         1st Qu.:0.0000      \n Median :1942         Median :1968         Median :0.0000      \n Mean   :1937         Mean   :1963         Mean   :0.1128      \n 3rd Qu.:1977         3rd Qu.:1996         3rd Qu.:0.0475      \n Max.   :2014         Max.   :2013         Max.   :1.8400      \n average_precipitation record_precipitation    Month          \n Min.   :0.0900        Min.   :0.900        Length:90         \n 1st Qu.:0.0900        1st Qu.:1.343        Class :character  \n Median :0.1000        Median :1.585        Mode  :character  \n Mean   :0.1027        Mean   :1.687                          \n 3rd Qu.:0.1100        3rd Qu.:1.847                          \n Max.   :0.1300        Max.   :3.860                          \n\nsummary(spring)\n\n      date            actual_mean_temp actual_min_temp actual_max_temp\n Min.   :2015-03-01   Min.   :19.00    Min.   :11.00   Min.   :26.00  \n 1st Qu.:2015-03-23   1st Qu.:44.00    1st Qu.:34.00   1st Qu.:51.75  \n Median :2015-04-15   Median :56.00    Median :46.00   Median :66.50  \n Mean   :2015-04-15   Mean   :55.09    Mean   :45.04   Mean   :64.67  \n 3rd Qu.:2015-05-08   3rd Qu.:65.50    3rd Qu.:54.00   3rd Qu.:77.00  \n Max.   :2015-05-31   Max.   :81.00    Max.   :71.00   Max.   :92.00  \n average_min_temp average_max_temp record_min_temp record_max_temp\n Min.   :30.00    Min.   :48.00    Min.   : 5.00   Min.   :71.00  \n 1st Qu.:36.75    1st Qu.:55.75    1st Qu.:14.00   1st Qu.:82.00  \n Median :44.00    Median :64.00    Median :27.00   Median :90.00  \n Mean   :44.22    Mean   :63.51    Mean   :25.62   Mean   :87.16  \n 3rd Qu.:51.25    3rd Qu.:71.25    3rd Qu.:35.25   3rd Qu.:92.25  \n Max.   :59.00    Max.   :78.00    Max.   :43.00   Max.   :97.00  \n record_min_temp_year record_max_temp_year actual_precipitation\n Min.   :1872         Min.   :1880         Min.   :0.0000      \n 1st Qu.:1894         1st Qu.:1936         1st Qu.:0.0000      \n Median :1956         Median :1962         Median :0.0000      \n Mean   :1938         Mean   :1958         Mean   :0.1118      \n 3rd Qu.:1966         3rd Qu.:1990         3rd Qu.:0.0300      \n Max.   :2002         Max.   :2013         Max.   :2.0100      \n average_precipitation record_precipitation    Month          \n Min.   :0.1000        Min.   :0.850        Length:92         \n 1st Qu.:0.1100        1st Qu.:1.478        Class :character  \n Median :0.1200        Median :1.765        Mode  :character  \n Mean   :0.1202        Mean   :1.882                          \n 3rd Qu.:0.1300        3rd Qu.:2.192                          \n Max.   :0.1400        Max.   :4.420                          \n\n\nIn this step, I used ChatGPT to create table. Here is the prompt that I used:\nNow I have 4 subsets: summer, fall,, winter, and spring. Those subsets have actual_mean_temp, average_min_temp, average_max_temp, average_precipitation. I want to create a beautiful and easy read table. Please give me idea and step by step, including code.\nThe first table generated did not look good. Therefore, I add more prompt in each feature that I want. For example, I want to add icons representing the season, I want to arrange indicators in the column, and seasons as rows. I actually added more prompt.\n\n# Summarize the data for each season\nsummary_table &lt;- bind_rows(\n  summer %&gt;% summarise(Season = \"Summer\", \n                       `Avg Min Temp (°F)` = mean(average_min_temp, na.rm = TRUE),\n                       `Avg Mean Temp (°F)` = mean(actual_mean_temp, na.rm = TRUE),\n                       `Avg Max Temp (°F)` = mean(average_max_temp, na.rm = TRUE),\n                       `Avg Precipitation (inches)` = mean(average_precipitation, na.rm = TRUE)),\n  fall %&gt;% summarise(Season = \"Fall\", \n                     `Avg Min Temp (°F)` = mean(average_min_temp, na.rm = TRUE),\n                     `Avg Mean Temp (°F)` = mean(actual_mean_temp, na.rm = TRUE),\n                     `Avg Max Temp (°F)` = mean(average_max_temp, na.rm = TRUE),\n                     `Avg Precipitation (inches)` = mean(average_precipitation, na.rm = TRUE)),\n  winter %&gt;% summarise(Season = \"Winter\", \n                       `Avg Min Temp (°F)` = mean(average_min_temp, na.rm = TRUE),\n                       `Avg Mean Temp (°F)` = mean(actual_mean_temp, na.rm = TRUE),\n                       `Avg Max Temp (°F)` = mean(average_max_temp, na.rm = TRUE),\n                       `Avg Precipitation (inches)` = mean(average_precipitation, na.rm = TRUE)),\n  spring %&gt;% summarise(Season = \"Spring\", \n                       `Avg Min Temp (°F)` = mean(average_min_temp, na.rm = TRUE),\n                       `Avg Mean Temp (°F)` = mean(actual_mean_temp, na.rm = TRUE),\n                       `Avg Max Temp (°F)` = mean(average_max_temp, na.rm = TRUE),\n                       `Avg Precipitation (inches)` = mean(average_precipitation, na.rm = TRUE))\n)\n\n\n# Add seasonal icons in a separate column\nsummary_table &lt;- summary_table %&gt;%\n  mutate(` ` = c(\"☀️\", \"🍂\", \"❄️\", \"🌸\"))  # Adding icons for Summer, Fall, Winter, Spring\n\n# Create a styled gt table\nstyled_table &lt;- summary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = md(\"**Philadelphia Seasonal Weather Summary**\"),\n    subtitle = md(\"*Weather data from July 2014 - June 2015*\")\n  ) %&gt;%\n  tab_spanner(label = \"Temperature (°F)\", columns = c(`Avg Min Temp (°F)`, `Avg Mean Temp (°F)`, `Avg Max Temp (°F)`)) %&gt;%\n  tab_spanner(label = \"Precipitation\", columns = `Avg Precipitation (inches)`) %&gt;%\n  fmt_number(\n    columns = where(is.numeric),\n    decimals = 1\n  ) %&gt;%\n  cols_move_to_start(columns = \"Season\") %&gt;%\n  cols_move(columns = \" \", after = \"Season\") %&gt;%  # Moves icon column after season\n  cols_align(\n    align = \"left\",\n    columns = \"Season\"  # Keep Season column left-aligned\n  ) %&gt;%\n  cols_align(\n    align = \"center\",\n    columns = c(` `, `Avg Min Temp (°F)`, `Avg Mean Temp (°F)`, `Avg Max Temp (°F)`, `Avg Precipitation (inches)`)  # Center all other columns\n  ) %&gt;%\n  tab_source_note(md(\"**Source:** https://www.wunderground.com/, July 2014 - June 2015.\")) %&gt;%\n  tab_footnote(footnote = \"Precipitation measured in inches\", locations = cells_column_labels(columns = `Avg Precipitation (inches)`))%&gt;%\n  data_color(\n    columns = c(`Avg Min Temp (°F)`, `Avg Max Temp (°F)`),\n    colors = scales::col_numeric(\n      palette = c(\"blue\", \"white\", \"red\"),\n      domain = range(summary_table$`Avg Min Temp (°F)`, summary_table$`Avg Max Temp (°F)`)\n    )\n  )\n\nWarning: Since gt v0.9.0, the `colors` argument has been deprecated.\n• Please use the `fn` argument instead.\nThis warning is displayed once every 8 hours.\n\ntable_file1 = here(\"presentation-exercise\",\"results\", \"tables\", \"weather.rds\")\nsaveRDS(styled_table, file = table_file1)\n\n# Print the table\nstyled_table\n\n\n\n\n  \n    \n      Philadelphia Seasonal Weather Summary\n    \n    \n      Weather data from July 2014 - June 2015\n    \n    \n      Season\n       \n      \n        Temperature (°F)\n      \n      \n        Precipitation\n      \n    \n    \n      Avg Min Temp (°F)\n      Avg Mean Temp (°F)\n      Avg Max Temp (°F)\n      Avg Precipitation (inches)1\n    \n  \n  \n    Summer\n☀️\n67.1\n76.1\n85.0\n0.1\n    Fall\n🍂\n49.3\n58.8\n66.9\n0.1\n    Winter\n❄️\n27.8\n33.1\n42.9\n0.1\n    Spring\n🌸\n44.2\n55.1\n63.5\n0.1\n  \n  \n    \n      Source: https://www.wunderground.com/, July 2014 - June 2015.\n    \n  \n  \n    \n      1 Precipitation measured in inches"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Models Exercise",
    "section": "",
    "text": "install.packages(\"tidyverse\")\n\nThe following package(s) will be installed:\n- tidyverse [2.0.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidyverse ...                      OK [linked from cache]\nSuccessfully installed 1 package in 30 milliseconds.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ninstall.packages(\"ggplot2\")\n\nThe following package(s) will be installed:\n- ggplot2 [3.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggplot2 ...                        OK [linked from cache]\nSuccessfully installed 1 package in 24 milliseconds.\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\ninstall.packages(\"patchwork\")  # This package is to redefine \"/\" operator for plot arrangement\n\nThe following package(s) will be installed:\n- patchwork [1.3.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing patchwork ...                      OK [linked from cache]\nSuccessfully installed 1 package in 27 milliseconds.\n\nlibrary(patchwork)\ninstall.packages(\"writexl\")\n\nThe following package(s) will be installed:\n- writexl [1.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing writexl ...                        OK [linked from cache]\nSuccessfully installed 1 package in 23 milliseconds.\n\nlibrary(writexl)\nlibrary(haven)\ninstall.packages(\"ggforce\")\n\nThe following package(s) will be installed:\n- ggforce [0.4.2]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggforce ...                        OK [linked from cache]\nSuccessfully installed 1 package in 30 milliseconds.\n\nlibrary(ggforce)\ninstall.packages(\"dplyr\")\n\nThe following package(s) will be installed:\n- dplyr [1.1.4]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing dplyr ...                          OK [linked from cache]\nSuccessfully installed 1 package in 28 milliseconds.\n\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(lubridate)\ninstall.packages(\"ggridges\") \n\nThe following package(s) will be installed:\n- ggridges [0.5.6]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggridges ...                       OK [linked from cache]\nSuccessfully installed 1 package in 27 milliseconds.\n\nlibrary(ggridges)  #\nlibrary(forcats)\ninstall.packages(\"gt\")\n\nThe following package(s) will be installed:\n- gt [0.11.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing gt ...                             OK [linked from cache]\nSuccessfully installed 1 package in 29 milliseconds.\n\nlibrary(gt)\ninstall.packages(\"gtExtras\", dependencies = TRUE)\n\nThe following package(s) will be installed:\n- gtExtras [0.5.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing gtExtras ...                       OK [linked from cache]\nSuccessfully installed 1 package in 28 milliseconds.\n\nlibrary(gtExtras)\ninstall.packages(\"gtsummary\")\n\nThe following package(s) will be installed:\n- gtsummary [2.1.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing gtsummary ...                      OK [linked from cache]\nSuccessfully installed 1 package in 26 milliseconds.\n\nlibrary(gtsummary)   \ninstall.packages(\"cli\")\n\nThe following package(s) will be installed:\n- cli [3.6.4]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing cli ...                            OK [linked from cache]\nSuccessfully installed 1 package in 30 milliseconds.\n\nlibrary(cli)\ninstall.packages(\"tidymodels\")\n\nThe following package(s) will be installed:\n- tidymodels [1.3.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidymodels ...                     OK [linked from cache]\nSuccessfully installed 1 package in 28 milliseconds.\n\nlibrary(tidymodels)  # for the parsnip package, along with the rest of tidymodels\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\ninstall.packages(\"broom.mixed\")\n\nThe following package(s) will be installed:\n- broom.mixed [0.2.9.6]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing broom.mixed ...                    OK [linked from cache]\nSuccessfully installed 1 package in 30 milliseconds.\n\nlibrary(broom.mixed) # for converting bayesian models to tidy tibbles\ninstall.packages(\"dotwhisker\")\n\nThe following package(s) will be installed:\n- dotwhisker [0.8.3]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing dotwhisker ...                     OK [linked from cache]\nSuccessfully installed 1 package in 42 milliseconds.\n\nlibrary(dotwhisker)  # for visualizing regression results\ninstall.packages(\"ggcorrplot\")\n\nThe following package(s) will be installed:\n- ggcorrplot [0.1.4.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggcorrplot ...                     OK [linked from cache]\nSuccessfully installed 1 package in 28 milliseconds.\n\nlibrary(ggcorrplot)\ninstall.packages(\"corrplot\")\n\nThe following package(s) will be installed:\n- corrplot [0.95]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing corrplot ...                       OK [linked from cache]\nSuccessfully installed 1 package in 27 milliseconds.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ninstall.packages(\"ggpubr\")# combine plot\n\nThe following package(s) will be installed:\n- ggpubr [0.6.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggpubr ...                         OK [linked from cache]\nSuccessfully installed 1 package in 42 milliseconds.\n\n# Load the library\nlibrary(ggpubr)\n\ninstall.packages(\"broom\") \n\nThe following package(s) will be installed:\n- broom [1.0.7]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing broom ...                          OK [linked from cache]\nSuccessfully installed 1 package in 38 milliseconds.\n\nlibrary(broom)# Installs broom separately (optional)\ninstall.packages(\"gtsummary\")\n\nThe following package(s) will be installed:\n- gtsummary [2.1.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing gtsummary ...                      OK [linked from cache]\nSuccessfully installed 1 package in 41 milliseconds.\n\nlibrary(gtsummary)"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#data-exploration",
    "href": "fitting-exercise/fitting-exercise.html#data-exploration",
    "title": "Fitting Models Exercise",
    "section": "Data Exploration",
    "text": "Data Exploration\nInitial data visualization\nFirst of all, it is important to visualize the main variable of interest. In this case, Mavoglurant is the main variable interest (variable response). Spaghetti plot is created to show the individual level of Mavoglurant over the time based on Dose (25, 37.5, and 50).\n\n# Spaghetti plot\nspaghetti_pot &lt;- ggplot(data, aes(x = TIME, y = DV, group = ID, color = as.factor(ID))) +\n  geom_line(alpha = 0.6) +  # Adds individual lines with transparency\n  facet_wrap(~DOSE)+ # to facet by dose\n  theme_minimal() +  # Uses a clean theme\n  labs(title = \"Individual level of Mavoglurant over Time by Dose\",\n       x = \"Time\",\n       y = \"DV\",\n       color = \"Subject ID\") +\n  theme(legend.position = \"none\")  # Hides legend if too many IDs\nprint(spaghetti_pot)\n\n\n\n\n\n\n\n\nOccasion Rate\nA bar chart for Occasion Rate (OCC) is created to look at the distribution of OCC. OCC is one of the interest in this exercise.\n\nplot_occ &lt;- ggplot(data, aes(x = factor(OCC))) +\n  geom_bar(aes(fill = factor(OCC)), show.legend = FALSE, width = 0.7) +  # Color bars dynamically\n  scale_fill_brewer(palette = \"Dark2\") +  # Use a more vibrant color palette\n  labs(title = \"Distribution of Occasion Rate (OCC)\",\n       x = \"Occasion Rate (OCC)\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +  # Increase text size for readability\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),  # Rotate X labels for clarity\n    panel.grid.major = element_blank(),  # Remove major grid lines for a cleaner look\n    panel.grid.minor = element_blank()\n  )\nprint(plot_occ)"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#data-cleaning",
    "href": "fitting-exercise/fitting-exercise.html#data-cleaning",
    "title": "Fitting Models Exercise",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nFirst step in the data cleaning, we want to select only OCC= 1, and calculate summary of Mavoglurant\n\n#OCC =1 \ndata1 &lt;- data %&gt;%\n  filter(OCC==1) #select rows with OCC=1 only \n\n\nSummary_DV &lt;- data1 %&gt;%\n  filter(TIME != 0) %&gt;%  # Remove rows where TIME is 0\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV, na.rm = TRUE))  # Corrected sum function\nprint(Summary_DV)\n\n# A tibble: 120 × 2\n      ID     Y\n   &lt;dbl&gt; &lt;dbl&gt;\n 1   793 2691.\n 2   794 2639.\n 3   795 2150.\n 4   796 1789.\n 5   797 3126.\n 6   798 2337.\n 7   799 3007.\n 8   800 2796.\n 9   801 3866.\n10   802 1762.\n# ℹ 110 more rows\n\n\nOnly including data with time= 0\n\ntime_zero &lt;- data1 %&gt;%\n  filter(TIME == 0)\nprint(time_zero)\n\n# A tibble: 120 × 17\n      ID   CMT  EVID  EVI2   MDV    DV  LNDV   AMT  TIME  DOSE   OCC  RATE   AGE\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   793     1     1     1     1     0     0    25     0    25     1    75    42\n 2   794     1     1     1     1     0     0    25     0    25     1   150    24\n 3   795     1     1     1     1     0     0    25     0    25     1   150    31\n 4   796     1     1     1     1     0     0    25     0    25     1   150    46\n 5   797     1     1     1     1     0     0    25     0    25     1   150    41\n 6   798     1     1     1     1     0     0    25     0    25     1   150    27\n 7   799     1     1     1     1     0     0    25     0    25     1   150    23\n 8   800     1     1     1     1     0     0    25     0    25     1   150    20\n 9   801     1     1     1     1     0     0    25     0    25     1   150    23\n10   802     1     1     1     1     0     0    25     0    25     1   150    28\n# ℹ 110 more rows\n# ℹ 4 more variables: SEX &lt;dbl&gt;, RACE &lt;dbl&gt;, WT &lt;dbl&gt;, HT &lt;dbl&gt;\n\n\nCombine two data frames (Summary_DV and time_zero)\n\ndata_combined &lt;- left_join(time_zero, Summary_DV, by = join_by(ID))\nprint(data_combined)\n\n# A tibble: 120 × 18\n      ID   CMT  EVID  EVI2   MDV    DV  LNDV   AMT  TIME  DOSE   OCC  RATE   AGE\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1   793     1     1     1     1     0     0    25     0    25     1    75    42\n 2   794     1     1     1     1     0     0    25     0    25     1   150    24\n 3   795     1     1     1     1     0     0    25     0    25     1   150    31\n 4   796     1     1     1     1     0     0    25     0    25     1   150    46\n 5   797     1     1     1     1     0     0    25     0    25     1   150    41\n 6   798     1     1     1     1     0     0    25     0    25     1   150    27\n 7   799     1     1     1     1     0     0    25     0    25     1   150    23\n 8   800     1     1     1     1     0     0    25     0    25     1   150    20\n 9   801     1     1     1     1     0     0    25     0    25     1   150    23\n10   802     1     1     1     1     0     0    25     0    25     1   150    28\n# ℹ 110 more rows\n# ℹ 5 more variables: SEX &lt;dbl&gt;, RACE &lt;dbl&gt;, WT &lt;dbl&gt;, HT &lt;dbl&gt;, Y &lt;dbl&gt;\n\n\n\ndf &lt;- data_combined %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT) %&gt;% #Selecting variables of Interest\n  mutate(RACE = as_factor(RACE), SEX = as_factor(SEX)) #To convert race and sex to factor variables\n\nstr(df)# check the variable classes\n\ntibble [120 × 7] (S3: tbl_df/tbl/data.frame)\n $ Y   : num [1:120] 2691 2639 2150 1789 3126 ...\n $ DOSE: num [1:120] 25 25 25 25 25 25 25 25 25 25 ...\n $ AGE : num [1:120] 42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num [1:120] 94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num [1:120] 1.77 1.76 1.81 1.65 1.56 ..."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#exploratory-data-analysis",
    "href": "fitting-exercise/fitting-exercise.html#exploratory-data-analysis",
    "title": "Fitting Models Exercise",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nIn this section, the data is visualized using appropriate table, charts or graphics based on the data type. it is better to provide a big picture of the data by providing summary table (Characteristic Table). The table will help understand the data better.\nCreate summary table\n\ntbl_summary(df)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1201\n\n\n\n\nY\n2,349 (1,689, 3,054)\n\n\nDOSE\n\n\n\n\n    25\n59 (49%)\n\n\n    37.5\n12 (10%)\n\n\n    50\n49 (41%)\n\n\nAGE\n31 (26, 41)\n\n\nSEX\n\n\n\n\n    1\n104 (87%)\n\n\n    2\n16 (13%)\n\n\nRACE\n\n\n\n\n    1\n74 (62%)\n\n\n    2\n36 (30%)\n\n\n    7\n2 (1.7%)\n\n\n    88\n8 (6.7%)\n\n\nWT\n82 (73, 90)\n\n\nHT\n1.77 (1.70, 1.82)\n\n\n\n1 Median (Q1, Q3); n (%)\n\n\n\n\n\n\n\n\nBased on the table above, 120 participants received a single 25 mg, 37.5 mg or 50 mg of MVG (49%, 10%, and 41% respectively). Most subjects were young (mean age of 33 years), race group one (62 %), male (87 %) with a mean BW of 83 kg and mean of HT of 1.76 m.\nSummary Table for all variables based on sex\n\n# Summary table for all variables by SEX\ndf %&gt;%\n  tbl_summary(by=SEX, type=list(where(is.numeric) ~ \"continuous\"), # Specifies that all numeric variables should be treated as\n              statistic=list(all_continuous() ~ \"{median} ({p25}, {p75})\"), #Numeric (continuous) variables will be summarized using the median and interquartile range (IQR: 25th and 75th percentiles).\n              digits=list(all_continuous() ~ 0, HT ~ 2), # Specifies that all continuous variables should be rounded to 0 decimal places, except for HT (Height), which is rounded to 2 decimal places.\n\n              label=list(Y ~ \"Response\",\n                         DOSE ~ \"Drug dose\",\n                         AGE ~ \"Age\",\n                         RACE ~ \"Race\",\n                         WT ~ \"Weight\",\n                         HT ~ \"Height\")) %&gt;%\n  add_p(test=list(all_continuous() ~ \"wilcox.test\",\n                  all_categorical() ~ \"fisher.test\"), # test differences between groups (SEX in this case).\n        pvalue_fun=function(x) style_number(x, digits=3)) %&gt;%\n  modify_header(p.value=\"*p*-value\") %&gt;%\n  modify_spanning_header(all_stat_cols() ~ \"**Sex**\") %&gt;%\n  as_gt()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nSex\n\np-value2\n\n\n1 N = 1041\n2 N = 161\n\n\n\n\nResponse\n2,398 (1,722, 3,083)\n2,060 (1,478, 2,728)\n0.296\n\n\nDrug dose\n38 (25, 50)\n25 (25, 44)\n0.191\n\n\nAge\n30 (25, 39)\n42 (38, 46)\n0.000\n\n\nRace\n\n\n\n\n0.324\n\n\n    1\n63 (61%)\n11 (69%)\n\n\n\n\n    2\n33 (32%)\n3 (19%)\n\n\n\n\n    7\n1 (1.0%)\n1 (6.3%)\n\n\n\n\n    88\n7 (6.7%)\n1 (6.3%)\n\n\n\n\nWeight\n83 (75, 92)\n70 (63, 82)\n0.001\n\n\nHeight\n1.78 (1.73, 1.82)\n1.63 (1.58, 1.67)\n0.000\n\n\n\n1 Median (Q1, Q3); n (%)\n\n\n2 Wilcoxon rank sum test; Fisher’s exact test\n\n\n\n\n\n\n\n\nFor better understanding about the data, data are visualized in Histogram and Boxplot\nHistogram of Weight and height distribution\n\n# Create a histogram for WT (Weight)\nplot1 &lt;- ggplot(df, aes(x = WT)) + \n  geom_histogram(binwidth = 5, fill = \"#EEAEEE\", color = \"black\", alpha = 0.7) +  # Blue fill, black border, transparency\n  labs(title = \"Histogram of Weight (WT)\",\n       x = \"Weight (kg)\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +  # Improves readability\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  ) +\n  geom_density(aes(y = ..count.. * 5), color = \"red\", linetype = \"dashed\", size = 1)  # Add smooth density curve\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n# Create a histogram for HT (Height)\nplot2 &lt;- ggplot(df, aes(x = HT)) + \n  geom_histogram(position = \"identity\", fill = \"#EEB4B4\", color = \"black\", alpha = 0.7) +  # Blue fill, black border, transparency\n  labs(title = \"Histogram of Height (HT)\",\n       x = \"Height (cm)\",\n       y = \"Count\") +\n  theme_minimal()\n#Combine the histogram \nplot1 + plot2\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nBased on the Histogram of Weight, the weight is slighly skewed to the right. On the other hand, the height is skewed to the left. It is indicating that the data is not normally distributed both Weight and Heigth.\nBoxplot of Y variable based on Categorical Variables\nBoxplots were created to better visualization between response variable based on sex, race and dose.\n\n# Boxplot of Y by SEX\nboxplot_sex &lt;- ggplot(df, aes(x = factor(SEX), y = Y, fill = factor(SEX))) +\n  geom_boxplot(alpha = 0.7, color = \"black\") +  # Add transparency and black borders\n  labs(title = \"Boxplot of Y by SEX\",\n       x = \"Sex\",\n       y = \"Y\") +\n  theme_minimal(base_size = 14) +  # Improve readability\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"none\",  # Remove legend since SEX is on x-axis\n    panel.border = element_rect(color = \"black\", fill = NA, size = 1)  # Add frame\n  ) +\n  scale_fill_brewer(palette = \"Pastel1\")  # Use a nice color palette\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n# Boxplot of Y by DOSE\nboxplot_dose &lt;- ggplot(df, aes(x = factor(DOSE), y = Y, fill = factor(DOSE))) +\n  geom_boxplot(alpha = 0.7, color = \"black\") +  # Add transparency and black borders\n  labs(title = \"Boxplot of Y by DOSE\",\n       x = \"Dose\",\n       y = \"Y\") +\n  theme_minimal(base_size = 14) +  # Improve readability\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"none\",  # Remove legend since DOSE is on x-axis\n    panel.border = element_rect(color = \"black\", fill = NA, size = 1)  # Add frame\n  ) +\n  scale_fill_brewer(palette = \"Set2\")  # Use a distinct color palette\n\n# Boxplot of Y by RACE\nboxplot_race &lt;- ggplot(df, aes(x = factor(RACE), y = Y, fill = factor(RACE))) +\n  geom_boxplot(alpha = 0.7, color = \"black\") +  # Transparent boxes with black borders\n  labs(title = \"Boxplot of Y by RACE\",\n       x = \"Race\",\n       y = \"Y\") +\n  theme_minimal(base_size = 14) +  # Improve readability\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    legend.position = \"none\",  # Remove legend since RACE is already labeled on x-axis\n    panel.border = element_rect(color = \"black\", fill = NA, size = 1)  # Add frame\n  ) +\n  scale_fill_brewer(palette = \"Set3\")  # Apply a nice color palette\n\n# Combine the boxplots\n(boxplot_sex + boxplot_dose) / boxplot_race\n\n\n\n\n\n\n\n\nBased on the boxplot above, the mean of response (y) is higher in sex1 compared to sex 2. In the dose category, it can be seen that the higher dose, the higher mean of response variable Y. Moreover, Race 1 and 2 have higher mean of Y compared two other races.\nscatter plot of y based on continouse variables\nScatter plot is better way to show the relationship between continouse variable (both response and predictors). Loess method is used to draw the regression line to clearly look at nonlinear relationship between Y and predictors.\n\n# Scatterplot of Y by AGE\nplot_age &lt;- ggplot(df, aes(x = AGE, y = Y)) +\n  geom_point(alpha = 0.7, size = 2, fill= \"#EE00EE\", color=\"#EE00EE\", stroke=1, shape=18) +  # Transparent points for better visibility\n  geom_smooth(method = \"loess\", se = TRUE, color = \"black\", linetype = \"dashed\", size = 1) +  # Regression line\n  labs(title = \"Y vs Age\",\n       x = \"Age\",\n       y = \"Y\") + \n theme_bw()+\n  theme(axis.title=element_text(size=10, color=\"black\", face=\"bold\"),\n        axis.text=element_text(size=8, color=\"black\"),\n        plot.title=element_text(size=12, color=\"black\", face=\"bold\", hjust= 0.5,))\n\n# Scatterplot of Y by WT (Weight)\nplot_wt &lt;- ggplot(df, aes(x = WT, y = Y)) +\n  geom_point(alpha = 0.7, size = 2, fill= \"#9ACD32\", color=\"#9ACD32\", stroke=1, shape=18) +  # Transparent points for better visibility\n  geom_smooth(method = \"loess\", se = TRUE, color = \"black\", linetype = \"dashed\", size = 1) +  # Regression line\n  labs(title = \"Y vs WT\",\n       x = \"Weight\",\n       y = \"Y\") + \n theme_bw()+\n  theme(axis.title=element_text(size=10, color=\"black\", face=\"bold\"),\n        axis.text=element_text(size=8, color=\"black\"),\n        plot.title=element_text(size=12, color=\"black\", face=\"bold\", hjust= 0.5))\n\n# Scatterplot of Y by HT (Height) with correct color scale\nplot_ht&lt;- ggplot(df, aes(x = HT, y = Y)) +\n  geom_point(alpha = 0.7, size = 2, fill= \"#EE4000\", color=\"#EE4000\", stroke=1, shape=18) +  # Transparent points for better visibility\n  geom_smooth(method = \"loess\", se = TRUE, color = \"black\", linetype = \"dashed\", size = 1) +  # Regression line\n  labs(title = \"Y vs HT\",\n       x = \"Height\",\n       y = \"Y\") + \n theme_bw()+\n  theme(axis.title=element_text(size=10, color=\"black\", face=\"bold\"),\n        axis.text=element_text(size=8, color=\"black\"),\n        plot.title=element_text(size=12, color=\"black\", face=\"bold\", hjust= 0.5))\n \n# Combine and output the three scatterplots\nggarrange(plot_age, plot_wt, plot_ht, ncol=3, nrow=1, align=\"h\", \n          heights=c(1, 1, 1))\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBased on the scatter plots above, it can be seen that there is no linear relationship between Y and the predictors. - For Age, the trend appears somewhat flat with fluctuations, indicating weak or no strong association. The confidence interval (shaded area) is wide, especially at the edges, suggesting greater uncertainty in predictions at extreme ages. There is a large spread of data points, meaning variability in Y is high across different age.\n\nFor Weight, the confidence interval is relatively narrow in the middle but widens at lower and higher WT values, indicating more uncertainty at extreme weights. There is a cluster of data points around a moderate WT range, with more variability at lower and higher weights.\nFor height, the LOESS curve exhibits a U-shaped or fluctuating trend, suggesting a non-linear relationship between Height and Y. Initially, Y decreases with increasing Height, but at certain points, it fluctuates and slightly increases. The confidence interval is wider at extreme heights, suggesting greater uncertainty in predictions. The spread of data is relatively uniform, but there are some extreme Y values.\n\n\n# Scatterplot Y vs Race\nplot_race &lt;- ggplot(df, aes(x = factor(RACE), y = Y, color = factor(RACE))) +\n  geom_jitter(alpha = 0.7, size = 2, width = 0.2) +  # Jitter to avoid overlapping points\n  geom_boxplot(outlier.shape = NA, alpha = 0.3) +  # Boxplot for distribution\n  labs(title = \"Scatterplot of Y by Race\",\n       x = \"Race\",\n       y = \"Outcome Variable (Y)\") +\n  theme_minimal(base_size = 14) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, size = 1)  # Add frame border\n  )\n\n# Scatterplot Y vs SEX\nplot_sex &lt;- ggplot(df, aes(x = factor(SEX), y = Y, color = factor(SEX))) +\n  geom_jitter(alpha = 0.7, size = 2, width = 0.2) +  # Jitter to separate overlapping points\n  geom_boxplot(outlier.shape = NA, alpha = 0.3) +  # Boxplot for visualization\n  labs(title = \"Scatterplot of Y by Sex\",\n       x = \"Sex\",\n       y = \"Outcome Variable (Y)\") +\n  theme_minimal(base_size = 14) +\n  scale_color_brewer(palette = \"Dark2\") +  # Different color scheme\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, size = 1)  # Add frame border\n  )\n\n# Scatterplot Y vs Doses\nplot_dose &lt;- ggplot(df, aes(x = factor(DOSE), y = Y, color = factor(DOSE))) +\n  geom_jitter(alpha = 0.7, size = 2, width = 0.2) +  # Jitter for better visualization\n  geom_boxplot(outlier.shape = NA, alpha = 0.3) +  # Boxplot for distribution\n  labs(title = \"Scatterplot of Y by Dose\",\n       x = \"Dose\",\n       y = \"Outcome Variable (Y)\") +\n  theme_minimal(base_size = 14) +\n  scale_color_brewer(palette = \"Set3\") +  # Different color scheme\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, size = 1)  # Add frame border\n  )\n\n# Combine and output the three scatterplots\nggarrange(\n  ggarrange(plot_race, plot_sex, ncol = 2, nrow = 1),  # Row 1: Two plots side by side\n  plot_dose,  # Row 2: Full-width plot\n  ncol = 1, nrow = 2,  # 2 rows total\n  heights = c(2, 2)  # Equal height for both rows\n)\n\n\n\n\n\n\n\n\nBased on the scatter plot above, race 1 has a wider spread of Y values with more extreme values (outliers). The spread of Y for Sex 1 is slightly larger than for Sex 2. Sex 2 appears to have slightly higher median Y values. The interquartile range (IQR) suggests that the distribution of Y values differs between sexes.The median Y value increases with increasing dose. The spread of Y also increases as the dose increases. The dose group 50 shows the widest variability, with more extreme values.\n\n# Select only continuous variables (excluding categorical variables like SEX, RACE, DOSE)\ndf_cont &lt;- df %&gt;% select(where(is.numeric))\n\n# Compute correlation matrix\ncor_matrix &lt;- cor(df_cont, use = \"complete.obs\")  # Use only complete cases\n\n# Visualize correlation matrix with correlation values\ncorrplot(cor_matrix, \n         method = \"color\",  # Color-coded visualization\n         type = \"lower\",  # Show only lower triangle to reduce redundancy\n         tl.col = \"black\",  # Black text labels for variable names\n         tl.srt = 45,  # Rotate labels for better readability\n         addCoef.col = \"black\",  # Show correlation values in black\n         number.cex = 0.8)  # Adjust text size for correlation numbers\n\n\n\n\n\n\n\n\nBased on the correlation matrix, DOSE is the strongest predictor of Y, showing a positive correlation (0.72). AGE does not seem to have a meaningful relationship with any variable. Weight (WT) and Height (HT) are moderately correlated (0.60), which makes sense biologically. There are weak negative correlations of Y with WT and HT, but their impact is likely small."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-fitting",
    "href": "fitting-exercise/fitting-exercise.html#model-fitting",
    "title": "Fitting Models Exercise",
    "section": "Model Fitting",
    "text": "Model Fitting\nModel 1: Y ~ Dose For model fitting, I will start with simple model (y~ DOSE). Tidymodels is used in this modeling.\n\ndf$DOSE &lt;- as.factor(df$DOSE)\n\n# Linear regression: Y ~ DOSE\nm1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(Y ~ DOSE, df)\n\n# Output the fitting result\ntidy(m1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1783.      87.9     20.3  3.97e-40\n2 DOSE37.5        681.     214.       3.19 1.84e- 3\n3 DOSE50         1456.     130.      11.2  3.79e-20\n\n\nInterpretation:\n\nIntercept (Baseline: DOSE 25): the estimated mean Y when using DOSE 25 is 1782.67\nDOSE 37.5 (681.24): increasing DOSE from 25 to 37.5 leads to an increase in Y by 681.24 on average with p-value = 0.0018 (&lt; 0.05) → This effect is statistically significant.\nDOSE 50 (1456.20): increasing DOSE from 25 to 50 leads to an increase in Y by 1456.20 on average. p-value &lt; 0.0001 → Strong evidence that this effect is statistically significant.\n\nModel2: Y ~ all predictors\n\nm2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(Y ~ ., df)\n\n# Output the fitting result\ntidy(m2)\n\n# A tibble: 10 × 5\n   term        estimate std.error statistic  p.value\n   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  4891.     1823.       2.68  8.42e- 3\n 2 DOSE37.5      664.      200.       3.31  1.26e- 3\n 3 DOSE50       1499.      122.      12.2   2.92e-22\n 4 AGE             3.52      7.90     0.446 6.57e- 1\n 5 SEX2         -360.      218.      -1.65  1.01e- 1\n 6 RACE2         149.      130.       1.15  2.54e- 1\n 7 RACE7        -421.      451.      -0.933 3.53e- 1\n 8 RACE88        -65.3     247.      -0.264 7.92e- 1\n 9 WT            -23.3       6.44    -3.62  4.54e- 4\n10 HT           -741.     1108.      -0.669 5.05e- 1\n\n\nInterpretation: - The intercept (4890.92) represents the expected Y value when all predictors are zero. - Dose 37.5, Dose 50, and Weight are statistically significant with Y. Doses have positive relationship with Y, while Weight had significant effect on Y. Other predictors are not statistically correlated with Y.\ncompute RMSE and R-squared\n\nm1_RMSE_R2&lt;- predict(m1, df) %&gt;%\n  bind_cols(df) %&gt;%\n  metrics(truth=Y, estimate=.pred) %&gt;%\n  print()\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n3 mae     standard     517.   \n\nm2_RMSE_R2&lt;- predict(m2, df) %&gt;%\n  bind_cols(df) %&gt;%\n  metrics(truth=Y, estimate=.pred) %&gt;%\n  print()\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     590.   \n2 rsq     standard       0.620\n3 mae     standard     445.   \n\n\nBased on the RMSE and R-Squared, model 2 (all predictors) performed better fit to the data compared to Model 1 (Dose Only) with RMSE and R-Squared (666.31 and 0.51 respectively for model 1, and 590.31 and 0.62 for model 2).\nModel 3: Logistic Regression (Sex ~ Dose)\n\n#SEX ~ DOSE\nm3 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(SEX ~ DOSE, df)\ntidy(m3)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)  -1.59       0.347   -4.58   0.00000465\n2 DOSE37.5     -0.0202     0.849   -0.0238 0.981     \n3 DOSE50       -0.831      0.627   -1.33   0.185     \n\n\nInterpretation:\nThe Intercept is significant, meaning there is an underlying distribution of SEX probabilities. Neither DOSE37.5 nor DOSE50 significantly affect SEX because their p-values &gt; 0.05. This suggests that DOSE does not strongly predict SEX.\nModel 4: Logistic Regression (Sex ~ All Predictors)\n\n#SEX ~ all predictors\nm4 &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  fit(SEX ~ DOSE + AGE + RACE + WT + HT, df)\n\n# Output the fitting result\ntidy(m4)\n\n# A tibble: 9 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  55.7      16.8      3.31    0.000937\n2 DOSE37.5     -3.11      1.84    -1.69    0.0916  \n3 DOSE50       -2.47      1.33    -1.86    0.0633  \n4 AGE           0.114     0.0696   1.64    0.101   \n5 RACE2        -2.40      1.39    -1.73    0.0843  \n6 RACE7         0.0168    3.56     0.00473 0.996   \n7 RACE88       -1.74      2.34    -0.743   0.457   \n8 WT           -0.0463    0.0716  -0.646   0.518   \n9 HT          -33.3      10.7     -3.10    0.00191 \n\n\nBased on the output, only Height is significantly correlated with Sex. Dose levels have a small, negative, but non-significant effect on Sex. Age, race and WT are not significant with Sex.\nCalculate RMSE and R-squared\n\nm3_RMSE_R2 &lt;- predict(m3, df, type=\"class\") %&gt;%\n  bind_cols(predict(m3, df, type=\"prob\")) %&gt;%\n  bind_cols(df) %&gt;%\n  metrics(truth=SEX, estimate=.pred_class, .pred_1) %&gt;%\n  print()\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.867\n2 kap         binary         0    \n3 mn_log_loss binary         0.384\n4 roc_auc     binary         0.592\n\nm4_RMSE_R2 &lt;- predict(m4, df, type=\"class\") %&gt;%\n  bind_cols(predict(m4, df, type=\"prob\")) %&gt;%\n  bind_cols(df) %&gt;%\n  metrics(truth=SEX, estimate=.pred_class, .pred_1) %&gt;%\n  print()\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.95 \n2 kap         binary         0.772\n3 mn_log_loss binary         0.133\n4 roc_auc     binary         0.978\n\n\n\n# Create a data frame for both models\nmetrics_df &lt;- data.frame(\n  metric = rep(c(\"accuracy\", \"kap\", \"mn_log_loss\", \"roc_auc\"), 2),  # Repeating metrics\n  estimate = c(0.8667, 0.0000, 0.3843, 0.5919,   # First model\n               0.9500, 0.7716, 0.1334, 0.9784),  # Second model\n  model = rep(c(\"Model 1\", \"Model 2\"), each = 4)  # Model labels\n)\n\n# Create a grouped bar plot\nggplot(metrics_df, aes(x = metric, y = estimate, fill = model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\", alpha = 0.8) +  # Grouped bars\n  labs(title = \"Comparison of Model Performance Metrics\",\n       x = \"Metric\",\n       y = \"Score\",\n       fill = \"Model\") +\n  theme_minimal(base_size = 14) +  # Clean theme\n  scale_fill_manual(values = c(\"Model 1\" = \"lightpink\", \"Model 2\" = \"lightblue\")) +  # Custom colors\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\nInterpretation: - Model 2 (accuracy 0.95) is more accurate in making correct predictions than Model 1 (accuracy 0.87). - Kappa measures how well the model’s predictions agree with the true labels, adjusting for chance. Model 2 shows a much stronger agreement with the actual data, while Model 1 shows almost no agreement beyond chance. - Lower log loss indicates better probabilistic predictions. Model 2 has a significantly lower log loss, meaning its probability estimates are more reliable. - A higher ROC AUC means the model is better at distinguishing between classes. Model 2 is much better than Model 1 at identifying positive vs. negative classes."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "Install all packages needed",
    "section": "",
    "text": "y— title: “R Coding Exercise”\nPlaceholder file for the future R coding exercise."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#natalies-addition-to-muhammads-exercise-3",
    "href": "coding-exercise/coding-exercise.html#natalies-addition-to-muhammads-exercise-3",
    "title": "Install all packages needed",
    "section": "Natalie’s Addition to Muhammad’s Exercise 3",
    "text": "Natalie’s Addition to Muhammad’s Exercise 3\nThis section is contributed to by Natalie Cann.\nI will first load packages needed for this exercise.\n\nlibrary(dslabs)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\n\nI will use the “help()” function on the “us_contagious_diseases” data from dslabs.\n\nhelp(us_contagious_diseases)\n\nThe help file told me that this data frame contains yearly counts for Hepatitis A, Measles, Mumps, Pertussis, Polio, Rubella, and Smallpox in the United States.\nNow, I will use str() on the “us_contagious_diseases” data frame.\n\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n\nThis informs me that the data set contains 6 variables (disease, state, year, weeks_reporting, count, and population) and 16,065 observations. Disease and state are factors, while the other four variables are numeric.\nNext, I will use the summary() on the “us_contagious_diseases” data frame.\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n\nThe summary above shows the diseases and states. It also shows the minimum, 1st quartile, median, mean, 3rd quartile and maximum values for the numeric variables (year, weeks_reporting, count, and population).\nFirst, I will create a data frame containing only data for the state of Georgia.\n\nGA_contagious_diseases &lt;- filter(us_contagious_diseases, state == \"Georgia\") # I am using filter to obtain only data from GA\nView(GA_contagious_diseases) # I am viewing the data frame to ensure it only contains data from GA\n\nThis worked, as only GA data is shown in the new data frame.\nNow, I will run str() and summary() on the GA_contagious_diseases data frame.\n\nstr(GA_contagious_diseases)\n\n'data.frame':   315 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 11 11 11 11 11 11 11 11 11 11 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  47 51 50 48 51 50 47 50 48 48 ...\n $ count          : num  509 922 990 750 763 ...\n $ population     : num  4306523 4373252 4442463 4514462 4589575 ...\n\nsummary(GA_contagious_diseases)\n\n        disease          state          year      weeks_reporting\n Hepatitis A:46   Georgia   :315   Min.   :1928   Min.   : 0.00  \n Measles    :75   Alabama   :  0   1st Qu.:1950   1st Qu.:33.00  \n Mumps      :35   Alaska    :  0   Median :1975   Median :45.00  \n Pertussis  :56   Arizona   :  0   Mean   :1971   Mean   :37.66  \n Polio      :41   Arkansas  :  0   3rd Qu.:1990   3rd Qu.:49.00  \n Rubella    :37   California:  0   Max.   :2011   Max.   :52.00  \n Smallpox   :25   (Other)   :  0                                 \n     count           population     \n Min.   :    0.0   Min.   :2901933  \n 1st Qu.:    8.5   1st Qu.:3444578  \n Median :   42.0   Median :5009127  \n Mean   :  643.0   Mean   :5235135  \n 3rd Qu.:  352.0   3rd Qu.:6478216  \n Max.   :22965.0   Max.   :9830160  \n                                    \n\n\nThis shows me that all the variables are the same as before, but they only contain data from GA.\nNow, I will create a data frame that contains only data from GA in 1950. I will use this data frame to create a plot of the number of cases of each disease in GA in 1950.\n\nGA_contagious_diseases_1950 &lt;- filter(GA_contagious_diseases, year == 1950) # I am using filter to obtain only data from GA in 1950\n\n# I will run str() and summar() on the GA_contagious_diseases_1950 data frame to get a better look at the data\nstr(GA_contagious_diseases_1950) \n\n'data.frame':   4 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 2 4 5 7\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 11 11 11 11\n $ year           : num  1950 1950 1950 1950\n $ weeks_reporting: num  48 50 38 0\n $ count          : num  2159 1041 492 0\n $ population     : num  3444578 3444578 3444578 3444578\n\nsummary(GA_contagious_diseases_1950)\n\n        disease         state        year      weeks_reporting     count       \n Hepatitis A:0   Georgia   :4   Min.   :1950   Min.   : 0.0    Min.   :   0.0  \n Measles    :1   Alabama   :0   1st Qu.:1950   1st Qu.:28.5    1st Qu.: 369.0  \n Mumps      :0   Alaska    :0   Median :1950   Median :43.0    Median : 766.5  \n Pertussis  :1   Arizona   :0   Mean   :1950   Mean   :34.0    Mean   : 923.0  \n Polio      :1   Arkansas  :0   3rd Qu.:1950   3rd Qu.:48.5    3rd Qu.:1320.5  \n Rubella    :0   California:0   Max.   :1950   Max.   :50.0    Max.   :2159.0  \n Smallpox   :1   (Other)   :0                                                  \n   population     \n Min.   :3444578  \n 1st Qu.:3444578  \n Median :3444578  \n Mean   :3444578  \n 3rd Qu.:3444578  \n Max.   :3444578  \n                  \n\n\nYou can see that now, the only year appearing in the data set is 1950. Therefore, all the variables now reflect only data from 1950 in GA.\nNext, I will rename the “count” variable to “number_of_cases”.\n\nGA_contagious_diseases_1950 &lt;- rename(GA_contagious_diseases_1950, number_of_cases = count) # I am renaming the \"count\" variable to \"number_of_cases\" via the rename() function (and then assigning it back to GA_contagious_diseases_1950)\n\ncolnames(GA_contagious_diseases_1950) # I am checking to see if the previous step was done properly\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"number_of_cases\" \"population\"     \n\n\nI see this was done correctly, as the variable that used to be called “count” is now called “number_of_cases”.\nNow, I will create a bar graph to display the number of cases of each contagious disease reported in GA in 1950.\n\ncustom_colors_1950 &lt;- c(\"Measles\" = \"#6fe51e\", \"Pertussis\" = \"#2ce1b0\", \"Polio\" = \"#2cb2e1\", \"Smallpox\" = \"#3a83e6\") # I am creating a vector of colors that I will use to fill each disease's bar on the graph below\n\nggplot(GA_contagious_diseases_1950, aes (x = disease, y = number_of_cases, fill = disease)) + # Using ggplot on the GA_contagious_diseases_1950 data frame and setting x and y equal to diseases and number of cases (respectively) and setting fill equal to disease\n  geom_bar(stat = \"identity\") + # Specifying the geom as geom_bar() to create a bar graph \n  labs(title = \"Reported Number of Cases of Contagious Diseases \\n in State of Georgia in 1950\", x = \"Disease\", y = \"Number of Cases\") + # Renaming title and axes\n  scale_fill_manual(values = custom_colors_1950) + # Setting the fill colors of the bars to the custom colors I created above \n  geom_text(aes(label = number_of_cases), vjust = -0.5) + # Adding text labels to the top of each bar to show the number of cases\n  theme(legend.position = \"bottom\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) + # Putting legend at the bottom; increasing size, boldness, and center of title/axes\n  scale_y_continuous(limits = c(0, 2500), breaks = seq(0, 2500, by = 500)) # Setting the y-axis limits and breaks to better see the text of number of cases at the top\n\n\n\n\n\n\n\n\nYou can see from the bar graph above that the number of Measles cases in GA in 1950 was the highest out of all the diseases shown here (with the number of cases being 2159).\nNow, I will create a data frame that contains Measles data from all states in 1950.\n\nMeasles_1950 &lt;- filter(us_contagious_diseases, disease == \"Measles\", year == 1950) # I am using filter to obtain only Measles data from all states in 1950\n\n# I will run str() and summary() on the Measles_1950 data frame to get a better look at the data\nstr(Measles_1950) \n\n'data.frame':   51 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year           : num  1950 1950 1950 1950 1950 1950 1950 1950 1950 1950 ...\n $ weeks_reporting: num  47 0 49 50 50 50 51 44 43 49 ...\n $ count          : num  1556 0 2389 1739 14728 ...\n $ population     : num  3061743 NA 749587 1909511 10586224 ...\n\nsummary(Measles_1950)\n\n        disease          state         year      weeks_reporting\n Hepatitis A: 0   Alabama   : 1   Min.   :1950   Min.   : 0.0   \n Measles    :51   Alaska    : 1   1st Qu.:1950   1st Qu.:46.0   \n Mumps      : 0   Arizona   : 1   Median :1950   Median :49.0   \n Pertussis  : 0   Arkansas  : 1   Mean   :1950   Mean   :45.9   \n Polio      : 0   California: 1   3rd Qu.:1950   3rd Qu.:50.0   \n Rubella    : 0   Colorado  : 1   Max.   :1950   Max.   :51.0   \n Smallpox   : 0   (Other)   :45                                 \n     count           population      \n Min.   :    0.0   Min.   :  160083  \n 1st Qu.:  853.5   1st Qu.:  791896  \n Median : 2301.0   Median : 2233351  \n Mean   : 5909.4   Mean   : 3075456  \n 3rd Qu.: 6000.0   3rd Qu.: 3444578  \n Max.   :36859.0   Max.   :14830192  \n                   NA's   :2         \n\n\nThis shows data only from the Measles disease; it includes data from every state that reported measles data in 1950.\nOnce again, I will rename the “count” variable to “number_of_cases”.\n\nMeasles_1950 &lt;- rename(Measles_1950, number_of_cases = count) # I am renaming the \"count\" variable to \"number_of_cases\" via the rename() function \n\ncolnames(Measles_1950) # Making sure this was done correctly\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"number_of_cases\" \"population\"     \n\n\nAs seen by the output, I can see that this variable is now called “number_of_cases”.\nNow, I will create a bar graph to display the number of Measles cases reported in each state in 1950.\n\ncustom_colors_Measles_1950 &lt;- c(\"#53d127\", \"#71f45d\", \"#2ce1b0\", \"#40dcd9\", \"#2cb2e1\", \"#3a83e6\")\n\nggplot(Measles_1950, aes(x = state, y = number_of_cases, fill = number_of_cases)) + \n  geom_bar(stat = \"identity\") +  # Correct stat for using y data directly\n  labs(title = \"Reported Number of Measles Cases \\n in the United States in 1950\", \n       x = \"State\", \n       y = \"Number of Cases\") + # Rename title and axes\n  geom_text(aes(label = number_of_cases), vjust = -0.5, size = 1.5) +  # Display labels above bars\n  theme(legend.position = \"none\",  # Remove legend \n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\"), # Increase size and boldness of title and axes\n        axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate state names\n  scale_fill_gradientn(colors = custom_colors_Measles_1950) # Set custom color scheme made\n\n\n\n\n\n\n\n\nThis bar graph shows the number of Measles cases reported in each state in 1950. As you can see, Michigan had the highest number of reported cases of Measles; on the other hand, Alaska had the lowest number of reported cases of Measles.\nNow, I will create a new variable within the Measles_1950 data frame that assess number of cases in each region of the country.\n\nMeasles_1950$region &lt;- ifelse(Measles_1950$state %in% c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\", \"New York\", \"New Jersey\", \"Pennsylvania\", \"Delaware\", \"Maryland\", \"District Of Columbia\"), \"Northeast\", # contains states from the Northeast\n                        ifelse(Measles_1950$state %in% c(\"Ohio\", \"Michigan\", \"Indiana\", \"Illinois\", \"Wisconsin\", \"Minnesota\", \"Iowa\", \"Missouri\", \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\"),  \"Midwest\", # contains states from the Midwest\n                        ifelse(Measles_1950$state %in% c(\"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\", \"Kentucky\", \"Tennessee\", \"Alabama\", \"Mississippi\", \"Arkansas\", \"Louisiana\"), \"South\", # contains states from the South\n                        ifelse(Measles_1950$state %in% c(\"Texas\", \"Oklahoma\", \"New Mexico\", \"Arizona\"), \"Southwest\", # contains states from the Southwest\n                        ifelse(Measles_1950$state %in% c(\"Alaska\", \"Hawaii\"), \"Non-Contiguous\", # contains states not connected to the main US\n                        \"West\"))))) # any other state will be categorized as the West\n\nhead(Measles_1950) # I am checking to ensure this was done properly\n\n  disease      state year weeks_reporting number_of_cases population\n1 Measles    Alabama 1950              47            1556    3061743\n2 Measles     Alaska 1950               0               0         NA\n3 Measles    Arizona 1950              49            2389     749587\n4 Measles   Arkansas 1950              50            1739    1909511\n5 Measles California 1950              50           14728   10586224\n6 Measles   Colorado 1950              50            5239    1325089\n          region\n1          South\n2 Non-Contiguous\n3      Southwest\n4          South\n5           West\n6           West\n\n\nI will create a version of this data frame that contains the number of cases of Measles in each region of the US.\n\nregion_Measles_cases_1950 &lt;- Measles_1950 %&gt;% # Creating new data frame that groups data by region and then includes the total number of cases in each region\n  group_by(region) %&gt;%\n  summarize(total_cases = sum(number_of_cases))\n\nNow, we can create a bar graph to display the number of Measles cases reported in each region in 1950.\n\nggplot(region_Measles_cases_1950, aes(x = region, y = total_cases, fill = total_cases)) +  # Using ggplot on the region_Measles_cases_1950 data frame and setting x and y equal to region and total_cases (respectively) and setting fill equal to region\n  geom_bar(stat = \"identity\") +  # Use 'identity' since we are providing y values directly\n  labs(title = \"Reported Measles Cases by Region in 1950\", \n       x = \"Region\", \n       y = \"Number of Cases\") + # Rename title and axes\n  geom_text(aes(label = total_cases), vjust = -0.5, size = 3) +  # Add labels above bars\n  theme(legend.position = \"right\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\"), # Increase size and boldness of title and axes\n        axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotate state names\n  scale_y_continuous(limits = c(0, 130000), breaks = seq(0, 130000, by = 20000)) + # Setting the y-axis limits and breaks to better see the text of number of cases at the top\n  scale_fill_gradientn(colors = custom_colors_Measles_1950) # Set custom color scheme made\n\n\n\n\n\n\n\n\nAs you can see, the Midwest had the highest number of reported Measles cases in 1950 (115836). The Non-Contiguous region had the lowest number of reported Measles cases in 1950 (0) - it is important to note that Alaska and Hawaii are the only states in this region and that Alaska did not report any cases.\nI will create a Scatterplot of the number of cases of Measles in each state in 1950.\n\nMeasles_1950$state &lt;- factor(Measles_1950$state, \n                             levels = unique(Measles_1950$state[order(Measles_1950$region)]))\n# I am reordering the states based on the region they are in (instead of alphabetical order) via levels(), unique(), and order()\n\nggplot(Measles_1950, aes(x = state, y = number_of_cases, color = region)) + \n  geom_point(size = 3) +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"Reported Measles Cases in Each State in 1950\", \n       x = \"State\", \n       y = \"Number of Cases\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) +  # Increase size and boldness of title and axes\n  scale_color_manual(values = c(\"Northeast\" = \"#53d127\", \"Midwest\" = \"#71f45d\", \"South\" = \"#2ce1b0\", \n                               \"Southwest\" = \"#47e2e7\", \"Non-Contiguous\" = \"#2cb2e1\", \"West\" = \"#3a83e6\"))  # Assign specific colors to each region\n\n\n\n\n\n\n\n\nAs you can see from the scatterplot above, the Midwest and the Northeast have the greatest amount of variation in the number of reported Measles cases in 1950.\nI will now use the lm() function to fit a linear model with number_of_cases as the outcome and region of the United States as the predictor. Then, I will apply the summary() function to view the results.\n\nfit_1950 &lt;- lm(number_of_cases ~ region, data = Measles_1950) # Running a linear fit via lm()\nsummary(fit_1950) # Viewing results of linear model via summary()\n\n\nCall:\nlm(formula = number_of_cases ~ region, data = Measles_1950)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -9259  -5078  -1130   1937  27206 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              9653       2353   4.103 0.000169 ***\nregionNon-Contiguous    -9620       6224  -1.546 0.129227    \nregionNortheast         -1420       3327  -0.427 0.671600    \nregionSouth             -6784       3327  -2.039 0.047339 *  \nregionSouthwest         -5254       4705  -1.117 0.270105    \nregionWest              -5802       3594  -1.615 0.113405    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8150 on 45 degrees of freedom\nMultiple R-squared:  0.1334,    Adjusted R-squared:  0.03716 \nF-statistic: 1.386 on 5 and 45 DF,  p-value: 0.2476\n\n\nThe P-value of this model is greater than 0.05, which means we must fail to reject the null hypothesis (that there is no relationship between the number of Measles cases and the region of the US). This means that the region of the US does not have a significant impact on the number of Measles cases reported in 1950.\nI will run an ANOVA (Analysis of Variance) test to asses if there are significant differences in the means of the number of Measles cases reported in each region in the US in 1950.\n\nanova_result &lt;- aov(number_of_cases ~ region, data = Measles_1950) # Running ANOVA test with aov() function\nsummary(anova_result) # Viewing a summary of results\n\n            Df    Sum Sq  Mean Sq F value Pr(&gt;F)\nregion       5 4.602e+08 92049896   1.386  0.248\nResiduals   45 2.989e+09 66418472               \n\n\nThe F-value of the ANOVA table is 1.386, and the P-value is 0.248. Since 0.248 &gt; 0.05, we must fail to reject the null hypothesis, which is stated previously. Therefore, this result is consistent with the linear fit model results above; region of the US does not appear to have a significant impact on the number of Measles cases in 1950.\nI will now put the summary into a table.\n\nanova_df &lt;- as.data.frame(summary(anova_result)[[1]]) # Putting the summary of the ANOVA test into a data frame \nprint(anova_df) # To Printing the  data frame\n\n            Df     Sum Sq  Mean Sq  F value    Pr(&gt;F)\nregion       5  460249478 92049896 1.385908 0.2476114\nResiduals   45 2988831236 66418472       NA        NA\n\n\nHere is the printed ANOVA results."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "My Name is Muhammad Nasir, Call me “NASIR”, it’s originally from Arabic Language “Naseer” meaning “The Helper”. I love that name. However, many people hard to pronounce it, so many people here (or when I was in the UK) call me Muhammad, which I is fine for me. I am from Indonesia. I am a little boy from small village Called “Kindang”, a remote area in the foot of Bawakaraeng Mountain in South Sulawesi Indonesia. However, I left the village since I was 12 YO, moving to a city where I did my middle school, and keep moving from one city to other cities for each level of my education. I am the luckiest guy in the village (or in the world) because I can skip the chain of poverty and the darkness of lack of education for myself (and my family now), I am the first generation going to school in entire my big Family. So I am so PROUD to be here and lived in several different countries (Education is the gate of the world, and education changes life).\nI love “NATURE”, specially hidden gem. and I am lucky to be born and grown up in Indonesia, where billions of hidden gem that we can visit. One of the hidden “HEAVEN” that I can show you in the picture below, called ” PAISU POK LAKE” in Central Sulawesi, it takes eight hours by boat to enjoy this beautiful view. You can see “ancient trees buried in the lake”.\n\n\nPictures: PAISU POK Lake in Banggai Kepulauan Island in Indonesia.\nFun facts: After I got 12 YO, I never live in a city more than 4 years.\nHobby: Playing badminton, Hiking and Hide myself and sleep in hidden gems for days.\n\nEducational Background\nI obtained my Bachelor of Public health with Environmental Health Concentration from Universitas Muslim Indonesia in Makassar. I did my Master degree at The University of Birmingham, UK on Public and Environmental Science. This course was prepared for being a practitioner in environmental health. I did pre doctoral program at the University of Illinois at Urbana-Champaign before starting my PhD at the University of Georgia.\n\n\nWorking Experience\nI worked in several different topics/ fields. Before I started my master, I worked as an intern at Ministry of Environment Republic Indonesia on a project called ” Culture and biodiversity in Raja Ampat ” (I highly recommend you to GOOGLE “RAJA AMPAT INDONESIA”, you would see a heaven that you must put in your travel lists). After my Master, I worked on a Project “WASH Post Disaster in Central Sulawesi” funded by WHO. Then in 2019, I worked with Universitas Indonesia and UNICEF on Adolescence Mental Health Post Disaster in Central Sulawesi. In 2020, I worked at Kerti Praja Foundation on a project called ” Implementation of Integrated Gender-Based Violence (GBV) and Sexual Reproductive Health (SRH) Service in Primary Care in four different provinces in Indonesia, funded by UNFPA Indonesia and Government of Japan. Right before I came to the USA, I work at WHO Indonesia as a national consultant for Malaria Elimination in Central Sulawesi (only three months and have to resign to pursue higher degree). Although I worked in many different topics, the nature of the jobs that I did was similar, most of the things that I did was about community empowerment, program development, and policy advocacy to local governments. None of my previous work related to Epidemiology or Biostatistics, but I am so lucky to be in this department at the moment.\n\nPicture: Delivering a training for community leader about GBV survivor outreach in community level (my little guard, SEAN, who always accompanies me at work. Luckily, most work places in my place are child friendly).\n\n\nData analysis experience\nHonestly, I am very new in quantitative data analysis, let say I started it at UGA. Most of my work deal with qualitative data or for quantitative data, they are prepared by data analysts. Therefore, my data analysis experience only relate to class assignment or projects. I started Using R last year, so please help me grow and I am happy if people can involve me in their project for data analysis (for Free :) ), I just need to learn and get more experience.\n\n\nResearch Interest\nI am currently interested in Climate Change and its implication to public health. I am also interested in communicable diseases such as Tuberculosis, Soil-transmitted Helminths, water born diseases, and vector born diseases."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "About the dataset\nThe Botswana Combination Prevention Project (BCPP) was a collaborative research effort led by the Botswana Ministry of Health (MOH), the Harvard School of Public Health/Botswana Harvard AIDS Institute Partnership (BHP), and the U.S. Centers for Disease Control and Prevention (CDC). This community-based randomized trial aimed to assess the impact of various HIV prevention strategies on reducing HIV incidence across 15 intervention and 15 control communities. The intervention communities received comprehensive HIV testing, linkage to care, and universal treatment services, guided by the UNAIDS 90-90-90 targets: ensuring 90% of individuals with HIV are aware of their status, 90% of those diagnosed are on antiretroviral therapy (ART), and 90% of those on ART achieve viral suppression.\nThe BCPP study was structured around two interrelated protocols: the Evaluation Protocol and the Intervention Protocol. The Evaluation Protocol assessed the primary outcome—HIV incidence—along with key secondary outcomes, focusing on data collected from the Baseline Household Survey, the HIV Incidence Cohort, and the End of Study Survey. Meanwhile, the Intervention Protocol involved the implementation of a combination prevention (CP) package in combination prevention communities (CPCs), monitoring the uptake of interventions such as expanded HIV testing and counseling, enhanced male circumcision services, and improved access to HIV care and treatment.\nThe dataset is available and free to download at CDC Website:https://data.cdc.gov/Global-Health/Botswana-Combination-Prevention-Project-BCPP-Publi/qcw5-4m9q/about_data.\nThe study is a cohort study with 3 phases. In this exercise, I only work on year 3 dataset. There are many information covered in this dataset, including demographic information of the respondent, soccioeconomic factors, HIV exposure, HIV status and conditions.\n\n\nInstall all packages needed\nInstall and library all packages needed in this section.\n\ninstall.packages(\"tidyverse\")\n\nThe following package(s) will be installed:\n- tidyverse [2.0.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing tidyverse ...                      OK [linked from cache]\nSuccessfully installed 1 package in 47 milliseconds.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ninstall.packages(\"ggplot2\")\n\nThe following package(s) will be installed:\n- ggplot2 [3.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing ggplot2 ...                        OK [linked from cache]\nSuccessfully installed 1 package in 44 milliseconds.\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\ninstall.packages(\"patchwork\")  # This package is to redefine \"/\" operator for plot arrangement\n\nThe following package(s) will be installed:\n- patchwork [1.3.0]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing patchwork ...                      OK [linked from cache]\nSuccessfully installed 1 package in 42 milliseconds.\n\nlibrary(patchwork)\ninstall.packages(\"writexl\")\n\nThe following package(s) will be installed:\n- writexl [1.5.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing writexl ...                        OK [linked from cache]\nSuccessfully installed 1 package in 39 milliseconds.\n\nlibrary(writexl)\nlibrary(haven)\nlibrary(dplyr)\ninstall.packages(\"janitor\")\n\nThe following package(s) will be installed:\n- janitor [2.2.1]\nThese packages will be installed into \"C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025/renv/library/windows/R-4.4/x86_64-w64-mingw32\".\n\n# Installing packages --------------------------------------------------------\n- Installing janitor ...                        OK [linked from cache]\nSuccessfully installed 1 package in 41 milliseconds.\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(procs)\n\nLoading required package: common\n\n\n\n\nLoading the dataset\n\n## set path  dictionary \nbostwana &lt;- here::here(\"cdcdata-exercise\",\"data\", \"cdcbostwana.csv\") # set the pathway to create relative path \n\nbostwana &lt;- read_csv(bostwana) # read the dataset stored in specified path\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 11369 Columns: 322\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (217): de_subj_idC, de_hh_idC, de_plot_idC, region, random_arm, survey, ...\ndbl  (64): community_rndmN, pair_rndmN, interview_days, yeardone, age_at_int...\nlgl  (41): religion_name, religious_affil, relig_theogrp, ethnicity, prev_hi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(bostwana)\n\n# A tibble: 6 × 322\n  de_subj_idC de_hh_idC de_plot_idC region community_rndmN pair_rndmN random_arm\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     \n1 10011A      16863A    14700A      North…              25         13 Intervent…\n2 1001A       8552A     17400A      South…               4          1 Intervent…\n3 10021A      13235A    4578A       Centr…               5          8 Standard …\n4 10023A      7028A     7820A       Centr…               9          4 Standard …\n5 10026A      5500A     4704A       South…              19          3 Intervent…\n6 10032A      2667A     8896A       South…              14          2 Standard …\n# ℹ 315 more variables: interview_days &lt;dbl&gt;, yeardone &lt;dbl&gt;, survey &lt;chr&gt;,\n#   gender &lt;chr&gt;, age_at_interview &lt;dbl&gt;, age5cat &lt;chr&gt;,\n#   length_residence &lt;chr&gt;, permanent_resident &lt;chr&gt;, intend_residency &lt;chr&gt;,\n#   nights_away &lt;chr&gt;, cattle_postlands &lt;chr&gt;, religion_name &lt;lgl&gt;,\n#   religious_affil &lt;lgl&gt;, relig_theogrp &lt;lgl&gt;, ethnicity &lt;lgl&gt;,\n#   marital_status &lt;chr&gt;, num_wives &lt;dbl&gt;, husband_wives &lt;dbl&gt;,\n#   live_alone &lt;chr&gt;, livewith_family &lt;chr&gt;, livewith_partner &lt;chr&gt;, …\n\ndim(bostwana)\n\n[1] 11369   322\n\n\nThe dataset contains 11369 observations and 322 columns.\n\n\nData Cleaning\nI want to select interesting variables for further analysis and clean the data as needed including dropping/ lablig missing values.\n\nbost_df &lt;- bostwana %&gt;% \n  select(\"region\", \"random_arm\", \"gender\", \"age_at_interview\", \"age5cat\", \"employment_status\", \"circumcised\", \"hiv_status_current\", \"hiv_status_time\", \"viral_load_yr3\",  \"cd4_survey_yr3\", \"cd4_surveydays_yr3\", \"arv_duration_days\")\n\nhead(bost_df)\n\n# A tibble: 6 × 13\n  region   random_arm       gender age_at_interview age5cat    employment_status\n  &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            \n1 Northern Intervention     M                  45.7 45-54 yea… Unemployed not l…\n2 Southern Intervention     F                  51.8 45-54 yea… Unemployed looki…\n3 Central  Standard of Care F                  40.9 35-44 yea… Unemployed looki…\n4 Central  Standard of Care F                  18.7 16-24 yea… Unemployed not l…\n5 Southern Intervention     F                  35.8 35-44 yea… Employed         \n6 Southern Standard of Care F                  56.9 55-64 yea… Unemployed looki…\n# ℹ 7 more variables: circumcised &lt;chr&gt;, hiv_status_current &lt;chr&gt;,\n#   hiv_status_time &lt;chr&gt;, viral_load_yr3 &lt;dbl&gt;, cd4_survey_yr3 &lt;dbl&gt;,\n#   cd4_surveydays_yr3 &lt;dbl&gt;, arv_duration_days &lt;dbl&gt;\n\nsummary(bost_df)\n\n    region           random_arm           gender          age_at_interview\n Length:11369       Length:11369       Length:11369       Min.   :17.20   \n Class :character   Class :character   Class :character   1st Qu.:26.70   \n Mode  :character   Mode  :character   Mode  :character   Median :35.70   \n                                                          Mean   :38.03   \n                                                          3rd Qu.:48.20   \n                                                          Max.   :67.90   \n                                                                          \n   age5cat          employment_status  circumcised        hiv_status_current\n Length:11369       Length:11369       Length:11369       Length:11369      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n hiv_status_time    viral_load_yr3    cd4_survey_yr3   cd4_surveydays_yr3\n Length:11369       Min.   :     40   Min.   :  20.0   Min.   : 358      \n Class :character   1st Qu.:     40   1st Qu.: 401.0   1st Qu.: 875      \n Mode  :character   Median :     40   Median : 573.0   Median : 895      \n                    Mean   :   4305   Mean   : 602.1   Mean   : 881      \n                    3rd Qu.:     40   3rd Qu.: 757.0   3rd Qu.: 911      \n                    Max.   :2243242   Max.   :1567.0   Max.   :1184      \n                    NA's   :8130      NA's   :10594    NA's   :8032      \n arv_duration_days\n Min.   :   0     \n 1st Qu.: 908     \n Median :2393     \n Mean   :2432     \n 3rd Qu.:3764     \n Max.   :5879     \n NA's   :8222     \n\n\nNote: To make easy for further analysis, I selected several variables:\n\nregion = region of sbject\nrandom_arm = randomized arm\nage= age\nage5cat = age category\nemployment_status = employment status\nhiv_status_current = Current HIV status\nviral_load_yr3 = viral load in year 3 of the study\ncd4_survey_yr3 = CD4 count in year 3\ncd4_surveydays_yr3 = numbers of days enrolled to survey year 3.\narv_duration_days = number of day (duration) taking ARV\n\nIn this excercise, I would like to see the relationship between viral load, lenght of enrollment, ARV duration, and CD4 count. Therefore, I will drop all missing data in those variables.\n\ndata &lt;- bost_df %&gt;% \n  drop_na(viral_load_yr3, cd4_survey_yr3, cd4_surveydays_yr3, arv_duration_days) %&gt;% # I drop all observation wit N/A data\n  filter(viral_load_yr3 != 0, \n         cd4_survey_yr3 != 0, \n         cd4_surveydays_yr3 != 0, \n         arv_duration_days !=0) # I found some observation contain 0 I drop those observation\n\n\nsapply(data, class) # I want to check class of all variables \n\n            region         random_arm             gender   age_at_interview \n       \"character\"        \"character\"        \"character\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n       \"character\"        \"character\"        \"character\"        \"character\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n       \"character\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nFor N/A information in categorical data, I lable those missing value wih 999. However, the values are in characters. I want to change the values into factors to make us easy in analysis.\n\ndata &lt;- data %&gt;% \n  mutate(region = recode(region, \n                         \"Northern\" = 0,\n                         \"Southern\" = 1, \n                         \"Central\" = 2), \n         random_arm = recode(random_arm, \n                             \"Standard of Care\" = 0,\n                             \"Intervention\" = 1), \n         gender = recode(gender,\n                         \"M\" = 0,\n                         \"F\" =1), \n         age5cat = recode (age5cat, \n                           \"16-24 years\" = 0, \n                           \"25-34 years\" = 1, \n                           \"35-44 years\" = 2, \n                           \"45-54 years\" = 3, \n                           \"55-64 years\" = 4), \n         employment_status = recode(employment_status , \n                                    \"Employed\" = 0, \n                                    \"Unemployed looking for work\" = 1, \n                                    \"Unemployed not looking for work\" = 2), \n         circumcised= recode(circumcised , \n                             \"No\" = 0, \n                             \"Yes\" = 1), \n         hiv_status_current = recode(hiv_status_current, \n                                     \"HIV-uninfected\" = 0, \n                                     \"HIV-infected\" = 1, \n                                     \"Refused HIV testing\" = 2), \n         hiv_status_time = recode(hiv_status_time, \n                                  \"HIV-negative\" = 0, \n                                  \"HIV-positive: previously diagnosed\" = 1, \n                                  \"Refused HIV testing\" = 2, \n                                  \"HIV-positive: newly discovered\" = 3)) # this part is to recode from character to numeric to allow us convert ito factors. \n\nsapply(data, class) # check character of the variables \n\n            region         random_arm             gender   age_at_interview \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n         \"numeric\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nNow, I want to convert categorical variables into factors\n\ndata &lt;- data %&gt;%\n  mutate(across (c(\"region\", \"random_arm\", \"gender\", \"age5cat\", \"employment_status\", \"circumcised\", \"hiv_status_current\",  \"hiv_status_time\"), as.factor)) # this is to convert multiple variables into factors. as.factor() only can work in single variable. \n\nsapply(data, class) # check character of the variables \n\n            region         random_arm             gender   age_at_interview \n          \"factor\"           \"factor\"           \"factor\"          \"numeric\" \n           age5cat  employment_status        circumcised hiv_status_current \n          \"factor\"           \"factor\"           \"factor\"           \"factor\" \n   hiv_status_time     viral_load_yr3     cd4_survey_yr3 cd4_surveydays_yr3 \n          \"factor\"          \"numeric\"          \"numeric\"          \"numeric\" \n arv_duration_days \n         \"numeric\" \n\n\nI want to replace all N/A with 999 in categorical variables.\n\ndata &lt;- data %&gt;%\n  mutate(across(where(is.factor), ~ replace_na(as.character(.), \"999\"))) %&gt;% # replace the values\n  mutate(across(where(is.character), as.factor)) # convert the variable back to factor. \n\ndim(data)\n\n[1] 539  13\n\n\nThe data is clean now and ready for data exploration and analysis. There are 539 obserations and 13 variables in the final data.\n\nsummary_stats &lt;- data %&gt;%\n  summarise(\n    mean_viral_load = mean(viral_load_yr3, na.rm = TRUE),\n    sd_viral_load = sd(viral_load_yr3, na.rm = TRUE),\n    mean_cd4 = mean(cd4_survey_yr3, na.rm = TRUE),\n    sd_cd4 = sd(cd4_survey_yr3, na.rm = TRUE)\n  )\nprint(summary_stats)\n\n# A tibble: 1 × 4\n  mean_viral_load sd_viral_load mean_cd4 sd_cd4\n            &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1           1784.        13182.     620.   279.\n\n\nNow I want to make summary statistics for Viral load and visualize it ina plot.\n\nsummary_by_arm &lt;- data %&gt;%\n  group_by(random_arm) %&gt;%\n  summarise(\n    mean_cd4 = mean(cd4_survey_yr3, na.rm = TRUE),\n    sd_cd4 = sd(cd4_survey_yr3, na.rm = TRUE)\n  ) # create by randomise_arm \n\nplot1 &lt;- ggplot(summary_by_arm, aes(x = random_arm, y = mean_cd4, fill = random_arm)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  labs(title = \"Mean CD4 Count by randomise group with Standard Deviation\",\n       x = \"Groups\", y = \"Mean CD4 Count\", \n       caption = \"Red (0)= Standard of Care, Green (1)= Intervention \") +\n  theme_minimal() +\n  theme(plot.background = element_rect(color = \"black\", size = 1)) \n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nprint(plot1)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"CD4 Mean and SD .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot1) # save the pictures created \n\nSaving 7 x 5 in image\n\n\nNow I want to make summary statistics for viral load and visualize it in a plot.\n\nsummary_by_arm_vl &lt;- data %&gt;%\n  group_by(random_arm) %&gt;%\n  summarise(\n    mean_vl = mean(viral_load_yr3, na.rm = TRUE),\n    sd_vl = sd(viral_load_yr3, na.rm = TRUE)\n  ) # create by randomise_arm \n\nplot2 &lt;- ggplot(summary_by_arm_vl, aes(x = random_arm, y = mean_vl, fill = random_arm)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_vl - sd_vl, ymax = mean_vl + sd_vl), width = 0.2) +\n  labs(title = \"Mean Viral Load Count by randomise group with Standard Deviation\",\n       x = \"Groups\", y = \"Mean Viral Load\", \n       caption = \"Red (0)= Standard of Care, Green (1)= Intervention \") +\n  theme_minimal() +\n  theme(plot.background = element_rect(color = \"black\", size = 1)) \n\nprint(plot2)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"Viral Load Mean and SD .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot2) # save the pictures created\n\nSaving 7 x 5 in image\n\n\n\n\nData Exploration\nIn this part, I want to explore the data and vizualise it before data analysis.\n\nplot3 &lt;- ggplot(data, aes(x = viral_load_yr3, y = cd4_survey_yr3, color = region)) + \n  geom_point(size = 3) +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"Viral Load VS CD4 count based on the region\", \n       x = \"Viral load\", \n       y = \"CD4\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) + # Increase size and boldness of title and axes\n  scale_color_manual(values = c(\"0\" = \"#53d127\", \"1\" = \"#2ce1b0\", \"2\" = \"#ff5733\"))+ # crete color manually \n  theme(plot.background = element_rect(color = \"black\", size = 1))\n\nprint(plot3)\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"Viral Load VS CD4 based on region .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot3) # save the pictures created\n\nSaving 7 x 5 in image\n\n\n\nplot4 &lt;- ggplot(data, aes(x = arv_duration_days , y = cd4_survey_yr3, color = region)) + \n  geom_boxplot() +  # Specify geom as geom_point to make a scatterplot\n  labs(title = \"ARV Duration VS CD4 count based on the region\", \n       x = \"ARV Duration (days)\", \n       y = \"CD4 count\",\n       caption = \"Green = Nothern, Blue= Northern, and red= Central\") +  # Rename title and axes\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate state names \n        legend.position = \"bottom\",  # Position legend at the bottom\n        plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), \n        axis.title.x = element_text(size = 12, face = \"bold\"), \n        axis.title.y = element_text(size = 12, face = \"bold\")) + # Increase size and boldness of title and axes\n  theme(plot.background = element_rect(color = \"black\", size = 1)) +\n  scale_x_continuous(limits = c(0, 1000)) + # Set x-axis maximum limit to 1000\n  scale_color_manual(values = c(\"0\" = \"#53d127\", \"1\" = \"#2ce1b0\", \"2\" = \"#ff5733\")) # crete color manually \n\nprint(plot4)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\nfigure_file = here(\"cdcdata-exercise\", \"pictures\", \"ARV Duration VS CD4 count based on the region .png\") # to set up location for the pictures created \nggsave(filename = figure_file, plot=plot4) # save the pictures created\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`stat_boxplot()`).\n\n\n\n\nThis section contributed by Rayleen Lewis.\n\nPerforming a few more checks\nPrior to creating the synthetic dataset, I wanted to check on a few more things with the data, like number of persons in each region and arm of the study. It also looks like most people are virally suppressed, so I wanted to look at the viral load among people who had values &gt;200. I also want to get the median and IQR of CD4 count by region.\n\n#Number of persons in each region\ntable(data$region)\n\n\n  0   1   2 \n160 192 187 \n\n#Number of persons in each arm of the study\ntable(data$random_arm)\n\n\n  0   1 \n239 300 \n\n#Confirming that basically everyone is virally suppressed\ndata_supp &lt;- data %&gt;%\n  filter(viral_load_yr3&gt;200)\nggplot(data_supp, aes(x = viral_load_yr3)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\ndata_rl &lt;- data %&gt;%\n  mutate(viral_2 = if_else(viral_load_yr3 &lt; 200, 0, 1))\ntable(data_rl$viral_2)\n\n\n  0   1 \n510  29 \n\n#95% are virally suppressed (viral load &lt;200)\n\n#Mean (SD) of CD4 count by arm\nproc_means(data, var = cd4_survey_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN   STD   MIN   MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 cd4_survey_yr3   539  620.  279.    70  1567\n2 0         1   239 cd4_survey_yr3   239  604.  265.    70  1540\n3 1         1   300 cd4_survey_yr3   300  632.  290.    78  1567\n\nproc_means(data, var = viral_load_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN    STD   MIN    MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 viral_load_yr3   539 1784. 13182.    40 177075\n2 0         1   239 viral_load_yr3   239 3256. 19241.    40 177075\n3 1         1   300 viral_load_yr3   300  610.  3854.    40  37415\n\n\n\n\nGenerating synthetic data\nI aimed to reproduce a dataset similar to the dataset above. The following prompt was used in ChatGPT to generate the code in the next code chunk: using R, create a dataset with 539 observations and variables called region with 3 levels (values 0, 1, 2), arm with 2 levels (0, 1), cd4 (continuous), viral_load (continuous). 55% of observations should have an arm value of 1. 95% of the values for viral_load should be &lt;200. Among observations with arm = 0, viral_load should have a mean of 3256 and standard deviation of 19241 and cd4 should have a mean of 604 and standard deviation of 265. Among observations with arm = 1, viral_load should have a mean of 610 and standard deviation of 3854 and cd4 should have a mean of 632 and standard deviation of 290. 160 observations should have a region value of 0, 192 should have a region value equal to 1 and the rest should have a region value equal to 2.\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- ifelse(arm == 0,\n                     rnorm(sum(arm == 0), mean = 3256, sd = 19241),\n                     rnorm(sum(arm == 1), mean = 610, sd = 3854))\n\n# Ensure 95% of viral_load values are &lt;200\nthreshold &lt;- quantile(viral_load, 0.95)\nviral_load &lt;- ifelse(viral_load &gt; 200, runif(length(viral_load), min = 0, max = 200), viral_load)\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load       \n 0:160   0:228   Min.   :-251.6   Min.   :-48569.28  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.: -3267.70  \n 2:187           Median : 637.1   Median :    24.23  \n                 Mean   : 625.9   Mean   : -3507.56  \n                 3rd Qu.: 800.0   3rd Qu.:   125.58  \n                 Max.   :1518.6   Max.   :   198.68  \n\n\nI noticed that &gt;25% of the viral load values were negative. I needed to update the code to set the minimum value to 40 (based on the original dataset). To update the code, I used the following follow-up prompt: can you update the code so that the min value for viral_load is 40?\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- ifelse(arm == 0,\n                     rnorm(sum(arm == 0), mean = 3256, sd = 19241),\n                     rnorm(sum(arm == 1), mean = 610, sd = 3854))\n\n# Ensure 95% of viral_load values are &lt;200 while maintaining a minimum of 40\nthreshold &lt;- quantile(viral_load, 0.95)\nviral_load &lt;- ifelse(viral_load &gt; 200, runif(length(viral_load), min = 40, max = 200), viral_load)\nviral_load &lt;- pmax(viral_load, 40)  # Ensure minimum value is 40\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load    \n 0:160   0:228   Min.   :-251.6   Min.   : 40.00  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.: 40.00  \n 2:187           Median : 637.1   Median : 59.39  \n                 Mean   : 625.9   Mean   : 87.73  \n                 3rd Qu.: 800.0   3rd Qu.:140.22  \n                 Max.   :1518.6   Max.   :198.95  \n\n\n\n\nRecreating tables and plots from above\nTo check the synthetic dataset, I will recreate some of the results from above.\n\n# Mean and SD of viral load and CD4 count by arm\n#Synthetic dataset\nproc_means(dataset, var = cd4, class = arm)\n\n  CLASS TYPE FREQ VAR   N     MEAN      STD       MIN      MAX\n1  &lt;NA&gt;    0  539 cd4 539 625.8906 261.6480 -251.6203 1518.566\n2     0    1  228 cd4 228 599.0607 250.5687 -201.0123 1255.786\n3     1    1  311 cd4 311 645.5601 268.1762 -251.6203 1518.566\n\n#Original dataset\nproc_means(data, var = cd4_survey_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN   STD   MIN   MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 cd4_survey_yr3   539  620.  279.    70  1567\n2 0         1   239 cd4_survey_yr3   239  604.  265.    70  1540\n3 1         1   300 cd4_survey_yr3   300  632.  290.    78  1567\n\n#Synthetic dataset\nproc_means(dataset, var = viral_load, class = arm)\n\n  CLASS TYPE FREQ        VAR   N     MEAN      STD MIN      MAX\n1  &lt;NA&gt;    0  539 viral_load 539 87.72695 54.60308  40 198.9452\n2     0    1  228 viral_load 228 90.65165 55.76379  40 198.9452\n3     1    1  311 viral_load 311 85.58280 53.72527  40 198.2108\n\n#Original dataset\nproc_means(data, var = viral_load_yr3, class = random_arm)\n\n# A tibble: 3 × 9\n  CLASS  TYPE  FREQ VAR                N  MEAN    STD   MIN    MAX\n  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 &lt;NA&gt;      0   539 viral_load_yr3   539 1784. 13182.    40 177075\n2 0         1   239 viral_load_yr3   239 3256. 19241.    40 177075\n3 1         1   300 viral_load_yr3   300  610.  3854.    40  37415\n\n\nThe means and standard deviations for each arm are very comparable between the original and synthetic datasets. The means are within 15 of each other. The number in each arm is also similar.\nI also used ChatGPT to create plots similar to the plots above to see how well ChatGPT could continue to use the dataset. I supplied this prompt to create the code for the plots: write code to create three plots. first, plot the mean cd4 by arm with error bars showing 1 standard deviation, color arm = 0 as red and arm = 1 as teal. For the second plot, perform the same as the first plot using viral_load instead of cd4. For the third plot, create a scatterplot with cd4 as the y and viral_load as the x, coloring the dots by region.\nAs a note, based on my prompt, R assigned one of the colors in Plots 1 and 2 as teal, which isn’t a color R recognized. I changed teal to blue in the code below.\n\n# Compute summary statistics for CD4 by arm\ncd4_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_cd4 = mean(cd4),\n            sd_cd4 = sd(cd4)) \n\n# First plot: Mean CD4 by arm with error bars\np1 &lt;- ggplot(cd4_summary, aes(x = arm, y = mean_cd4, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean CD4 by Arm\", x = \"Arm\", y = \"Mean CD4 Count\") +\n  theme_minimal()\n\n# Compute summary statistics for Viral Load by arm\nviral_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_viral = mean(viral_load),\n            sd_viral = sd(viral_load)) \n\n# Second plot: Mean Viral Load by arm with error bars\np2 &lt;- ggplot(viral_summary, aes(x = arm, y = mean_viral, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_viral - sd_viral, ymax = mean_viral + sd_viral), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean Viral Load by Arm\", x = \"Arm\", y = \"Mean Viral Load\") +\n  theme_minimal()\n\n# Third plot: Scatterplot of CD4 vs. Viral Load, colored by Region\np3 &lt;- ggplot(dataset, aes(x = viral_load, y = cd4, color = region)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"blue\", \"1\" = \"green\", \"2\" = \"purple\")) +\n  labs(title = \"Scatterplot of CD4 vs Viral Load\", x = \"Viral Load\", y = \"CD4 Count\") +\n  theme_minimal()\n\n# Display plots\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\nThe synthetic viral load variable has a max of 200, even though 5% of values were supposed to be above 200 leading to a mean value of 3300 for arm = 0 and 610 for arm = 1. To get around this, I told ChatGPT what the problem was and suggested a solution: For the dataset creation, the mean viral_load is too low. Can the order of the steps for confirming 95% of value are below 200 and sampling from the distribution be switched?\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- numeric(n)  # Initialize vector\nviral_load[arm == 0] &lt;- rnorm(sum(arm == 0), mean = 3256, sd = 19241)\nviral_load[arm == 1] &lt;- rnorm(sum(arm == 1), mean = 610, sd = 3854)\n\n# Ensure 95% of viral_load values are &lt;200 while keeping original distribution\nhigh_values &lt;- which(viral_load &gt;= quantile(viral_load, 0.95))\nviral_load[high_values] &lt;- runif(length(high_values), min = 40, max = 200)  # Keep min at 40\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load      \n 0:160   0:228   Min.   :-251.6   Min.   :-48569.3  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.: -3309.8  \n 2:187           Median : 637.1   Median :   268.9  \n                 Mean   : 625.9   Mean   :   101.8  \n                 3rd Qu.: 800.0   3rd Qu.:  4466.5  \n                 Max.   :1518.6   Max.   : 25375.5  \n\n\n\n# Compute summary statistics for CD4 by arm\ncd4_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_cd4 = mean(cd4),\n            sd_cd4 = sd(cd4)) \n\n# First plot: Mean CD4 by arm with error bars\np1 &lt;- ggplot(cd4_summary, aes(x = arm, y = mean_cd4, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean CD4 by Arm\", x = \"Arm\", y = \"Mean CD4 Count\") +\n  theme_minimal()\n\n# Compute summary statistics for Viral Load by arm\nviral_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_viral = mean(viral_load),\n            sd_viral = sd(viral_load)) \n\n# Second plot: Mean Viral Load by arm with error bars\np2 &lt;- ggplot(viral_summary, aes(x = arm, y = mean_viral, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_viral - sd_viral, ymax = mean_viral + sd_viral), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean Viral Load by Arm\", x = \"Arm\", y = \"Mean Viral Load\") +\n  theme_minimal()\n\n# Third plot: Scatterplot of CD4 vs. Viral Load, colored by Region\np3 &lt;- ggplot(dataset, aes(x = viral_load, y = cd4, color = region)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"blue\", \"1\" = \"green\", \"2\" = \"purple\")) +\n  labs(title = \"Scatterplot of CD4 vs Viral Load\", x = \"Viral Load\", y = \"CD4 Count\") +\n  theme_minimal()\n\n# Display plots\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\nThis resulted in viral_load values that were negative again, so I added a final prompt: Can you update so the lowest value for viral_load is 40?\nThe following code was created:\n\nset.seed(123)  # For reproducibility\n\n# Define the number of observations\nn &lt;- 539\n\n# Define region values\nregion &lt;- c(rep(0, 160), rep(1, 192), rep(2, n - 160 - 192))\n\n# Define arm values ensuring 55% are 1\narm &lt;- sample(c(0, 1), n, replace = TRUE, prob = c(0.45, 0.55))\n\n# Generate cd4 values based on arm\ncd4 &lt;- ifelse(arm == 0,\n              rnorm(sum(arm == 0), mean = 604, sd = 265),\n              rnorm(sum(arm == 1), mean = 632, sd = 290))\n\n# Generate viral_load values based on arm\nviral_load &lt;- numeric(n)  # Initialize vector\nviral_load[arm == 0] &lt;- rnorm(sum(arm == 0), mean = 3256, sd = 19241)\nviral_load[arm == 1] &lt;- rnorm(sum(arm == 1), mean = 610, sd = 3854)\n\n# Ensure 95% of viral_load values are &lt;200 while keeping original distribution\nhigh_values &lt;- which(viral_load &gt;= quantile(viral_load, 0.95))\nviral_load[high_values] &lt;- runif(length(high_values), min = 40, max = 200)  # Keep min at 40\n\n# Ensure minimum viral_load is 40\nviral_load &lt;- pmax(viral_load, 40)\n\n# Create data frame\ndataset &lt;- data.frame(region = factor(region), arm = factor(arm), cd4 = cd4, viral_load = viral_load)\n\n# Check dataset\nsummary(dataset)\n\n region  arm          cd4           viral_load     \n 0:160   0:228   Min.   :-251.6   Min.   :   40.0  \n 1:192   1:311   1st Qu.: 446.1   1st Qu.:   40.0  \n 2:187           Median : 637.1   Median :  268.9  \n                 Mean   : 625.9   Mean   : 3525.9  \n                 3rd Qu.: 800.0   3rd Qu.: 4466.5  \n                 Max.   :1518.6   Max.   :25375.5  \n\n\n\n# Compute summary statistics for CD4 by arm\ncd4_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_cd4 = mean(cd4),\n            sd_cd4 = sd(cd4)) \n\n# First plot: Mean CD4 by arm with error bars\np1 &lt;- ggplot(cd4_summary, aes(x = arm, y = mean_cd4, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_cd4 - sd_cd4, ymax = mean_cd4 + sd_cd4), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean CD4 by Arm\", x = \"Arm\", y = \"Mean CD4 Count\") +\n  theme_minimal()\n\n# Compute summary statistics for Viral Load by arm\nviral_summary &lt;- dataset %&gt;%\n  group_by(arm) %&gt;%\n  summarise(mean_viral = mean(viral_load),\n            sd_viral = sd(viral_load)) \n\n# Second plot: Mean Viral Load by arm with error bars\np2 &lt;- ggplot(viral_summary, aes(x = arm, y = mean_viral, fill = arm)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), color = \"black\") +\n  geom_errorbar(aes(ymin = mean_viral - sd_viral, ymax = mean_viral + sd_viral), width = 0.2) +\n  scale_fill_manual(values = c(\"0\" = \"red\", \"1\" = \"blue\")) +\n  labs(title = \"Mean Viral Load by Arm\", x = \"Arm\", y = \"Mean Viral Load\") +\n  theme_minimal()\n\n# Third plot: Scatterplot of CD4 vs. Viral Load, colored by Region\np3 &lt;- ggplot(dataset, aes(x = viral_load, y = cd4, color = region)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"0\" = \"blue\", \"1\" = \"green\", \"2\" = \"purple\")) +\n  labs(title = \"Scatterplot of CD4 vs Viral Load\", x = \"Viral Load\", y = \"CD4 Count\") +\n  theme_minimal()\n\n# Display plots\nprint(p1)\n\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\nThe bar chart for CD4 by arm is very similar to the original data. Based on the bar chart for viral load, the means appear simialr but the standard errors are reduced compared to the original data. The final plot, the scatterplot of CD4 count by viral load, showed that many more observations had higher viral loads and many more had viral loads above 200. It seems like ChatGPT isn’t handling the prompts for this variable well. Overall, the synthetic data look good but not perfect."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "The data is obtained from The University of Michigan Health and Retirement Study (HRS). HRS is a longitudinal panel study that surveys a representative sample of approximately 20,000 people in America, supported by the National Institute on Aging (NIA U01AG009740) and the Social Security Administration.\nThrough its unique and in-depth interviews, the HRS provides an invaluable and growing body of multidisciplinary data that researchers can use to address important questions about the challenges and opportunities of aging.\nPlease visit: https://hrsdata.isr.umich.edu/data-products/public-survey-data?_gl=18ofhfg_gaMTc5MDM4ODE3NS4xNzM4MzU5MjU5_ga_FF28MW3MW2*MTczODM1OTI1OS4xLjEuMTczODM1OTI3NC4wLjAuMA..\nThe data is in STATA format.\nI plan to conduct survival analysis for retired people who had high Cholesterol in 2014 until they Heart Attack."
  },
  {
    "objectID": "data-exercise/data-exercise.html#select-variables-needed-for-each-year",
    "href": "data-exercise/data-exercise.html#select-variables-needed-for-each-year",
    "title": "Data Exercise",
    "section": "Select variables needed for each year",
    "text": "Select variables needed for each year\nVariables that needed in this study include Household Identification number, Respondent Person Identification number, High Cholesterol Status, Years first had heart attack, Month first had heart attack, Ever had heart attack.\n\n#data2014\nda2014 &lt;- df2014 %&gt;% \n  select(\"HHID\", \"PN\", \"OC283\", \"OC040\", \"OC258\", \"OC259\") %&gt;%\n  rename(hhid = HHID, pn = PN, chol= OC283, ha = OC040, year = OC258, month= OC259)\n\n\nda2014 &lt;- lapply(da2014, function(x) { attributes(x) &lt;- NULL; x }) #delete the column lables from dataset, because their position is incorrect after deleting some variables. \nda2014 &lt;- as.data.frame(da2014)  # Convert back to a dataframe\n\n#data2016\nda2016 &lt;- df2016 %&gt;% \n  select(\"HHID\", \"PN\", \"PC283\", \"PC040\", \"PC258\", \"PC259\") %&gt;%\n  rename(hhid = HHID, pn = PN, chol= PC283, ha = PC040, year = PC258, month= PC259)\n\n\nda2016 &lt;- lapply(da2016, function(x) { attributes(x) &lt;- NULL; x }) #delete the column lables from dataset, because their position is incorrect after deleting some variables. \nda2016 &lt;- as.data.frame(da2016)  # Convert back to a dataframe\n\n#data2018 \nda2018 &lt;- df2018 %&gt;%\n  select(\"hhid\", \"pn\", \"QC283\", \"QC040\", \"QC258\", \"QC259\") %&gt;%\n  rename( chol = QC283, ha= QC040, year = QC258, month = QC259) # I rename the column names because in this year, they use different name. \n\nda2018 &lt;- lapply(da2018, function(x) { attributes(x) &lt;- NULL; x }) #delete the column lables from dataset, because their position is incorrect after deleting some variables. \nda2018 &lt;- as.data.frame(da2018)  # Convert back to a dataframe\n\nda2020 &lt;- df2020 %&gt;%\n  select(\"HHID\", \"PN\", \"RC283\", \"RC040\", \"RC258\", \"RC259\") %&gt;%\n  rename( hhid = HHID, pn = PN, chol = RC283, ha= RC040, year = RC258, month = RC259) # I rename the column names because in this year, they use different name. \n\nda2022 &lt;- df2022 %&gt;%\n  select(\"HHID\", \"PN\", \"SC283\", \"SC040\", \"SC258\", \"SC259\") %&gt;%\n  rename( hhid = HHID, pn = PN, chol = SC283, ha= SC040, year = SC258, month = SC259) # I rename the column names because in this year, they use different name."
  },
  {
    "objectID": "data-exercise/data-exercise.html#combine-dataset",
    "href": "data-exercise/data-exercise.html#combine-dataset",
    "title": "Data Exercise",
    "section": "Combine dataset",
    "text": "Combine dataset\n\n# Perform a full join step-by-step\nmerged_data &lt;- da2014 %&gt;%\n  full_join(da2016, by = c(\"hhid\", \"pn\"), suffix = c(\"_2014\", \"_2016\")) %&gt;%\n  full_join(da2018, by = c(\"hhid\", \"pn\"), suffix = c(\"\", \"_2018\")) %&gt;%\n  full_join(da2020, by = c(\"hhid\", \"pn\"), suffix = c(\"\", \"_2020\")) %&gt;%\n  full_join(da2022, by = c(\"hhid\", \"pn\"), suffix = c(\"\", \"_2022\")) \n\n# View result\nhead(merged_data)\n\n    hhid  pn chol_2014 ha_2014 year_2014 month_2014 chol_2016 ha_2016 year_2016\n1 000003 020         5      NA        NA         NA        NA      NA        NA\n2 010001 010         1      NA        NA         NA        NA      NA        NA\n3 010003 030         1      NA        NA         NA        NA      NA        NA\n4 010004 040         5      NA        NA         NA        NA      NA        NA\n5 010013 040         1      NA        NA         NA        NA      NA        NA\n6 010013 010         1       5      1995         NA        NA      NA       995\n  month_2016 chol ha year month chol_2020 ha_2020 year_2020 month_2020\n1         NA   NA NA   NA    NA        NA      NA        NA         NA\n2         NA   NA NA   NA    NA        NA      NA        NA         NA\n3         NA    1 NA   NA    NA         1      NA        NA         NA\n4         NA    5 NA   NA    NA         1      NA        NA         NA\n5         NA    1 NA   NA    NA         1      NA        NA         NA\n6         NA   NA NA   NA    NA        NA      NA        NA         NA\n  chol_2022 ha_2022 year_2022 month_2022\n1        NA      NA        NA         NA\n2         5      NA        NA         NA\n3         5       5        NA         NA\n4         1      NA        NA         NA\n5         5      NA        NA         NA\n6        NA      NA        NA         NA\n\n\nI realize that we only need cholesterol status in 2014, therefore, I want to delete cholesterol in 2016 and 2018\n\ndf &lt;- merged_data[, !(colnames(merged_data) %in% c(\"chol_2016\", \"chol\", \"chol_2020\", \"chol_2022\"))] %&gt;% # deleting cholesterol from 2016 and 2018 \n  rename(ha_2018 = ha, year_2018 =year, month_2018 = month) # I rename variables from 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "HELLO\n\nYOU ARE IN NASIR’S PERSONAL WEBSITE\nWelcome to my website and data analysis portfolio.\nPlease use the Menu Bar above to explore about me and my portofolio in research, work, and academic life."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  factor                   3     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n2 Agecat                0             1 FALSE          3 3: 4, 2: 3, 0: 2\n3 Emp_status            0             1 FALSE          3 3: 4, 2: 3, 0: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Chol                  0             1 201.  63.2 120 160 176 243  310 ▅▇▁▇▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible.\n\n\nBox plot with Agecat and Height\n\n# Ensure Agecat is a factor\nmydata &lt;- mydata %&gt;%\n  mutate(Agecat = as.factor(Agecat))\n\nmydata$Agecat\n\n[1] 0 0 3 2 3 2 3 2 3\nLevels: 0 2 3\n\n# Ensure Emp_status is a factor with meaningful labels\nmydata$Agecat &lt;- factor(mydata$Agecat, \n                            levels = c(0, 1, 2, 3), \n                            labels = c(\"&lt;=30 Yo\", \"31-40 Yo\", \"41-50 Yo\", \"51 or more\"))\n\n# Create the boxplot\np5 &lt;- mydata %&gt;% ggplot(aes(x = Agecat, y = Height, fill = Agecat)) + \n  geom_boxplot() + \n  labs(title = \"Boxplot of Height by Agecat\",\n       x = \"Age category\",\n       y = \"Height\") +\n  theme_minimal() +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"&lt;=30 Yo\" = \"#1f78b4\",  # Blue\n                               \"31-40 Yo\" = \"#33a02c\",  # Green\n                               \"41-50 Yo\" = \"#e31a1c\",# Red\n                               \"51 or more\"= \"#CA1BDE\")) +  \n  theme(legend.position = \"none\") \nplot(p5)\n\n\n\n\n\n\n\n# Save the figure\nfigure_file = here(\"starter-analysis-exercise\", \"results\", \"figures\", \"boxplot.png\")\nggsave(filename = figure_file, plot = p5)\n\nSaving 7 x 5 in image\n\n\n\n\nScatter plot with weight and chol\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=Chol, color = Agecat)) + \n  geom_point() + \n  labs(title = \"Scatterplot of Weight and Total Cholesterol\",\n       x = \"Weight\",\n       y = \"Chol\") +\n  theme_minimal()+\n  scale_color_manual(values = c(\"&lt;=30 Yo\" = \"#1f78b4\",  # Blue\n                               \"31-40 Yo\" = \"#33a02c\",  # Green\n                               \"41-50 Yo\" = \"#e31a1c\",# Red\n                               \"51 or more\"= \"#CA1BDE\"))\n\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\", \"results\", \"figures\", \"scatterplot.png\")\nggsave(filename = figure_file, plot=p6)\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at C:/Users/mn27712/OneDrive - University of Georgia/DATA BACKUP/MACBOOK AND IPHONE_/Desktop/MADA2025/muhammadnasir-mada2025\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n$ Chol   &lt;dbl&gt; 160, 140, 163, 243, 300, 259, 120, 310, 270, 123, 145, 176, 167…\n$ Agecat &lt;dbl&gt; 0, 0, 1, 3, 3, 2, 3, 2, 3, 0, 0, 3, 2, 3\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Chol      \n Length:14          Min.   :  45.0   Length:14          Min.   :120.0  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:148.8  \n Mode  :character   Median :  70.0   Mode  :character   Median :171.5  \n                    Mean   : 602.7                      Mean   :200.8  \n                    3rd Qu.:  90.0                      3rd Qu.:255.0  \n                    Max.   :7000.0                      Max.   :310.0  \n                    NA's   :1                                          \n     Agecat     \n Min.   :0.000  \n 1st Qu.:0.250  \n Median :2.000  \n Mean   :1.786  \n 3rd Qu.:3.000  \n Max.   :3.000  \n                \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender  Chol Agecat\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 180        80 M        160      0\n2 175        70 O        140      0\n3 sixty      60 F        163      1\n4 178        76 F        243      3\n5 192        90 NA       300      3\n6 6          55 F        259      2\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n200.79\n66.29\n120\n148.75\n171.5\n255\n310\n▇▇▁▇▃\n\n\nAgecat\n0\n1.00\n1.79\n1.31\n0\n0.25\n2.0\n3\n3\n▅▁▁▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n203.69\n68.07\n120\n145.00\n176\n259\n310\n▇▆▁▇▃\n\n\nAgecat\n0\n1.00\n1.85\n1.34\n0\n0.00\n2\n3\n3\n▅▁▁▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nChol\n0\n1.00\n203.69\n68.07\n120\n145.00\n176\n259\n310\n▇▆▁▇▃\n\n\nAgecat\n0\n1.00\n1.85\n1.34\n0\n0.00\n2\n3\n3\n▅▁▁▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nChol\n0\n1\n216.36\n66.24\n120\n163.5\n235\n264.5\n310\n▃▆▁▇▃\n\n\nAgecat\n0\n1\n2.18\n1.17\n0\n2.0\n3\n3.0\n3\n▂▁▁▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nChol\n0\n1\n216.36\n66.24\n120\n163.5\n235\n264.5\n310\n▃▆▁▇▃\n\n\nAgecat\n0\n1\n2.18\n1.17\n0\n2.0\n3\n3.0\n3\n▂▁▁▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nChol\n0\n1\n201.11\n63.16\n120\n160\n176\n243\n310\n▅▇▁▇▂\n\n\nAgecat\n0\n1\n2.00\n1.22\n0\n2\n2\n3\n3\n▃▁▁▆▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "MUhammad Nasir Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  }
]